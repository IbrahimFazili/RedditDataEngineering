[2024-07-22T22:10:24.014+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-07-22T22:10:24.036+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: etl_reddit_pipeline.reddit_extraction manual__2024-07-22T22:10:22.777211+00:00 [queued]>
[2024-07-22T22:10:24.043+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: etl_reddit_pipeline.reddit_extraction manual__2024-07-22T22:10:22.777211+00:00 [queued]>
[2024-07-22T22:10:24.043+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-07-22T22:10:24.054+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): reddit_extraction> on 2024-07-22 22:10:22.777211+00:00
[2024-07-22T22:10:24.060+0000] {standard_task_runner.py:64} INFO - Started process 61 to run task
[2024-07-22T22:10:24.065+0000] {standard_task_runner.py:90} INFO - Running: ['airflow', 'tasks', 'run', 'etl_reddit_pipeline', 'reddit_extraction', 'manual__2024-07-22T22:10:22.777211+00:00', '--job-id', '6', '--raw', '--subdir', 'DAGS_FOLDER/reddit_dag.py', '--cfg-path', '/tmp/tmpc_ai9qu0']
[2024-07-22T22:10:24.067+0000] {standard_task_runner.py:91} INFO - Job 6: Subtask reddit_extraction
[2024-07-22T22:10:24.133+0000] {task_command.py:426} INFO - Running <TaskInstance: etl_reddit_pipeline.reddit_extraction manual__2024-07-22T22:10:22.777211+00:00 [running]> on host 7e89e30cc188
[2024-07-22T22:10:24.243+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Ibrahim Fazili' AIRFLOW_CTX_DAG_ID='etl_reddit_pipeline' AIRFLOW_CTX_TASK_ID='reddit_extraction' AIRFLOW_CTX_EXECUTION_DATE='2024-07-22T22:10:22.777211+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-07-22T22:10:22.777211+00:00'
[2024-07-22T22:10:24.247+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-07-22T22:10:24.269+0000] {logging_mixin.py:188} INFO - Connected to Reddit!
[2024-07-22T22:10:24.742+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f2ae123a940>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': '', 'author_fullname': 't2_uamr9xer', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Marketing: Be where your users are! At conference:', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': 140, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1e9hb4d', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.87, 'author_flair_background_color': None, 'ups': 42, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': True, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Meme', 'can_mod_post': False, 'score': 42, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'https://b.thumbs.redditmedia.com/ZRK97bXoUQleUNQrM-pbHReccgMJsrtAm7VL9sYSgHc.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'image', 'content_categories': None, 'is_self': False, 'subreddit_type': 'public', 'created': 1721662601.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'i.redd.it', 'allow_live_comments': False, 'selftext_html': None, 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'url_overridden_by_dest': 'https://i.redd.it/mkt9w3hoa3ed1.png', 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://preview.redd.it/mkt9w3hoa3ed1.png?auto=webp&s=5539d4b6ca90554e924b4268e58e3f0b26817f9a', 'width': 980, 'height': 1156}, 'resolutions': [{'url': 'https://preview.redd.it/mkt9w3hoa3ed1.png?width=108&crop=smart&auto=webp&s=7921f38da452d9c4db2dbbe5066b05031a8079ec', 'width': 108, 'height': 127}, {'url': 'https://preview.redd.it/mkt9w3hoa3ed1.png?width=216&crop=smart&auto=webp&s=a279575ac4a7325bc4553e9257eeb7c4da91d969', 'width': 216, 'height': 254}, {'url': 'https://preview.redd.it/mkt9w3hoa3ed1.png?width=320&crop=smart&auto=webp&s=e38a87a1006ba70301f504d25e1d5abebc8e215c', 'width': 320, 'height': 377}, {'url': 'https://preview.redd.it/mkt9w3hoa3ed1.png?width=640&crop=smart&auto=webp&s=b6612bacc6d5f41bbe2329eb563fd14e7469abd8', 'width': 640, 'height': 754}, {'url': 'https://preview.redd.it/mkt9w3hoa3ed1.png?width=960&crop=smart&auto=webp&s=9ff7b6911c8e81136c6bed231326c3c7eb8a4f29', 'width': 960, 'height': 1132}], 'variants': {}, 'id': 'LJtMuJ0gRAe2dBHpb7Df_9IMWVZbDLRDUaYcZXWceo8'}], 'enabled': True}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': 'dc9742bc-a7de-11eb-a2e7-0e0348c8a4c1', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'mod_note': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'num_reports': None, 'removal_reason': None, 'link_flair_background_color': '#ff66ac', 'id': '1e9hb4d', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Thinker_Assignment'), 'discussion_type': None, 'num_comments': 6, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1e9hb4d/marketing_be_where_your_users_are_at_conference/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://i.redd.it/mkt9w3hoa3ed1.png', 'subreddit_subscribers': 198788, 'created_utc': 1721662601.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-07-22T22:10:24.743+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f2ae123a940>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'how would you answer this question\n. This was a i question. What methodology can I use or lookup to answer these type of questions. \n', 'author_fullname': 't2_ett7w95u', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': "You receive a call over the weekend that an important datasource refresh has failed. You are tasked with fixing the issue, What steps do you take to fix and ensure it doesn't happen again? How would you handle it?", 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1e9gqiz', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.81, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 23, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Career', 'can_mod_post': False, 'score': 23, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': 1721662142.0, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1721661176.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>how would you answer this question\n. This was a i question. What methodology can I use or lookup to answer these type of questions. </p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '069dd614-a7dc-11eb-8e48-0e90f49436a3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#349e48', 'id': '1e9gqiz', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='maestro-5838'), 'discussion_type': None, 'num_comments': 31, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1e9gqiz/you_receive_a_call_over_the_weekend_that_an/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1e9gqiz/you_receive_a_call_over_the_weekend_that_an/', 'subreddit_subscribers': 198788, 'created_utc': 1721661176.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-07-22T22:10:24.743+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f2ae123a940>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'People who moved from Data Analyst role to DE , have you ever felt impostor feeling?\n\nI will soon switch to DE and having a feel that i know nothing.\n\nThanks', 'author_fullname': 't2_k0nzni0wx', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Impostor syndrome', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1e9czu3', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.89, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 27, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Career', 'can_mod_post': False, 'score': 27, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1721651138.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>People who moved from Data Analyst role to DE , have you ever felt impostor feeling?</p>\n\n<p>I will soon switch to DE and having a feel that i know nothing.</p>\n\n<p>Thanks</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '069dd614-a7dc-11eb-8e48-0e90f49436a3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#349e48', 'id': '1e9czu3', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Effective_Bluebird19'), 'discussion_type': None, 'num_comments': 15, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1e9czu3/impostor_syndrome/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1e9czu3/impostor_syndrome/', 'subreddit_subscribers': 198788, 'created_utc': 1721651138.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-07-22T22:10:24.744+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f2ae123a940>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "I'm currently I'd say 5/10 in terms of python, and I'm starting to see it's weaknesses and strengths.\n\nAs I'm starting to have a deeper understanding about the fundamentals of programming, I think it would be a good time to start looking at another language.\n\nI've been recommended GoLang (as it seems to be the best for microservices) and the Matt Holiday series on YouTube. Does anyone know of any good books or courses?\n\nI'm also happy to take any recommendations on other languages.\n ", 'author_fullname': 't2_13g905ckkt', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Thinking of learning another language', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1e981gb', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.83, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 24, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 24, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1721632369.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>I&#39;m currently I&#39;d say 5/10 in terms of python, and I&#39;m starting to see it&#39;s weaknesses and strengths.</p>\n\n<p>As I&#39;m starting to have a deeper understanding about the fundamentals of programming, I think it would be a good time to start looking at another language.</p>\n\n<p>I&#39;ve been recommended GoLang (as it seems to be the best for microservices) and the Matt Holiday series on YouTube. Does anyone know of any good books or courses?</p>\n\n<p>I&#39;m also happy to take any recommendations on other languages.</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1e981gb', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='sillypickl'), 'discussion_type': None, 'num_comments': 34, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1e981gb/thinking_of_learning_another_language/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1e981gb/thinking_of_learning_another_language/', 'subreddit_subscribers': 198788, 'created_utc': 1721632369.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-07-22T22:10:24.744+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f2ae123a940>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Hi, i am a data anlyst trying to transition into DE. What tech stack is most in demand these days. I am going to build my first DE project using the following \n1-Apache spark for processing \n2-Apache iceberg for lake storage\n3-MinIo as storage layer for iceberg\n4-StarRocks as DWH\n5-DBT for transformation from lake to dwh.\n\nAny advice will be helpful. ', 'author_fullname': 't2_8cp69uq4', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Data Engineering Project - Tech Stack', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1e98iu7', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.88, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 17, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 17, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1721634410.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hi, i am a data anlyst trying to transition into DE. What tech stack is most in demand these days. I am going to build my first DE project using the following \n1-Apache spark for processing \n2-Apache iceberg for lake storage\n3-MinIo as storage layer for iceberg\n4-StarRocks as DWH\n5-DBT for transformation from lake to dwh.</p>\n\n<p>Any advice will be helpful. </p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1e98iu7', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Glass_Jellyfish_9963'), 'discussion_type': None, 'num_comments': 8, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1e98iu7/data_engineering_project_tech_stack/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1e98iu7/data_engineering_project_tech_stack/', 'subreddit_subscribers': 198788, 'created_utc': 1721634410.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-07-22T22:10:24.744+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f2ae123a940>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Happy to share the [second part](https://medium.com/@nydas/kimball-star-schemas-in-data-warehousing-part-2-4ee64be25cf3?source=friends_link&sk=94919b6ef2ce92b93fd06f4b869f22a2) of my three part article mini-series on Kimball modelling. The [first article](https://medium.com/@nydas/kimball-star-schemas-in-data-warehousing-part-1-0ed765574712?source=friends_link&sk=33dcb7916ce7845e46aa986fb56902f1) looked at dimension tables, while this second article dives into the business process of creating fact tables - the glue that holds your presentation layer together.\n\nI hope this is useful to those of you starting out on your analytics engineering journey.\n\nAs always, my links to this sub are paywall bypassed.', 'author_fullname': 't2_ojr03vx2i', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Kimball Modelling - Article discussing the business process of model creation', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1e92jf1', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.87, 'author_flair_background_color': 'transparent', 'subreddit_type': 'public', 'ups': 15, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': '9ecf3c88-e787-11ed-957e-de1616aeae13', 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Blog', 'can_mod_post': False, 'score': 15, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1721612833.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Happy to share the <a href="https://medium.com/@nydas/kimball-star-schemas-in-data-warehousing-part-2-4ee64be25cf3?source=friends_link&amp;sk=94919b6ef2ce92b93fd06f4b869f22a2">second part</a> of my three part article mini-series on Kimball modelling. The <a href="https://medium.com/@nydas/kimball-star-schemas-in-data-warehousing-part-1-0ed765574712?source=friends_link&amp;sk=33dcb7916ce7845e46aa986fb56902f1">first article</a> looked at dimension tables, while this second article dives into the business process of creating fact tables - the glue that holds your presentation layer together.</p>\n\n<p>I hope this is useful to those of you starting out on your analytics engineering journey.</p>\n\n<p>As always, my links to this sub are paywall bypassed.</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/LowXNf-e70q1rh1Zpm3he_rQDiR--wICebKEvs97_QY.jpg?auto=webp&s=cd8a09f56e783bb14f4a7531afce87e52b1b3ed4', 'width': 1200, 'height': 732}, 'resolutions': [{'url': 'https://external-preview.redd.it/LowXNf-e70q1rh1Zpm3he_rQDiR--wICebKEvs97_QY.jpg?width=108&crop=smart&auto=webp&s=ad5fa0c0117808150addcb9ea0c22ba0d9c913b0', 'width': 108, 'height': 65}, {'url': 'https://external-preview.redd.it/LowXNf-e70q1rh1Zpm3he_rQDiR--wICebKEvs97_QY.jpg?width=216&crop=smart&auto=webp&s=5311d63c6a4a6a140ffce3718da724ae10667254', 'width': 216, 'height': 131}, {'url': 'https://external-preview.redd.it/LowXNf-e70q1rh1Zpm3he_rQDiR--wICebKEvs97_QY.jpg?width=320&crop=smart&auto=webp&s=536fb780d2d9e5671fa5fea2e833d7d2418aa4d5', 'width': 320, 'height': 195}, {'url': 'https://external-preview.redd.it/LowXNf-e70q1rh1Zpm3he_rQDiR--wICebKEvs97_QY.jpg?width=640&crop=smart&auto=webp&s=3a1b82fa6329bac3fa367d3bd534139cf9a0f73c', 'width': 640, 'height': 390}, {'url': 'https://external-preview.redd.it/LowXNf-e70q1rh1Zpm3he_rQDiR--wICebKEvs97_QY.jpg?width=960&crop=smart&auto=webp&s=cb719a4ccfbf99d85ecb7237a3a995640f48f335', 'width': 960, 'height': 585}, {'url': 'https://external-preview.redd.it/LowXNf-e70q1rh1Zpm3he_rQDiR--wICebKEvs97_QY.jpg?width=1080&crop=smart&auto=webp&s=949fb34b8d3182b309226e8f24d0394598414f86', 'width': 1080, 'height': 658}], 'variants': {}, 'id': 'BZFMiLhYkW_kjJzJgB8M-2nLMcBfXN93Xnly97RRcbs'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': 'eb739554-a7db-11eb-95d7-0ec0f8f30313', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': 'Data Engineering Manager', 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#0079d3', 'id': '1e92jf1', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='nydasco'), 'discussion_type': None, 'num_comments': 3, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': 'dark', 'permalink': '/r/dataengineering/comments/1e92jf1/kimball_modelling_article_discussing_the_business/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1e92jf1/kimball_modelling_article_discussing_the_business/', 'subreddit_subscribers': 198788, 'created_utc': 1721612833.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-07-22T22:10:24.745+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f2ae123a940>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Hi all,\n\nWith the revolution of recent AI technologies I wondered if and where you have used AI in regards of data engineering projects. I am not talking only about copilots for intelligent autocomplete etc. But more sophisticated usecases. Maybe productive ones ideally. For example in some of our projects we use different AI APIs to document our python code automatically and also to migrate old environments from one SQL dialect into another (thousands of script files required automated approach), including simplifying the code during migration and testing this code. \n\nDo you have any use cases where AI helped you working with data or data architecture?\n\n', 'author_fullname': 't2_dmrtpbpl', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'AI in data engineering', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1e97qc2', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.88, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 12, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 12, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1721631154.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hi all,</p>\n\n<p>With the revolution of recent AI technologies I wondered if and where you have used AI in regards of data engineering projects. I am not talking only about copilots for intelligent autocomplete etc. But more sophisticated usecases. Maybe productive ones ideally. For example in some of our projects we use different AI APIs to document our python code automatically and also to migrate old environments from one SQL dialect into another (thousands of script files required automated approach), including simplifying the code during migration and testing this code. </p>\n\n<p>Do you have any use cases where AI helped you working with data or data architecture?</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1e97qc2', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Dear_Boysenberry_521'), 'discussion_type': None, 'num_comments': 11, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1e97qc2/ai_in_data_engineering/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1e97qc2/ai_in_data_engineering/', 'subreddit_subscribers': 198788, 'created_utc': 1721631154.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-07-22T22:10:24.745+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f2ae123a940>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Before you hit me with the downvote, some context:\n\n>We were reviewing candidates for a Sr. DE position, and one of them explicitly stated, on both their CV and screening  call, to be extremely proficient in designing, developing, and deploying data pipelines.  \nOn call, all their examples were based on Tableau Prep.. meaning they used a data connector, pulled the database either through drag and drop or a custom sql statement, then transformed it using standard steps, and published to Tablea Server. The flow was scheduled on the server as well, through UI.\n\nNow my question is, are these really Pipelines by definition? ', 'author_fullname': 't2_e2yfk59o', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Are Tableau Prep flows pipelines?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1e96ee4', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.8, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 8, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 8, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1721625835.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Before you hit me with the downvote, some context:</p>\n\n<blockquote>\n<p>We were reviewing candidates for a Sr. DE position, and one of them explicitly stated, on both their CV and screening  call, to be extremely proficient in designing, developing, and deploying data pipelines.<br/>\nOn call, all their examples were based on Tableau Prep.. meaning they used a data connector, pulled the database either through drag and drop or a custom sql statement, then transformed it using standard steps, and published to Tablea Server. The flow was scheduled on the server as well, through UI.</p>\n</blockquote>\n\n<p>Now my question is, are these really Pipelines by definition? </p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1e96ee4', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='samjenkins377'), 'discussion_type': None, 'num_comments': 10, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1e96ee4/are_tableau_prep_flows_pipelines/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1e96ee4/are_tableau_prep_flows_pipelines/', 'subreddit_subscribers': 198788, 'created_utc': 1721625835.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-07-22T22:10:24.746+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f2ae123a940>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': '* 3 Follower nodes, each with 20GB RAM, 12 CPU, and 200GB SSD\n* 1 Observer node with 8GB RAM, 8 CPU, and 100GB SSD\n* 3 Backend nodes, each with 64GB RAM, 32 CPU, and 3TB SSD\n\nDetails about the [use case, workload, architecture, evaluation of the new system, and key lessons learned](https://doris.apache.org/blog/migrate-lakehouse-from-bigquery-to-doris/).', 'author_fullname': 't2_no0j2ndo', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Data lakehouse saving $4500 per month (BigQuery -> Apache Doris)', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1e9dze0', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.71, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 10, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Open Source', 'can_mod_post': False, 'score': 10, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1721653987.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><ul>\n<li>3 Follower nodes, each with 20GB RAM, 12 CPU, and 200GB SSD</li>\n<li>1 Observer node with 8GB RAM, 8 CPU, and 100GB SSD</li>\n<li>3 Backend nodes, each with 64GB RAM, 32 CPU, and 3TB SSD</li>\n</ul>\n\n<p>Details about the <a href="https://doris.apache.org/blog/migrate-lakehouse-from-bigquery-to-doris/">use case, workload, architecture, evaluation of the new system, and key lessons learned</a>.</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/0oskypRHdIls-PvZuWaLFloGYK7qYdZy4lt1XISrNNI.jpg?auto=webp&s=190ec54d9eb49a586b43cbb7f3d0bc431e9a5dec', 'width': 6272, 'height': 2688}, 'resolutions': [{'url': 'https://external-preview.redd.it/0oskypRHdIls-PvZuWaLFloGYK7qYdZy4lt1XISrNNI.jpg?width=108&crop=smart&auto=webp&s=768d3face440e13f881ef0a4171a4fa42e7328dc', 'width': 108, 'height': 46}, {'url': 'https://external-preview.redd.it/0oskypRHdIls-PvZuWaLFloGYK7qYdZy4lt1XISrNNI.jpg?width=216&crop=smart&auto=webp&s=59624f0e685f681b0b5586e51f5c279a9c8a4794', 'width': 216, 'height': 92}, {'url': 'https://external-preview.redd.it/0oskypRHdIls-PvZuWaLFloGYK7qYdZy4lt1XISrNNI.jpg?width=320&crop=smart&auto=webp&s=513dd3cc3d51795edc41fa9ba1b0206388cd377c', 'width': 320, 'height': 137}, {'url': 'https://external-preview.redd.it/0oskypRHdIls-PvZuWaLFloGYK7qYdZy4lt1XISrNNI.jpg?width=640&crop=smart&auto=webp&s=fa6bef3abd59c2e82feee917d8ecc931eda17ff1', 'width': 640, 'height': 274}, {'url': 'https://external-preview.redd.it/0oskypRHdIls-PvZuWaLFloGYK7qYdZy4lt1XISrNNI.jpg?width=960&crop=smart&auto=webp&s=2e7012c6f4d0809553b7e9dc97d1a58590906ddf', 'width': 960, 'height': 411}, {'url': 'https://external-preview.redd.it/0oskypRHdIls-PvZuWaLFloGYK7qYdZy4lt1XISrNNI.jpg?width=1080&crop=smart&auto=webp&s=332375911a5ea7d6f92209c852f9aa10b9ec059f', 'width': 1080, 'height': 462}], 'variants': {}, 'id': 'P1MFCwGp34IJAw1wiARFV8aWLaHRV1MpouDpWAPKJoc'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '3957ca64-3440-11ed-8329-2aa6ad243a59', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#005ba1', 'id': '1e9dze0', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='ApacheDoris'), 'discussion_type': None, 'num_comments': 4, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1e9dze0/data_lakehouse_saving_4500_per_month_bigquery/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1e9dze0/data_lakehouse_saving_4500_per_month_bigquery/', 'subreddit_subscribers': 198788, 'created_utc': 1721653987.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-07-22T22:10:24.746+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f2ae123a940>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'I\'m working on a pipeline to transfer data from S3 to ADLS. I\'m moving a variety of file types, about 200,000 files, with different sizes and different folder structures. \n\nSynapse is great for this case and works much quicker than anything else. However I\'ve run into the issue where any file names or folder names with special characters (!?$(){}\\) cause the pipeline to break with a "Error Code Forbidden" and 403 Forbidden.Source System. The file permissions are correct on S3. If I rename the file without special characters they move over just fine. \n\nGiven the size of the files, I can\'t rename all of them. The other problem is that I want to target files that were modified before a certain date. If I rename the files the Last Modified date changes to today and I miss date in the transfer. I\'m wondering if anyone has run into this issue? I\'ve googled the problem but found no solution. Most issues with special characters are about a schema or column header. \n\nPlease advise! ', 'author_fullname': 't2_139utosrvv', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Anyone use Synapse or ADF from Azure to move data?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1e9gc4u', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 6, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 6, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1721660192.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>I&#39;m working on a pipeline to transfer data from S3 to ADLS. I&#39;m moving a variety of file types, about 200,000 files, with different sizes and different folder structures. </p>\n\n<p>Synapse is great for this case and works much quicker than anything else. However I&#39;ve run into the issue where any file names or folder names with special characters (!?$(){}) cause the pipeline to break with a &quot;Error Code Forbidden&quot; and 403 Forbidden.Source System. The file permissions are correct on S3. If I rename the file without special characters they move over just fine. </p>\n\n<p>Given the size of the files, I can&#39;t rename all of them. The other problem is that I want to target files that were modified before a certain date. If I rename the files the Last Modified date changes to today and I miss date in the transfer. I&#39;m wondering if anyone has run into this issue? I&#39;ve googled the problem but found no solution. Most issues with special characters are about a schema or column header. </p>\n\n<p>Please advise! </p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1e9gc4u', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='unfair_angels'), 'discussion_type': None, 'num_comments': 3, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1e9gc4u/anyone_use_synapse_or_adf_from_azure_to_move_data/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1e9gc4u/anyone_use_synapse_or_adf_from_azure_to_move_data/', 'subreddit_subscribers': 198788, 'created_utc': 1721660192.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-07-22T22:10:24.746+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f2ae123a940>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "Hello, I'm an intern and I need help designing a banking data warehouse.\n\n**(The pictures contain the OLTP DB schema (source) and the DWH schema I'm currently working on)**\n\nI'm interested in analysing transactions, loans and client behaviour that's why I'm using a galaxy schema with two fact tables: Transaction and Loan (the coloured ones) but **I'm facing trouble introducing the client & district tables to the schema**.\n\nin the source DB **clients and accounts have a many-to-many relationship** and each **client is linked to a district** which is the table I'm most interested in as it contains demographic data about where clients live which will be very beneficial to the analysis and data science parts of the project.\n\nHope I explained the situation well, I'm looking for solutions and feedback please.\n\nhttps://preview.redd.it/mva35741e2ed1.png?width=750&format=png&auto=webp&s=63578d3059ec854171d380c1ae7e5a7b59f3eb6e\n\nhttps://preview.redd.it/y1xmpk22e2ed1.png?width=1095&format=png&auto=webp&s=bab29c11dde74c94d211b6ba70c117227896c466\n\n", 'author_fullname': 't2_kjvbhqyx', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Need help designing Data warehouse', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': 120, 'top_awarded_type': None, 'hide_score': False, 'media_metadata': {'y1xmpk22e2ed1': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 41, 'x': 108, 'u': 'https://preview.redd.it/y1xmpk22e2ed1.png?width=108&crop=smart&auto=webp&s=875ca94b4a0f1a6bb1dc4835a24b679fc9076056'}, {'y': 83, 'x': 216, 'u': 'https://preview.redd.it/y1xmpk22e2ed1.png?width=216&crop=smart&auto=webp&s=2c63e0ce94324ffe9803ab4f54c325536845bb59'}, {'y': 123, 'x': 320, 'u': 'https://preview.redd.it/y1xmpk22e2ed1.png?width=320&crop=smart&auto=webp&s=f099c41d4a39a95a71d23aabf29b8e801ec407be'}, {'y': 247, 'x': 640, 'u': 'https://preview.redd.it/y1xmpk22e2ed1.png?width=640&crop=smart&auto=webp&s=a39613142eadb0be38db5a721e2dae58f749770f'}, {'y': 370, 'x': 960, 'u': 'https://preview.redd.it/y1xmpk22e2ed1.png?width=960&crop=smart&auto=webp&s=b4322fb2005a8f223778e51bae92c1aa11e7e834'}, {'y': 417, 'x': 1080, 'u': 'https://preview.redd.it/y1xmpk22e2ed1.png?width=1080&crop=smart&auto=webp&s=bf9fc8487c6093f32d80bb6cbcad7ed5d506c830'}], 's': {'y': 423, 'x': 1095, 'u': 'https://preview.redd.it/y1xmpk22e2ed1.png?width=1095&format=png&auto=webp&s=bab29c11dde74c94d211b6ba70c117227896c466'}, 'id': 'y1xmpk22e2ed1'}, 'mva35741e2ed1': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 92, 'x': 108, 'u': 'https://preview.redd.it/mva35741e2ed1.png?width=108&crop=smart&auto=webp&s=9aa632cc01dc7a23c1a639630273ee312b66ec85'}, {'y': 185, 'x': 216, 'u': 'https://preview.redd.it/mva35741e2ed1.png?width=216&crop=smart&auto=webp&s=e7d0f58242679015d341b69089a04b8cb2a9bcb9'}, {'y': 274, 'x': 320, 'u': 'https://preview.redd.it/mva35741e2ed1.png?width=320&crop=smart&auto=webp&s=8206ef070921e0d46d4787b6cec90ededb892535'}, {'y': 548, 'x': 640, 'u': 'https://preview.redd.it/mva35741e2ed1.png?width=640&crop=smart&auto=webp&s=457ed416b7bdda3f6acff23ef639d6437b834714'}], 's': {'y': 643, 'x': 750, 'u': 'https://preview.redd.it/mva35741e2ed1.png?width=750&format=png&auto=webp&s=63578d3059ec854171d380c1ae7e5a7b59f3eb6e'}, 'id': 'mva35741e2ed1'}}, 'name': 't3_1e9d5ux', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.88, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 6, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 6, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'https://b.thumbs.redditmedia.com/Oknv4cPOwoSei1UMjesKASTVEbkXUXLp9DGg7z2AQkg.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1721651628.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hello, I&#39;m an intern and I need help designing a banking data warehouse.</p>\n\n<p><strong>(The pictures contain the OLTP DB schema (source) and the DWH schema I&#39;m currently working on)</strong></p>\n\n<p>I&#39;m interested in analysing transactions, loans and client behaviour that&#39;s why I&#39;m using a galaxy schema with two fact tables: Transaction and Loan (the coloured ones) but <strong>I&#39;m facing trouble introducing the client &amp; district tables to the schema</strong>.</p>\n\n<p>in the source DB <strong>clients and accounts have a many-to-many relationship</strong> and each <strong>client is linked to a district</strong> which is the table I&#39;m most interested in as it contains demographic data about where clients live which will be very beneficial to the analysis and data science parts of the project.</p>\n\n<p>Hope I explained the situation well, I&#39;m looking for solutions and feedback please.</p>\n\n<p><a href="https://preview.redd.it/mva35741e2ed1.png?width=750&amp;format=png&amp;auto=webp&amp;s=63578d3059ec854171d380c1ae7e5a7b59f3eb6e">https://preview.redd.it/mva35741e2ed1.png?width=750&amp;format=png&amp;auto=webp&amp;s=63578d3059ec854171d380c1ae7e5a7b59f3eb6e</a></p>\n\n<p><a href="https://preview.redd.it/y1xmpk22e2ed1.png?width=1095&amp;format=png&amp;auto=webp&amp;s=bab29c11dde74c94d211b6ba70c117227896c466">https://preview.redd.it/y1xmpk22e2ed1.png?width=1095&amp;format=png&amp;auto=webp&amp;s=bab29c11dde74c94d211b6ba70c117227896c466</a></p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1e9d5ux', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Electronic_Battle876'), 'discussion_type': None, 'num_comments': 5, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1e9d5ux/need_help_designing_data_warehouse/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1e9d5ux/need_help_designing_data_warehouse/', 'subreddit_subscribers': 198788, 'created_utc': 1721651628.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-07-22T22:10:24.747+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f2ae123a940>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Hey all - looking for feedback on an attempt to simplify SQL for some parts of data engineering, with the up-front acknowledgement that trying to replace SQL is generally a bad idea. \n\nSQL is great. Trilogy is an open-source attempt simplify *data warehouse* SQL (reporting, analytics, dashboards, ETL, etc) by augmenting core SQL syntax with a lightweight semantic binding layer that removes the need for FROM/JOIN clause(s).\n\nIt\'s a simple authoring framework for PK/FK definition that enables automatic traversal at query time with the ability to define reusable calculations - without requiring you to drop into a different language to modify the semantic layer, so you can iterate rapidly.\n\nQueries look like SQL, but operate on \'concepts\', a reusable semantic definition that can include a calculation on other concepts. Root concepts are bound to the actual warehouse via datasource definitions which associate them with columns on tables.\n\nAt query execution time, the compiler evaluates if the selected concepts can be resolved from the semantic layer by recursively sourcing all inputs of a given concept, and automatically infers any joins required and builds the relevant SQL to execute against a given backend (presto, bigquery, snowflake, etc). The query engine operating one level of abstraction up enables a lot of efficiency optimization - if you materialize a derived concept, it can be immediately referenced by a followup query without requiring recalculation, for example.\n\nThe semantic layer can be imported/reused, including reusable CTEs/concept definitions, and ported across dbs or refactored to new tables by just updating the root datasource bindings.\n\nGoals are:\n\n* Decouple business logic from the storage layer in the warehouse to enable them to evolve separately - don\'t worry about breaking your user queries when you refactor your model\n* Simplify syntax where possible and have it encourage "doing the right thing"\n* Maintain acceptable performance/generate reasonable SQL for a human to read\n\n[Github](https://github.com/trilogy-data/pytrilogy)\n\nOnline [Demo](https://trilogydata.dev/demo/)\n\nAll feedback/criticism/contributions welcome!', 'author_fullname': 't2_ey4fa443', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Trilogy - An [Experimental] Accessible SQL Semantic Layer', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1e9cqm8', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 8, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Open Source', 'can_mod_post': False, 'score': 8, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1721650350.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hey all - looking for feedback on an attempt to simplify SQL for some parts of data engineering, with the up-front acknowledgement that trying to replace SQL is generally a bad idea. </p>\n\n<p>SQL is great. Trilogy is an open-source attempt simplify <em>data warehouse</em> SQL (reporting, analytics, dashboards, ETL, etc) by augmenting core SQL syntax with a lightweight semantic binding layer that removes the need for FROM/JOIN clause(s).</p>\n\n<p>It&#39;s a simple authoring framework for PK/FK definition that enables automatic traversal at query time with the ability to define reusable calculations - without requiring you to drop into a different language to modify the semantic layer, so you can iterate rapidly.</p>\n\n<p>Queries look like SQL, but operate on &#39;concepts&#39;, a reusable semantic definition that can include a calculation on other concepts. Root concepts are bound to the actual warehouse via datasource definitions which associate them with columns on tables.</p>\n\n<p>At query execution time, the compiler evaluates if the selected concepts can be resolved from the semantic layer by recursively sourcing all inputs of a given concept, and automatically infers any joins required and builds the relevant SQL to execute against a given backend (presto, bigquery, snowflake, etc). The query engine operating one level of abstraction up enables a lot of efficiency optimization - if you materialize a derived concept, it can be immediately referenced by a followup query without requiring recalculation, for example.</p>\n\n<p>The semantic layer can be imported/reused, including reusable CTEs/concept definitions, and ported across dbs or refactored to new tables by just updating the root datasource bindings.</p>\n\n<p>Goals are:</p>\n\n<ul>\n<li>Decouple business logic from the storage layer in the warehouse to enable them to evolve separately - don&#39;t worry about breaking your user queries when you refactor your model</li>\n<li>Simplify syntax where possible and have it encourage &quot;doing the right thing&quot;</li>\n<li>Maintain acceptable performance/generate reasonable SQL for a human to read</li>\n</ul>\n\n<p><a href="https://github.com/trilogy-data/pytrilogy">Github</a></p>\n\n<p>Online <a href="https://trilogydata.dev/demo/">Demo</a></p>\n\n<p>All feedback/criticism/contributions welcome!</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/Eaotd3cqUtnkHEXYZKHDecXcadkgnwq-GJBrtxUbKNg.jpg?auto=webp&s=b344edc8b8d3b739e4baf26607763eb0c36a45ce', 'width': 1200, 'height': 600}, 'resolutions': [{'url': 'https://external-preview.redd.it/Eaotd3cqUtnkHEXYZKHDecXcadkgnwq-GJBrtxUbKNg.jpg?width=108&crop=smart&auto=webp&s=0cd82394e0f319ef5ab82c32f7882877bbb69cdb', 'width': 108, 'height': 54}, {'url': 'https://external-preview.redd.it/Eaotd3cqUtnkHEXYZKHDecXcadkgnwq-GJBrtxUbKNg.jpg?width=216&crop=smart&auto=webp&s=6f5383149ec5eefba0198b1760ad08cae52e9701', 'width': 216, 'height': 108}, {'url': 'https://external-preview.redd.it/Eaotd3cqUtnkHEXYZKHDecXcadkgnwq-GJBrtxUbKNg.jpg?width=320&crop=smart&auto=webp&s=35eff2d523170983de33c4dc78687a6bf8b8233f', 'width': 320, 'height': 160}, {'url': 'https://external-preview.redd.it/Eaotd3cqUtnkHEXYZKHDecXcadkgnwq-GJBrtxUbKNg.jpg?width=640&crop=smart&auto=webp&s=00299674c5ee9be118a5c103663b766aa2fb396f', 'width': 640, 'height': 320}, {'url': 'https://external-preview.redd.it/Eaotd3cqUtnkHEXYZKHDecXcadkgnwq-GJBrtxUbKNg.jpg?width=960&crop=smart&auto=webp&s=d4f14bb9960170a274929b8fa5f04866ff95add4', 'width': 960, 'height': 480}, {'url': 'https://external-preview.redd.it/Eaotd3cqUtnkHEXYZKHDecXcadkgnwq-GJBrtxUbKNg.jpg?width=1080&crop=smart&auto=webp&s=942d67d2498a67b1402a83d596932c0156476649', 'width': 1080, 'height': 540}], 'variants': {}, 'id': 'RcD4DBZk6UMkH6ky9dsJa3AAii2xjgmcx9-XPVNAvq8'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '3957ca64-3440-11ed-8329-2aa6ad243a59', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#005ba1', 'id': '1e9cqm8', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Prestigious_Bench_96'), 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1e9cqm8/trilogy_an_experimental_accessible_sql_semantic/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1e9cqm8/trilogy_an_experimental_accessible_sql_semantic/', 'subreddit_subscribers': 198788, 'created_utc': 1721650350.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-07-22T22:10:24.747+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f2ae123a940>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Hi guy my agency wants us to up skill and get affordable data privacy courses. Any suggestions would be welcome. Thanks \n\nFor context: we are a resident identity management agency. ', 'author_fullname': 't2_9zjjdb7h', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Data privacy courses ', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1e9j9z5', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 7, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Career', 'can_mod_post': False, 'score': 7, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1721667488.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hi guy my agency wants us to up skill and get affordable data privacy courses. Any suggestions would be welcome. Thanks </p>\n\n<p>For context: we are a resident identity management agency. </p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '069dd614-a7dc-11eb-8e48-0e90f49436a3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#349e48', 'id': '1e9j9z5', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Aymwafiq'), 'discussion_type': None, 'num_comments': 4, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1e9j9z5/data_privacy_courses/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1e9j9z5/data_privacy_courses/', 'subreddit_subscribers': 198788, 'created_utc': 1721667488.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-07-22T22:10:24.748+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f2ae123a940>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "I preface this question by saying I am a Data Scientist, leading my team, not a data engineer. But our team needs to do a lot of self-service and so we rely mostly on PySpark for doing our jobs (on EMR typically). For a very long time I have been interested in having our jobs orchestrated, with the thought that it would likely be Airflow. Suffice it to say that in recent weeks I have gone down a bit of rabbit hole and finally decided we need to make the plunge and build out our automation. I have chosen to go with Dagster over Airflow (which btw, I have used in the past for personal projects and know fairly well) and have also been playing around with DuckDB and immediately felt like it could be a game changer for our flows. So Dagster and DuckDB seems like a great combo for most of the data pipelines we run. \n\nAnd this brings me to the question at hand. I have looked at dlt and dbt as tools that could potentially be added to this stack, but in either case it's not clear to me exactly what they are bringing to the table that can't be done by Dagster+DuckDB without adding extra complexity. Am I missing something? What are the use cases where dlt and or dbt are helpful, if not critical to the modern data stack? Is it more about scaling? One thought I had is that these tools would make more sense if I had to deal with many different data sources coming into our pipelines, which we do not. We are dealing almost entirely with flat files sitting on S3.  Any thoughts and or advice will be appreciated!", 'author_fullname': 't2_iu6b4', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Do I "need" to add dlt+dbt to my Dagster+DuckDB stack?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1e9ienm', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.81, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 6, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 6, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1721665327.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>I preface this question by saying I am a Data Scientist, leading my team, not a data engineer. But our team needs to do a lot of self-service and so we rely mostly on PySpark for doing our jobs (on EMR typically). For a very long time I have been interested in having our jobs orchestrated, with the thought that it would likely be Airflow. Suffice it to say that in recent weeks I have gone down a bit of rabbit hole and finally decided we need to make the plunge and build out our automation. I have chosen to go with Dagster over Airflow (which btw, I have used in the past for personal projects and know fairly well) and have also been playing around with DuckDB and immediately felt like it could be a game changer for our flows. So Dagster and DuckDB seems like a great combo for most of the data pipelines we run. </p>\n\n<p>And this brings me to the question at hand. I have looked at dlt and dbt as tools that could potentially be added to this stack, but in either case it&#39;s not clear to me exactly what they are bringing to the table that can&#39;t be done by Dagster+DuckDB without adding extra complexity. Am I missing something? What are the use cases where dlt and or dbt are helpful, if not critical to the modern data stack? Is it more about scaling? One thought I had is that these tools would make more sense if I had to deal with many different data sources coming into our pipelines, which we do not. We are dealing almost entirely with flat files sitting on S3.  Any thoughts and or advice will be appreciated!</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1e9ienm', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='thecity2'), 'discussion_type': None, 'num_comments': 2, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1e9ienm/do_i_need_to_add_dltdbt_to_my_dagsterduckdb_stack/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1e9ienm/do_i_need_to_add_dltdbt_to_my_dagsterduckdb_stack/', 'subreddit_subscribers': 198788, 'created_utc': 1721665327.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-07-22T22:10:24.748+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f2ae123a940>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "I'm tired of excel data manipulation and want to provide a better approach to this problem. \n\nMy job uses excel models (around 20 different models), which is an excel file with ~10-15 worksheets, an input sheet and 2-5 output sheets, between them input variables populate the model that consists in sums, conditional logic between tables, ranges, constant values and different arithmetic operations. Between 100-500 input variables and around 400 output variables.\n\nThe ETL process is now automated ( set input data, calculate workbook and extract output ranges) but takes some time to process 3-10 min\n\nThis design tries to bring closer two groups of people, excel maintainers with no programming knowledge and a web application (my team) that through a web interface provides input variables and display output variables.\n\nDo you know of a technique that converts excel models and its relationships into SQL or another function?\n\nDo you know if low-code SQL is mature enough for this type of applications?", 'author_fullname': 't2_2t4xjv5a', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'I want to bridge excel "models" with SQL, is it possible?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1e97i9d', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 5, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 5, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1721630229.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>I&#39;m tired of excel data manipulation and want to provide a better approach to this problem. </p>\n\n<p>My job uses excel models (around 20 different models), which is an excel file with ~10-15 worksheets, an input sheet and 2-5 output sheets, between them input variables populate the model that consists in sums, conditional logic between tables, ranges, constant values and different arithmetic operations. Between 100-500 input variables and around 400 output variables.</p>\n\n<p>The ETL process is now automated ( set input data, calculate workbook and extract output ranges) but takes some time to process 3-10 min</p>\n\n<p>This design tries to bring closer two groups of people, excel maintainers with no programming knowledge and a web application (my team) that through a web interface provides input variables and display output variables.</p>\n\n<p>Do you know of a technique that converts excel models and its relationships into SQL or another function?</p>\n\n<p>Do you know if low-code SQL is mature enough for this type of applications?</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1e97i9d', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='erlototo'), 'discussion_type': None, 'num_comments': 4, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1e97i9d/i_want_to_bridge_excel_models_with_sql_is_it/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1e97i9d/i_want_to_bridge_excel_models_with_sql_is_it/', 'subreddit_subscribers': 198788, 'created_utc': 1721630229.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-07-22T22:10:24.748+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f2ae123a940>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': '', 'author_fullname': 't2_cbh6ollo', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Data Engg of LLM use cases for customer data', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': 73, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1e9jxk3', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.61, 'author_flair_background_color': None, 'ups': 2, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 2, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'https://b.thumbs.redditmedia.com/eT5N74HoNrs2VCN8jGlohUYaGT74siz4EBKCs3xkRkA.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'link', 'content_categories': None, 'is_self': False, 'subreddit_type': 'public', 'created': 1721669074.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'rudderstack.com', 'allow_live_comments': False, 'selftext_html': None, 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'url_overridden_by_dest': 'https://www.rudderstack.com/blog/two-prototypes-using-llms-with-customer-data/', 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/k9Nwye_40B4q0j-LJGhvH0kn_3iE0q5IGuebxpnOWSY.jpg?auto=webp&s=d0520e73e1ceffdbee378f67c57e0ffd7bcc91fd', 'width': 2400, 'height': 1256}, 'resolutions': [{'url': 'https://external-preview.redd.it/k9Nwye_40B4q0j-LJGhvH0kn_3iE0q5IGuebxpnOWSY.jpg?width=108&crop=smart&auto=webp&s=74e9a8e2061d958b1290d4661a604457c088ab4c', 'width': 108, 'height': 56}, {'url': 'https://external-preview.redd.it/k9Nwye_40B4q0j-LJGhvH0kn_3iE0q5IGuebxpnOWSY.jpg?width=216&crop=smart&auto=webp&s=e0f29eea9e62957c615f94935928aaeb0b822da7', 'width': 216, 'height': 113}, {'url': 'https://external-preview.redd.it/k9Nwye_40B4q0j-LJGhvH0kn_3iE0q5IGuebxpnOWSY.jpg?width=320&crop=smart&auto=webp&s=3718af31759e9cc9e9ea00acae94949b5b3e3784', 'width': 320, 'height': 167}, {'url': 'https://external-preview.redd.it/k9Nwye_40B4q0j-LJGhvH0kn_3iE0q5IGuebxpnOWSY.jpg?width=640&crop=smart&auto=webp&s=956d477b0526258162bc63881c402be5a3841081', 'width': 640, 'height': 334}, {'url': 'https://external-preview.redd.it/k9Nwye_40B4q0j-LJGhvH0kn_3iE0q5IGuebxpnOWSY.jpg?width=960&crop=smart&auto=webp&s=321853d871e3d5ac00e117dfdbd16d4e2263ec72', 'width': 960, 'height': 502}, {'url': 'https://external-preview.redd.it/k9Nwye_40B4q0j-LJGhvH0kn_3iE0q5IGuebxpnOWSY.jpg?width=1080&crop=smart&auto=webp&s=795c0840fc5126513d63d248ac95fef067de6f7b', 'width': 1080, 'height': 565}], 'variants': {}, 'id': '6-eEIMpxXZgJpR9sgaGEn3CR4YEsHZtyTXIEWQDt0ek'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'mod_note': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'num_reports': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1e9jxk3', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='ephemeral404'), 'discussion_type': None, 'num_comments': 2, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1e9jxk3/data_engg_of_llm_use_cases_for_customer_data/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.rudderstack.com/blog/two-prototypes-using-llms-with-customer-data/', 'subreddit_subscribers': 198788, 'created_utc': 1721669074.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-07-22T22:10:24.749+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f2ae123a940>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Has anyone tried to run dbt core on Azure Functions?.  Could you share your experience. ', 'author_fullname': 't2_o78u2p44', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Is it possible to run dbt (core) on Azure functions? ', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1e9huvf', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.72, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 3, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 3, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1721663969.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Has anyone tried to run dbt core on Azure Functions?.  Could you share your experience. </p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1e9huvf', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='SignificanceNo136'), 'discussion_type': None, 'num_comments': 3, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1e9huvf/is_it_possible_to_run_dbt_core_on_azure_functions/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1e9huvf/is_it_possible_to_run_dbt_core_on_azure_functions/', 'subreddit_subscribers': 198788, 'created_utc': 1721663969.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-07-22T22:10:24.749+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f2ae123a940>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'I know one can use the below query to get snapshots of an Iceberg table\n\nspark.sql(SELECT * FROM db.table1.snapshots)\n\nBut what if the table is accessible only via an S3 path? How do I get snapshots in this case?\n\nspark.read.format(iceberg).load(S3path)\n\nChatGPT given answer (see below) did not work. \n\nspark.sql(select * from history(s3path))', 'author_fullname': 't2_2dmm3k5o', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Accessing Snapshots of an Iceberg table stored in S3', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1e9dp1h', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.72, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 3, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 3, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1721653201.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>I know one can use the below query to get snapshots of an Iceberg table</p>\n\n<p>spark.sql(SELECT * FROM db.table1.snapshots)</p>\n\n<p>But what if the table is accessible only via an S3 path? How do I get snapshots in this case?</p>\n\n<p>spark.read.format(iceberg).load(S3path)</p>\n\n<p>ChatGPT given answer (see below) did not work. </p>\n\n<p>spark.sql(select * from history(s3path))</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1e9dp1h', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='crazybOzO'), 'discussion_type': None, 'num_comments': 2, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1e9dp1h/accessing_snapshots_of_an_iceberg_table_stored_in/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1e9dp1h/accessing_snapshots_of_an_iceberg_table_stored_in/', 'subreddit_subscribers': 198788, 'created_utc': 1721653201.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-07-22T22:10:24.749+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f2ae123a940>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "My team is in process of refactor Data Platform. Bunch of .json files land on RAW S3 bucket. Those data is exploded from document format to table format and converted to .parqet. Later there are created few denormalized tables with bunch of joins etc. (I know it is wrong... but this is requirement. I tried to tell them it should be done in Data Werehouse...)\n\nTech stack is - AWS Glue + Spark Dataframes + MWAA (Airflow). \n\nI don't like:\n\n- Spark Dataframes - prefer SQL -> is easier to understand for simple transofrmations\n\n- How data should be transformed (in what order) is stored in DAG. When you read just Glue python scripts there is no indication... Maybe Step functions or MWAA + DBT + Cosmos by Astonomer.\n\nBusiness requirement is that data will be processed in near real time. Spark and Glue seems to be great but it is a pain to model that and take care of logic and downstream dependencies of what order data should be transformed to Airflow. I have found that -> [https://aws.amazon.com/blogs/big-data/build-and-manage-your-modern-data-stack-using-dbt-and-aws-glue-through-dbt-glue-the-new-trusted-dbt-adapter/](https://aws.amazon.com/blogs/big-data/build-and-manage-your-modern-data-stack-using-dbt-and-aws-glue-through-dbt-glue-the-new-trusted-dbt-adapter/) DBT adapter for AWS Glue. I am wondering if it is possible or someone did it to set up Spark Structured Streaming in DBT + AWS Glue. \n\nThis solution would solve all our problems\n\n- easy to read and understand: SQL\n\n- scalable SPARK + Glue\n\n- Fast: Streaming\n\n- easy no manage dependencies: DBT\n\nMaybe there is some other tool like DBT for Data Warehousing but for Spark ???\n\nI am not sure in which direction should we go...", 'author_fullname': 't2_45aob9b94', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'DBT + AWS Glue (Spark SQL)... How it works ?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1e9bjls', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.72, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 3, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 3, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1721646462.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>My team is in process of refactor Data Platform. Bunch of .json files land on RAW S3 bucket. Those data is exploded from document format to table format and converted to .parqet. Later there are created few denormalized tables with bunch of joins etc. (I know it is wrong... but this is requirement. I tried to tell them it should be done in Data Werehouse...)</p>\n\n<p>Tech stack is - AWS Glue + Spark Dataframes + MWAA (Airflow). </p>\n\n<p>I don&#39;t like:</p>\n\n<ul>\n<li><p>Spark Dataframes - prefer SQL -&gt; is easier to understand for simple transofrmations</p></li>\n<li><p>How data should be transformed (in what order) is stored in DAG. When you read just Glue python scripts there is no indication... Maybe Step functions or MWAA + DBT + Cosmos by Astonomer.</p></li>\n</ul>\n\n<p>Business requirement is that data will be processed in near real time. Spark and Glue seems to be great but it is a pain to model that and take care of logic and downstream dependencies of what order data should be transformed to Airflow. I have found that -&gt; <a href="https://aws.amazon.com/blogs/big-data/build-and-manage-your-modern-data-stack-using-dbt-and-aws-glue-through-dbt-glue-the-new-trusted-dbt-adapter/">https://aws.amazon.com/blogs/big-data/build-and-manage-your-modern-data-stack-using-dbt-and-aws-glue-through-dbt-glue-the-new-trusted-dbt-adapter/</a> DBT adapter for AWS Glue. I am wondering if it is possible or someone did it to set up Spark Structured Streaming in DBT + AWS Glue. </p>\n\n<p>This solution would solve all our problems</p>\n\n<ul>\n<li><p>easy to read and understand: SQL</p></li>\n<li><p>scalable SPARK + Glue</p></li>\n<li><p>Fast: Streaming</p></li>\n<li><p>easy no manage dependencies: DBT</p></li>\n</ul>\n\n<p>Maybe there is some other tool like DBT for Data Warehousing but for Spark ???</p>\n\n<p>I am not sure in which direction should we go...</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/1c66_0YRu--OiZP8cbWxWJeZXMu3WJanxUPXpdz_Dic.jpg?auto=webp&s=f04638129ad6f1a0785862f32edd5990eaee1de6', 'width': 1120, 'height': 630}, 'resolutions': [{'url': 'https://external-preview.redd.it/1c66_0YRu--OiZP8cbWxWJeZXMu3WJanxUPXpdz_Dic.jpg?width=108&crop=smart&auto=webp&s=0b3e38ec9a906988f2e1c3f31efc70cc57fed67e', 'width': 108, 'height': 60}, {'url': 'https://external-preview.redd.it/1c66_0YRu--OiZP8cbWxWJeZXMu3WJanxUPXpdz_Dic.jpg?width=216&crop=smart&auto=webp&s=96b91835c4f129d2976ab044c2ff952f418b3f33', 'width': 216, 'height': 121}, {'url': 'https://external-preview.redd.it/1c66_0YRu--OiZP8cbWxWJeZXMu3WJanxUPXpdz_Dic.jpg?width=320&crop=smart&auto=webp&s=019d277bcda3bb47df19d2f125c271bf28e2aaf3', 'width': 320, 'height': 180}, {'url': 'https://external-preview.redd.it/1c66_0YRu--OiZP8cbWxWJeZXMu3WJanxUPXpdz_Dic.jpg?width=640&crop=smart&auto=webp&s=1a64173523c2d7540b7cc293b3b5902ecb59851b', 'width': 640, 'height': 360}, {'url': 'https://external-preview.redd.it/1c66_0YRu--OiZP8cbWxWJeZXMu3WJanxUPXpdz_Dic.jpg?width=960&crop=smart&auto=webp&s=ff7c517ac24dabbdc313ffbdf2573d536789eb35', 'width': 960, 'height': 540}, {'url': 'https://external-preview.redd.it/1c66_0YRu--OiZP8cbWxWJeZXMu3WJanxUPXpdz_Dic.jpg?width=1080&crop=smart&auto=webp&s=7dc50363073e014261d5bccf72d7aa9837e7c946', 'width': 1080, 'height': 607}], 'variants': {}, 'id': 'iKK6n3-azS8TokYTGo64RU98gOxHXE9QQTLTTw806rE'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1e9bjls', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Deep-Shape-323'), 'discussion_type': None, 'num_comments': 4, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1e9bjls/dbt_aws_glue_spark_sql_how_it_works/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1e9bjls/dbt_aws_glue_spark_sql_how_it_works/', 'subreddit_subscribers': 198788, 'created_utc': 1721646462.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-07-22T22:10:24.750+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f2ae123a940>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Hello,   \nI\'m currently working in a new project which uses Redshift as a DWH., and there is a DMS in the background that is replicating data from MySQL RDS to Redshift. \n\nI have a task to introduce DBT technology for Transformations on Redshift. And I have some questions/concerns.  \n  \nWhen replicating data from RDS to Redshift, I\'m wondering about the best approach to handle this data in my dbt workflow. I\'m considering two options and would appreciate your insights:\n\nTreat replicated tables as raw/untouchable source data:\n\n* Keep the replicated data unchanged\n* Run DBT workflows which basically are coping (maybe some little changes) these tables to new tables in my target "dbt" schema - and at this stage add dist keys, sortkeys to copied table. Let\'s call it my staging layer - but with table materializations\n* Create next layers based on these tables: intermediate/marts etc\n\nModify the raw replicated tables:\n\n* Add distkey and sortkey directly to the raw replicated tables in Redshift\n*  Still use dbt for further transformations and modeling, but in these casse treat "replicated tables" as my source data that is already prepared,and  modified for  staging layer.\n\nSo in the end I wonder if it\'s okey to touch replicated tables and build on top of them, or better to leave them untouched and just copy them in proper form into new base layer for my dbt workflows.\n\nWhich approach is generally considered best practice? Are there scenarios where one might be preferred over the other?\n\nAdditionally, if I go with the second approach, how can I automate the process of adding or updating distkey and sortkey for newly replicated tables in Redshift?\n\nI\'d appreciate any advice, especially from those who have experience with similar setups. Thanks in advance!', 'author_fullname': 't2_835lb23v', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': ' Best practices for handling replicated data from RDS to Redshift  for DBT workflows', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1e97o71', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.81, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 3, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 3, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1721630910.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hello,<br/>\nI&#39;m currently working in a new project which uses Redshift as a DWH., and there is a DMS in the background that is replicating data from MySQL RDS to Redshift. </p>\n\n<p>I have a task to introduce DBT technology for Transformations on Redshift. And I have some questions/concerns.  </p>\n\n<p>When replicating data from RDS to Redshift, I&#39;m wondering about the best approach to handle this data in my dbt workflow. I&#39;m considering two options and would appreciate your insights:</p>\n\n<p>Treat replicated tables as raw/untouchable source data:</p>\n\n<ul>\n<li>Keep the replicated data unchanged</li>\n<li>Run DBT workflows which basically are coping (maybe some little changes) these tables to new tables in my target &quot;dbt&quot; schema - and at this stage add dist keys, sortkeys to copied table. Let&#39;s call it my staging layer - but with table materializations</li>\n<li>Create next layers based on these tables: intermediate/marts etc</li>\n</ul>\n\n<p>Modify the raw replicated tables:</p>\n\n<ul>\n<li>Add distkey and sortkey directly to the raw replicated tables in Redshift</li>\n<li> Still use dbt for further transformations and modeling, but in these casse treat &quot;replicated tables&quot; as my source data that is already prepared,and  modified for  staging layer.</li>\n</ul>\n\n<p>So in the end I wonder if it&#39;s okey to touch replicated tables and build on top of them, or better to leave them untouched and just copy them in proper form into new base layer for my dbt workflows.</p>\n\n<p>Which approach is generally considered best practice? Are there scenarios where one might be preferred over the other?</p>\n\n<p>Additionally, if I go with the second approach, how can I automate the process of adding or updating distkey and sortkey for newly replicated tables in Redshift?</p>\n\n<p>I&#39;d appreciate any advice, especially from those who have experience with similar setups. Thanks in advance!</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1e97o71', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Purple_Wrap9596'), 'discussion_type': None, 'num_comments': 2, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1e97o71/best_practices_for_handling_replicated_data_from/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1e97o71/best_practices_for_handling_replicated_data_from/', 'subreddit_subscribers': 198788, 'created_utc': 1721630910.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-07-22T22:10:24.750+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f2ae123a940>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'I have a conundrum with how knowledge in DE teams works. I\'ve been working in Data Engineering for almost 2 years now and I don\'t think I will ever get the full understanding of what I like to call "the pipeline multiverse". \n\nOkay, so most people always talk about ETL or ELT or some combination of those two. But normally enterprise code pipelines feel more like EEEETTTTLLLL and mostly overcomplicated legacy code in a bunch of cases, not saying that is bad (because we are not going to refactor everything) but it always tends to boil down to "guess who possesses the tribal knowledge for creating a good enough solution under the pipeline construction creation".\n\nMy question would be, in those cases do we just accept tribal knowledge is acceptable and inheriting a project will always come at a cost, or we should think about avoiding keeping knowledge to ourselves and if so, how would be good solutions for it?', 'author_fullname': 't2_94dnqqrs', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'To Tribal Knowledge or not to Tribal Knowledge.', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1e9ky4y', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.63, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 2, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 2, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1721671526.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>I have a conundrum with how knowledge in DE teams works. I&#39;ve been working in Data Engineering for almost 2 years now and I don&#39;t think I will ever get the full understanding of what I like to call &quot;the pipeline multiverse&quot;. </p>\n\n<p>Okay, so most people always talk about ETL or ELT or some combination of those two. But normally enterprise code pipelines feel more like EEEETTTTLLLL and mostly overcomplicated legacy code in a bunch of cases, not saying that is bad (because we are not going to refactor everything) but it always tends to boil down to &quot;guess who possesses the tribal knowledge for creating a good enough solution under the pipeline construction creation&quot;.</p>\n\n<p>My question would be, in those cases do we just accept tribal knowledge is acceptable and inheriting a project will always come at a cost, or we should think about avoiding keeping knowledge to ourselves and if so, how would be good solutions for it?</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1e9ky4y', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Technical-Rip9688'), 'discussion_type': None, 'num_comments': 4, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1e9ky4y/to_tribal_knowledge_or_not_to_tribal_knowledge/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1e9ky4y/to_tribal_knowledge_or_not_to_tribal_knowledge/', 'subreddit_subscribers': 198788, 'created_utc': 1721671526.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-07-22T22:10:24.750+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f2ae123a940>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "\nHi everyone,\n\nI'm researching metadata stores that can work seamlessly across multiple cloud providers (GCP, AWS, Azure) and data platforms (Snowflake, Databricks).\n\nMy key requirements are:\n\nBroad Compatibility: I need a store that can catalog metadata from a wide range of sources, including data warehouses, data lakes, and even streaming platforms.\nCross-Platform Querying: The ability to query the metadata store directly from various platforms is essential. Ideally, I'd like to be able to run queries from Snowflake, Databricks, etc., without needing to move the metadata around.\nAPI-Driven: A robust API is important for integration with other tools in my data ecosystem.\nExample Use Case:\n\nLet's say I have some data in BigQuery (GCP) and some in Snowflake. I'd like to be able to:\n\nStore metadata about both datasets in the same metadata store.\nRun a query from Snowflake that analyzes metadata about both the Snowflake and BigQuery data.\nDo the same from BigQuery, querying metadata about both sources.\nHas anyone had experience with a metadata store that meets these criteria? Any recommendations or insights would be greatly appreciated!", 'author_fullname': 't2_vbotnpj1', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Choosing a Cloud-Agnostic Metadata Store with Cross-Platform Querying', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': True, 'name': 't3_1e9p3b4', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 2, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 2, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1721681500.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hi everyone,</p>\n\n<p>I&#39;m researching metadata stores that can work seamlessly across multiple cloud providers (GCP, AWS, Azure) and data platforms (Snowflake, Databricks).</p>\n\n<p>My key requirements are:</p>\n\n<p>Broad Compatibility: I need a store that can catalog metadata from a wide range of sources, including data warehouses, data lakes, and even streaming platforms.\nCross-Platform Querying: The ability to query the metadata store directly from various platforms is essential. Ideally, I&#39;d like to be able to run queries from Snowflake, Databricks, etc., without needing to move the metadata around.\nAPI-Driven: A robust API is important for integration with other tools in my data ecosystem.\nExample Use Case:</p>\n\n<p>Let&#39;s say I have some data in BigQuery (GCP) and some in Snowflake. I&#39;d like to be able to:</p>\n\n<p>Store metadata about both datasets in the same metadata store.\nRun a query from Snowflake that analyzes metadata about both the Snowflake and BigQuery data.\nDo the same from BigQuery, querying metadata about both sources.\nHas anyone had experience with a metadata store that meets these criteria? Any recommendations or insights would be greatly appreciated!</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1e9p3b4', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='SeaworthinessDue8121'), 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1e9p3b4/choosing_a_cloudagnostic_metadata_store_with/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1e9p3b4/choosing_a_cloudagnostic_metadata_store_with/', 'subreddit_subscribers': 198788, 'created_utc': 1721681500.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-07-22T22:10:24.751+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f2ae123a940>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "I'm currently working on an AWS Glue job that loads data from a legacy relational database into Amazon Redshift. I'm trying to understand how AWS Glue distributes its workloads across Data Processing Units (DPUs).\n\nMy specific questions are:\n\n1. Does AWS Glue automatically handle the distribution of workloads across DPUs, or is there any configuration required on my part?\n2. Are there any best practices or tips for optimizing the use of DPUs in Glue jobs?\n\nAny insights or resources you can share would be greatly appreciated. Thanks in advance!", 'author_fullname': 't2_jerod9h7', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'How does AWS Glue distribute workloads across DPUs?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1e9klzp', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.6, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 1, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 1, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1721670713.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>I&#39;m currently working on an AWS Glue job that loads data from a legacy relational database into Amazon Redshift. I&#39;m trying to understand how AWS Glue distributes its workloads across Data Processing Units (DPUs).</p>\n\n<p>My specific questions are:</p>\n\n<ol>\n<li>Does AWS Glue automatically handle the distribution of workloads across DPUs, or is there any configuration required on my part?</li>\n<li>Are there any best practices or tips for optimizing the use of DPUs in Glue jobs?</li>\n</ol>\n\n<p>Any insights or resources you can share would be greatly appreciated. Thanks in advance!</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1e9klzp', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='naruto_believe12'), 'discussion_type': None, 'num_comments': 2, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1e9klzp/how_does_aws_glue_distribute_workloads_across_dpus/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1e9klzp/how_does_aws_glue_distribute_workloads_across_dpus/', 'subreddit_subscribers': 198788, 'created_utc': 1721670713.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-07-22T22:10:24.751+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f2ae123a940>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Those of you who have adopted a data lakehouse architecture, how did you justify the need for it in terms of solving business issues- not tech issues? In other words, what use cases are you solving for the business/stakeholders by adopting a solution like Databricks or Big Query? I get these products from a technical perspective. Im interested to hear how you translated all these shiny features into something that drove your company forward (ie, reduced risk, revenue growth, reduced costs, better customer acquisition, higher customer conversion etc) thx! \n', 'author_fullname': 't2_wkqzw', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Line of Business Use Cases for Data Lakehouse', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1e9jfhf', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.6, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 1, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 1, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1721667839.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Those of you who have adopted a data lakehouse architecture, how did you justify the need for it in terms of solving business issues- not tech issues? In other words, what use cases are you solving for the business/stakeholders by adopting a solution like Databricks or Big Query? I get these products from a technical perspective. Im interested to hear how you translated all these shiny features into something that drove your company forward (ie, reduced risk, revenue growth, reduced costs, better customer acquisition, higher customer conversion etc) thx! </p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1e9jfhf', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='markaaronfox'), 'discussion_type': None, 'num_comments': 1, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1e9jfhf/line_of_business_use_cases_for_data_lakehouse/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1e9jfhf/line_of_business_use_cases_for_data_lakehouse/', 'subreddit_subscribers': 198788, 'created_utc': 1721667839.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-07-22T22:10:24.751+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f2ae123a940>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'We hear the term "Metadata Management" used a lot but there are a lot of types of metadata.\n\n\\- User facing data about datasets (think data catalogs like colibra and alation)\n\n\\- metadata of datasets used to optimize queries at the engine level (Think Iceberg, Delta Lake, Hudi)\n\netc.  \n\n\nWhen you think Metadata Management is one of the most important challenges, what do you think of it as?\n\n[View Poll](https://www.reddit.com/poll/1e9q0k9)', 'author_fullname': 't2_lnwagoki', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'What does "Metadata Management" Mean to you?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': True, 'name': 't3_1e9q0k9', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.5, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 0, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 0, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1721683728.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>We hear the term &quot;Metadata Management&quot; used a lot but there are a lot of types of metadata.</p>\n\n<p>- User facing data about datasets (think data catalogs like colibra and alation)</p>\n\n<p>- metadata of datasets used to optimize queries at the engine level (Think Iceberg, Delta Lake, Hudi)</p>\n\n<p>etc.  </p>\n\n<p>When you think Metadata Management is one of the most important challenges, what do you think of it as?</p>\n\n<p><a href="https://www.reddit.com/poll/1e9q0k9">View Poll</a></p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1e9q0k9', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='AMDataLake'), 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'poll_data': <praw.models.reddit.poll.PollData object at 0x7f2ae116b400>, 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1e9q0k9/what_does_metadata_management_mean_to_you/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'mod_reports': [], 'url': 'https://www.reddit.com/r/dataengineering/comments/1e9q0k9/what_does_metadata_management_mean_to_you/', 'subreddit_subscribers': 198788, 'created_utc': 1721683728.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-07-22T22:10:24.752+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f2ae123a940>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': '', 'author_fullname': 't2_cftg5xf7w', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': '11 DataBricks Tricks That Will Blow Your Mind', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': 93, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1e99wsp', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.27, 'author_flair_background_color': None, 'ups': 0, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Blog', 'can_mod_post': False, 'score': 0, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'https://b.thumbs.redditmedia.com/6M6Fu4TuQr832aQR21gy0lup59fMsKHVWDuy_Q9u87M.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'link', 'content_categories': None, 'is_self': False, 'subreddit_type': 'public', 'created': 1721640245.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'overcast.blog', 'allow_live_comments': False, 'selftext_html': None, 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'url_overridden_by_dest': 'https://overcast.blog/11-databricks-tricks-that-will-blow-your-mind-411226e1ac5b', 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/IZ8Zy8o0FXWaRI0zVER5tynxugqyRFPENHUjwpIyMYQ.jpg?auto=webp&s=e349e29d78fb504145a8a5182daa6d84407cb83a', 'width': 1010, 'height': 672}, 'resolutions': [{'url': 'https://external-preview.redd.it/IZ8Zy8o0FXWaRI0zVER5tynxugqyRFPENHUjwpIyMYQ.jpg?width=108&crop=smart&auto=webp&s=28b315f5c660ac7ddbf394d3420a2be80799bb57', 'width': 108, 'height': 71}, {'url': 'https://external-preview.redd.it/IZ8Zy8o0FXWaRI0zVER5tynxugqyRFPENHUjwpIyMYQ.jpg?width=216&crop=smart&auto=webp&s=97e8347c036439a67f46f8f92b8e815518740af0', 'width': 216, 'height': 143}, {'url': 'https://external-preview.redd.it/IZ8Zy8o0FXWaRI0zVER5tynxugqyRFPENHUjwpIyMYQ.jpg?width=320&crop=smart&auto=webp&s=90b6f5d1b43119485e910610e1df3cfdf504e83a', 'width': 320, 'height': 212}, {'url': 'https://external-preview.redd.it/IZ8Zy8o0FXWaRI0zVER5tynxugqyRFPENHUjwpIyMYQ.jpg?width=640&crop=smart&auto=webp&s=bcbbcbcf39b21b714e62212b98bb6cf6d9ae87a9', 'width': 640, 'height': 425}, {'url': 'https://external-preview.redd.it/IZ8Zy8o0FXWaRI0zVER5tynxugqyRFPENHUjwpIyMYQ.jpg?width=960&crop=smart&auto=webp&s=79fe886322b40e55e05d449cc944a21d03a6beb2', 'width': 960, 'height': 638}], 'variants': {}, 'id': 'bKmij5b_SqxxUIVRtyDBKiPhiPbv69C6bp8tEB8j29Q'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': 'eb739554-a7db-11eb-95d7-0ec0f8f30313', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'mod_note': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'num_reports': None, 'removal_reason': None, 'link_flair_background_color': '#0079d3', 'id': '1e99wsp', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='codingdecently'), 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1e99wsp/11_databricks_tricks_that_will_blow_your_mind/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://overcast.blog/11-databricks-tricks-that-will-blow-your-mind-411226e1ac5b', 'subreddit_subscribers': 198788, 'created_utc': 1721640245.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-07-22T22:10:24.752+0000] {python.py:237} INFO - Done. Returned value was: None
[2024-07-22T22:10:24.753+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-07-22T22:10:24.761+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=etl_reddit_pipeline, task_id=reddit_extraction, run_id=manual__2024-07-22T22:10:22.777211+00:00, execution_date=20240722T221022, start_date=20240722T221024, end_date=20240722T221024
[2024-07-22T22:10:24.799+0000] {local_task_job_runner.py:243} INFO - Task exited with return code 0
[2024-07-22T22:10:24.817+0000] {taskinstance.py:3503} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-07-22T22:10:24.819+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
