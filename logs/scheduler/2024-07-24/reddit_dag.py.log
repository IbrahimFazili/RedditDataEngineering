[2024-07-24T00:00:15.288+0000] {processor.py:161} INFO - Started process (PID=3797) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:00:15.289+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T00:00:15.290+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:00:15.290+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:00:15.388+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:00:15.389+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:00:15.441+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:00:15.441+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:00:15.609+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T00:00:15.609+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T00:00:15.617+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:00:15.841+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:00:15.841+0000] {dag.py:3089} INFO - Sync 1 DAGs
[2024-07-24T00:00:15.859+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:00:15.859+0000] {dag.py:3947} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24 00:00:00+00:00, run_after=2024-07-25 00:00:00+00:00
[2024-07-24T00:00:15.876+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.595 seconds
[2024-07-24T00:00:46.081+0000] {processor.py:161} INFO - Started process (PID=3799) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:00:46.085+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T00:00:46.092+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:00:46.091+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:00:46.327+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:00:46.328+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:00:46.417+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:00:46.418+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:00:46.741+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T00:00:46.742+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T00:00:46.757+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:00:46.796+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:00:46.795+0000] {dag.py:3089} INFO - Sync 1 DAGs
[2024-07-24T00:00:46.830+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:00:46.830+0000] {dag.py:3947} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24 00:00:00+00:00, run_after=2024-07-25 00:00:00+00:00
[2024-07-24T00:00:46.848+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.785 seconds
[2024-07-24T00:01:17.025+0000] {processor.py:161} INFO - Started process (PID=3801) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:01:17.026+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T00:01:17.028+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:01:17.028+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:01:17.124+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:01:17.124+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:01:17.164+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:01:17.164+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:01:17.325+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T00:01:17.325+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T00:01:17.331+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:01:17.353+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:01:17.352+0000] {dag.py:3089} INFO - Sync 1 DAGs
[2024-07-24T00:01:17.371+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:01:17.371+0000] {dag.py:3947} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24 00:00:00+00:00, run_after=2024-07-25 00:00:00+00:00
[2024-07-24T00:01:17.385+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.369 seconds
[2024-07-24T00:01:47.557+0000] {processor.py:161} INFO - Started process (PID=3803) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:01:47.558+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T00:01:47.559+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:01:47.559+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:01:47.648+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:01:47.649+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:01:47.695+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:01:47.695+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:01:47.863+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T00:01:47.864+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T00:01:47.872+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:01:47.906+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:01:47.905+0000] {dag.py:3089} INFO - Sync 1 DAGs
[2024-07-24T00:01:47.929+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:01:47.929+0000] {dag.py:3947} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24 00:00:00+00:00, run_after=2024-07-25 00:00:00+00:00
[2024-07-24T00:01:47.946+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.396 seconds
[2024-07-24T00:02:18.113+0000] {processor.py:161} INFO - Started process (PID=3805) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:02:18.114+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T00:02:18.116+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:02:18.116+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:02:18.207+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:02:18.208+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:02:18.248+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:02:18.249+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:02:18.406+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T00:02:18.407+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T00:02:18.414+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:02:18.434+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:02:18.434+0000] {dag.py:3089} INFO - Sync 1 DAGs
[2024-07-24T00:02:18.451+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:02:18.451+0000] {dag.py:3947} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24 00:00:00+00:00, run_after=2024-07-25 00:00:00+00:00
[2024-07-24T00:02:18.470+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.364 seconds
[2024-07-24T00:02:48.634+0000] {processor.py:161} INFO - Started process (PID=3807) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:02:48.635+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T00:02:48.637+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:02:48.637+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:02:48.724+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:02:48.724+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:02:48.762+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:02:48.763+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:02:48.914+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T00:02:48.914+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T00:02:48.921+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:02:48.948+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:02:48.947+0000] {dag.py:3089} INFO - Sync 1 DAGs
[2024-07-24T00:02:48.977+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:02:48.977+0000] {dag.py:3947} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24 00:00:00+00:00, run_after=2024-07-25 00:00:00+00:00
[2024-07-24T00:02:48.997+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.369 seconds
[2024-07-24T00:03:19.169+0000] {processor.py:161} INFO - Started process (PID=3809) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:03:19.170+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T00:03:19.172+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:03:19.171+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:03:19.257+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:03:19.257+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:03:19.297+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:03:19.297+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:03:19.448+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T00:03:19.448+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T00:03:19.455+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:03:19.488+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:03:19.487+0000] {dag.py:3089} INFO - Sync 1 DAGs
[2024-07-24T00:03:19.506+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:03:19.505+0000] {dag.py:3947} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24 00:00:00+00:00, run_after=2024-07-25 00:00:00+00:00
[2024-07-24T00:03:19.519+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.355 seconds
[2024-07-24T00:03:49.704+0000] {processor.py:161} INFO - Started process (PID=3811) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:03:49.705+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T00:03:49.707+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:03:49.706+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:03:49.791+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:03:49.792+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:03:49.830+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:03:49.830+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:03:49.977+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T00:03:49.977+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T00:03:49.984+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:03:50.008+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:03:50.007+0000] {dag.py:3089} INFO - Sync 1 DAGs
[2024-07-24T00:03:50.025+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:03:50.025+0000] {dag.py:3947} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24 00:00:00+00:00, run_after=2024-07-25 00:00:00+00:00
[2024-07-24T00:03:50.046+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.347 seconds
[2024-07-24T00:04:20.210+0000] {processor.py:161} INFO - Started process (PID=3813) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:04:20.211+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T00:04:20.213+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:04:20.212+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:04:20.300+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:04:20.300+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:04:20.341+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:04:20.341+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:04:20.499+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T00:04:20.499+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T00:04:20.506+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:04:20.529+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:04:20.529+0000] {dag.py:3089} INFO - Sync 1 DAGs
[2024-07-24T00:04:20.549+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:04:20.549+0000] {dag.py:3947} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24 00:00:00+00:00, run_after=2024-07-25 00:00:00+00:00
[2024-07-24T00:04:20.566+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.361 seconds
[2024-07-24T00:04:50.733+0000] {processor.py:161} INFO - Started process (PID=3815) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:04:50.734+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T00:04:50.737+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:04:50.736+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:04:50.825+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:04:50.826+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:04:50.865+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:04:50.865+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:04:51.027+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T00:04:51.028+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T00:04:51.035+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:04:51.059+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:04:51.058+0000] {dag.py:3089} INFO - Sync 1 DAGs
[2024-07-24T00:04:51.077+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:04:51.077+0000] {dag.py:3947} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24 00:00:00+00:00, run_after=2024-07-25 00:00:00+00:00
[2024-07-24T00:04:51.094+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.367 seconds
[2024-07-24T00:05:21.254+0000] {processor.py:161} INFO - Started process (PID=3817) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:05:21.255+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T00:05:21.258+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:05:21.257+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:05:21.343+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:05:21.343+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:05:21.382+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:05:21.382+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:05:21.544+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T00:05:21.545+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T00:05:21.552+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:05:21.574+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:05:21.573+0000] {dag.py:3089} INFO - Sync 1 DAGs
[2024-07-24T00:05:21.596+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:05:21.596+0000] {dag.py:3947} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24 00:00:00+00:00, run_after=2024-07-25 00:00:00+00:00
[2024-07-24T00:05:21.615+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.368 seconds
[2024-07-24T00:05:51.782+0000] {processor.py:161} INFO - Started process (PID=3819) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:05:51.783+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T00:05:51.784+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:05:51.784+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:05:51.868+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:05:51.869+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:05:51.909+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:05:51.909+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:05:52.064+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T00:05:52.064+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T00:05:52.072+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:05:52.094+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:05:52.094+0000] {dag.py:3089} INFO - Sync 1 DAGs
[2024-07-24T00:05:52.115+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:05:52.115+0000] {dag.py:3947} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24 00:00:00+00:00, run_after=2024-07-25 00:00:00+00:00
[2024-07-24T00:05:52.135+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.360 seconds
[2024-07-24T00:06:22.296+0000] {processor.py:161} INFO - Started process (PID=3821) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:06:22.297+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T00:06:22.298+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:06:22.298+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:06:22.378+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:06:22.379+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:06:22.414+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:06:22.414+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:06:22.573+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T00:06:22.574+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T00:06:22.580+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:06:22.605+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:06:22.604+0000] {dag.py:3089} INFO - Sync 1 DAGs
[2024-07-24T00:06:22.636+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:06:22.636+0000] {dag.py:3947} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24 00:00:00+00:00, run_after=2024-07-25 00:00:00+00:00
[2024-07-24T00:06:22.659+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.368 seconds
[2024-07-24T00:06:52.824+0000] {processor.py:161} INFO - Started process (PID=3823) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:06:52.825+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T00:06:52.826+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:06:52.826+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:06:52.906+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:06:52.906+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:06:52.942+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:06:52.942+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:06:53.083+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T00:06:53.084+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T00:06:53.090+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:06:53.115+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:06:53.114+0000] {dag.py:3089} INFO - Sync 1 DAGs
[2024-07-24T00:06:53.135+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:06:53.134+0000] {dag.py:3947} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24 00:00:00+00:00, run_after=2024-07-25 00:00:00+00:00
[2024-07-24T00:06:53.152+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.333 seconds
[2024-07-24T00:07:23.312+0000] {processor.py:161} INFO - Started process (PID=3825) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:07:23.313+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T00:07:23.314+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:07:23.314+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:07:23.394+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:07:23.394+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:07:23.432+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:07:23.433+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:07:23.582+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T00:07:23.582+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T00:07:23.589+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:07:23.611+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:07:23.611+0000] {dag.py:3089} INFO - Sync 1 DAGs
[2024-07-24T00:07:23.630+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:07:23.629+0000] {dag.py:3947} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24 00:00:00+00:00, run_after=2024-07-25 00:00:00+00:00
[2024-07-24T00:07:23.643+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.337 seconds
[2024-07-24T00:07:53.803+0000] {processor.py:161} INFO - Started process (PID=3827) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:07:53.804+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T00:07:53.806+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:07:53.806+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:07:53.895+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:07:53.895+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:07:53.939+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:07:53.939+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:07:54.103+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T00:07:54.104+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T00:07:54.110+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:07:54.131+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:07:54.130+0000] {dag.py:3089} INFO - Sync 1 DAGs
[2024-07-24T00:07:54.152+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:07:54.151+0000] {dag.py:3947} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24 00:00:00+00:00, run_after=2024-07-25 00:00:00+00:00
[2024-07-24T00:07:54.170+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.373 seconds
[2024-07-24T00:08:24.332+0000] {processor.py:161} INFO - Started process (PID=3829) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:08:24.333+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T00:08:24.335+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:08:24.335+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:08:24.422+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:08:24.422+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:08:24.463+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:08:24.464+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:08:24.622+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T00:08:24.623+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T00:08:24.629+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:08:24.653+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:08:24.653+0000] {dag.py:3089} INFO - Sync 1 DAGs
[2024-07-24T00:08:24.673+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:08:24.672+0000] {dag.py:3947} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24 00:00:00+00:00, run_after=2024-07-25 00:00:00+00:00
[2024-07-24T00:08:24.687+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.360 seconds
[2024-07-24T00:08:54.851+0000] {processor.py:161} INFO - Started process (PID=3831) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:08:54.852+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T00:08:54.853+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:08:54.853+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:08:54.935+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:08:54.936+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:08:54.971+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:08:54.971+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:08:55.130+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T00:08:55.131+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T00:08:55.139+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:08:55.161+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:08:55.160+0000] {dag.py:3089} INFO - Sync 1 DAGs
[2024-07-24T00:08:55.178+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:08:55.178+0000] {dag.py:3947} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24 00:00:00+00:00, run_after=2024-07-25 00:00:00+00:00
[2024-07-24T00:08:55.193+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.347 seconds
[2024-07-24T00:09:25.361+0000] {processor.py:161} INFO - Started process (PID=3833) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:09:25.361+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T00:09:25.363+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:09:25.363+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:09:25.452+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:09:25.452+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:09:25.493+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:09:25.493+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:09:25.651+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T00:09:25.652+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T00:09:25.660+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:09:25.685+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:09:25.684+0000] {dag.py:3089} INFO - Sync 1 DAGs
[2024-07-24T00:09:25.706+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:09:25.705+0000] {dag.py:3947} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24 00:00:00+00:00, run_after=2024-07-25 00:00:00+00:00
[2024-07-24T00:09:25.720+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.365 seconds
[2024-07-24T00:09:55.884+0000] {processor.py:161} INFO - Started process (PID=3835) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:09:55.885+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T00:09:55.886+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:09:55.886+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:09:55.970+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:09:55.970+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:09:56.011+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:09:56.011+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:09:56.169+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T00:09:56.170+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T00:09:56.177+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:09:56.198+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:09:56.198+0000] {dag.py:3089} INFO - Sync 1 DAGs
[2024-07-24T00:09:56.217+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:09:56.217+0000] {dag.py:3947} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24 00:00:00+00:00, run_after=2024-07-25 00:00:00+00:00
[2024-07-24T00:09:56.234+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.357 seconds
[2024-07-24T00:10:26.411+0000] {processor.py:161} INFO - Started process (PID=3837) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:10:26.412+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T00:10:26.414+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:10:26.414+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:10:26.506+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:10:26.506+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:10:26.551+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:10:26.551+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:10:26.755+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T00:10:26.756+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T00:10:26.765+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:10:26.791+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:10:26.791+0000] {dag.py:3089} INFO - Sync 1 DAGs
[2024-07-24T00:10:26.813+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:10:26.813+0000] {dag.py:3947} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24 00:00:00+00:00, run_after=2024-07-25 00:00:00+00:00
[2024-07-24T00:10:26.833+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.429 seconds
[2024-07-24T00:10:57.001+0000] {processor.py:161} INFO - Started process (PID=3839) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:10:57.002+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T00:10:57.003+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:10:57.003+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:10:57.083+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:10:57.083+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:10:57.121+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:10:57.121+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:10:57.264+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T00:10:57.265+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T00:10:57.271+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:10:57.292+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:10:57.292+0000] {dag.py:3089} INFO - Sync 1 DAGs
[2024-07-24T00:10:57.310+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:10:57.309+0000] {dag.py:3947} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24 00:00:00+00:00, run_after=2024-07-25 00:00:00+00:00
[2024-07-24T00:10:57.331+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.336 seconds
[2024-07-24T00:11:27.500+0000] {processor.py:161} INFO - Started process (PID=3841) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:11:27.501+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T00:11:27.503+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:11:27.502+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:11:27.600+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:11:27.601+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:11:27.645+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:11:27.646+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:11:27.811+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T00:11:27.812+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T00:11:27.819+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:11:27.842+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:11:27.841+0000] {dag.py:3089} INFO - Sync 1 DAGs
[2024-07-24T00:11:27.865+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:11:27.865+0000] {dag.py:3947} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24 00:00:00+00:00, run_after=2024-07-25 00:00:00+00:00
[2024-07-24T00:11:27.881+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.387 seconds
[2024-07-24T00:11:58.076+0000] {processor.py:161} INFO - Started process (PID=3843) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:11:58.078+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T00:11:58.081+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:11:58.080+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:11:58.187+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:11:58.188+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:11:58.252+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:11:58.253+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:11:58.517+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T00:11:58.518+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T00:11:58.527+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:11:58.560+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:11:58.559+0000] {dag.py:3089} INFO - Sync 1 DAGs
[2024-07-24T00:11:58.586+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:11:58.585+0000] {dag.py:3947} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24 00:00:00+00:00, run_after=2024-07-25 00:00:00+00:00
[2024-07-24T00:11:58.613+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.544 seconds
[2024-07-24T00:12:28.814+0000] {processor.py:161} INFO - Started process (PID=3845) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:12:28.815+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T00:12:28.817+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:12:28.817+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:12:28.910+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:12:28.911+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:12:28.955+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:12:28.956+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:12:29.129+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T00:12:29.130+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T00:12:29.138+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:12:29.163+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:12:29.163+0000] {dag.py:3089} INFO - Sync 1 DAGs
[2024-07-24T00:12:29.189+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:12:29.189+0000] {dag.py:3947} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24 00:00:00+00:00, run_after=2024-07-25 00:00:00+00:00
[2024-07-24T00:12:29.208+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.400 seconds
[2024-07-24T00:12:59.399+0000] {processor.py:161} INFO - Started process (PID=3847) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:12:59.400+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T00:12:59.402+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:12:59.402+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:12:59.490+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:12:59.490+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:12:59.534+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:12:59.534+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:12:59.699+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T00:12:59.699+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T00:12:59.706+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:12:59.730+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:12:59.730+0000] {dag.py:3089} INFO - Sync 1 DAGs
[2024-07-24T00:12:59.751+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:12:59.751+0000] {dag.py:3947} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24 00:00:00+00:00, run_after=2024-07-25 00:00:00+00:00
[2024-07-24T00:12:59.768+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.377 seconds
[2024-07-24T00:13:29.941+0000] {processor.py:161} INFO - Started process (PID=3849) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:13:29.942+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T00:13:29.943+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:13:29.943+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:13:30.031+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:13:30.032+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:13:30.070+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:13:30.071+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:13:30.224+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T00:13:30.225+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T00:13:30.232+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:13:30.255+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:13:30.254+0000] {dag.py:3089} INFO - Sync 1 DAGs
[2024-07-24T00:13:30.279+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:13:30.278+0000] {dag.py:3947} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24 00:00:00+00:00, run_after=2024-07-25 00:00:00+00:00
[2024-07-24T00:13:30.296+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.360 seconds
[2024-07-24T00:14:00.473+0000] {processor.py:161} INFO - Started process (PID=3851) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:14:00.474+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T00:14:00.476+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:14:00.476+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:14:00.569+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:14:00.569+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:14:00.614+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:14:00.614+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:14:00.778+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T00:14:00.779+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T00:14:00.788+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:14:00.813+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:14:00.813+0000] {dag.py:3089} INFO - Sync 1 DAGs
[2024-07-24T00:14:00.831+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:14:00.831+0000] {dag.py:3947} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24 00:00:00+00:00, run_after=2024-07-25 00:00:00+00:00
[2024-07-24T00:14:00.851+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.385 seconds
[2024-07-24T00:14:31.018+0000] {processor.py:161} INFO - Started process (PID=3853) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:14:31.019+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T00:14:31.020+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:14:31.020+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:14:31.114+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:14:31.114+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:14:31.155+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:14:31.155+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:14:31.313+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T00:14:31.313+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T00:14:31.320+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:14:31.341+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:14:31.341+0000] {dag.py:3089} INFO - Sync 1 DAGs
[2024-07-24T00:14:31.363+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:14:31.363+0000] {dag.py:3947} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24 00:00:00+00:00, run_after=2024-07-25 00:00:00+00:00
[2024-07-24T00:14:31.378+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.366 seconds
[2024-07-24T00:15:01.559+0000] {processor.py:161} INFO - Started process (PID=3855) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:15:01.560+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T00:15:01.562+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:15:01.561+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:15:01.657+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:15:01.658+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:15:01.701+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:15:01.702+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:15:01.877+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T00:15:01.878+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T00:15:01.886+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:15:01.918+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:15:01.917+0000] {dag.py:3089} INFO - Sync 1 DAGs
[2024-07-24T00:15:01.940+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:15:01.940+0000] {dag.py:3947} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24 00:00:00+00:00, run_after=2024-07-25 00:00:00+00:00
[2024-07-24T00:15:01.957+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.404 seconds
[2024-07-24T00:15:32.135+0000] {processor.py:161} INFO - Started process (PID=3857) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:15:32.136+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T00:15:32.138+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:15:32.138+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:15:32.228+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:15:32.229+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:15:32.275+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:15:32.276+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:15:32.435+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T00:15:32.436+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T00:15:32.443+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:15:32.467+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:15:32.466+0000] {dag.py:3089} INFO - Sync 1 DAGs
[2024-07-24T00:15:32.485+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:15:32.485+0000] {dag.py:3947} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24 00:00:00+00:00, run_after=2024-07-25 00:00:00+00:00
[2024-07-24T00:15:32.500+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.372 seconds
[2024-07-24T00:16:02.674+0000] {processor.py:161} INFO - Started process (PID=3859) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:16:02.675+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T00:16:02.676+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:16:02.676+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:16:02.767+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:16:02.768+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:16:02.823+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:16:02.824+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:16:02.994+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T00:16:02.994+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T00:16:03.002+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:16:03.025+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:16:03.024+0000] {dag.py:3089} INFO - Sync 1 DAGs
[2024-07-24T00:16:03.052+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:16:03.052+0000] {dag.py:3947} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24 00:00:00+00:00, run_after=2024-07-25 00:00:00+00:00
[2024-07-24T00:16:03.073+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.406 seconds
[2024-07-24T00:16:33.246+0000] {processor.py:161} INFO - Started process (PID=3861) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:16:33.248+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T00:16:33.250+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:16:33.250+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:16:33.347+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:16:33.348+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:16:33.393+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:16:33.393+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:16:33.550+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T00:16:33.551+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T00:16:33.557+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:16:33.588+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:16:33.588+0000] {dag.py:3089} INFO - Sync 1 DAGs
[2024-07-24T00:16:33.608+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:16:33.608+0000] {dag.py:3947} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24 00:00:00+00:00, run_after=2024-07-25 00:00:00+00:00
[2024-07-24T00:16:33.634+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.394 seconds
[2024-07-24T00:17:03.805+0000] {processor.py:161} INFO - Started process (PID=3863) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:17:03.806+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T00:17:03.807+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:17:03.807+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:17:03.896+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:17:03.897+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:17:03.936+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:17:03.937+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:17:04.099+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T00:17:04.099+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T00:17:04.106+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:17:04.127+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:17:04.127+0000] {dag.py:3089} INFO - Sync 1 DAGs
[2024-07-24T00:17:04.147+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:17:04.146+0000] {dag.py:3947} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24 00:00:00+00:00, run_after=2024-07-25 00:00:00+00:00
[2024-07-24T00:17:04.162+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.363 seconds
[2024-07-24T00:17:34.338+0000] {processor.py:161} INFO - Started process (PID=3865) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:17:34.340+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T00:17:34.342+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:17:34.341+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:17:34.440+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:17:34.441+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:17:34.485+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:17:34.486+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:17:34.656+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T00:17:34.657+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T00:17:34.664+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:17:34.690+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:17:34.689+0000] {dag.py:3089} INFO - Sync 1 DAGs
[2024-07-24T00:17:34.710+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:17:34.710+0000] {dag.py:3947} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24 00:00:00+00:00, run_after=2024-07-25 00:00:00+00:00
[2024-07-24T00:17:34.728+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.396 seconds
[2024-07-24T00:18:04.903+0000] {processor.py:161} INFO - Started process (PID=3867) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:18:04.904+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T00:18:04.905+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:18:04.905+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:18:04.994+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:18:04.995+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:18:05.037+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:18:05.038+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:18:05.195+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T00:18:05.195+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T00:18:05.202+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:18:05.224+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:18:05.224+0000] {dag.py:3089} INFO - Sync 1 DAGs
[2024-07-24T00:18:05.243+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:18:05.242+0000] {dag.py:3947} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24 00:00:00+00:00, run_after=2024-07-25 00:00:00+00:00
[2024-07-24T00:18:05.257+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.361 seconds
[2024-07-24T00:18:35.432+0000] {processor.py:161} INFO - Started process (PID=3869) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:18:35.433+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T00:18:35.435+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:18:35.434+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:18:35.539+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:18:35.540+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:18:35.583+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:18:35.584+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:18:35.755+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T00:18:35.755+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T00:18:35.764+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:18:35.792+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:18:35.791+0000] {dag.py:3089} INFO - Sync 1 DAGs
[2024-07-24T00:18:35.814+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:18:35.814+0000] {dag.py:3947} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24 00:00:00+00:00, run_after=2024-07-25 00:00:00+00:00
[2024-07-24T00:18:35.834+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.410 seconds
[2024-07-24T00:19:06.027+0000] {processor.py:161} INFO - Started process (PID=3871) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:19:06.028+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T00:19:06.031+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:19:06.030+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:19:06.127+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:19:06.127+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:19:06.175+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:19:06.176+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:19:06.365+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T00:19:06.365+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T00:19:06.373+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:19:06.402+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:19:06.401+0000] {dag.py:3089} INFO - Sync 1 DAGs
[2024-07-24T00:19:06.425+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:19:06.425+0000] {dag.py:3947} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24 00:00:00+00:00, run_after=2024-07-25 00:00:00+00:00
[2024-07-24T00:19:06.444+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.425 seconds
[2024-07-24T00:19:36.619+0000] {processor.py:161} INFO - Started process (PID=3873) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:19:36.620+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T00:19:36.621+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:19:36.621+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:19:36.712+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:19:36.713+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:19:36.757+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:19:36.757+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:19:36.933+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T00:19:36.934+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T00:19:36.940+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:19:36.964+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:19:36.963+0000] {dag.py:3089} INFO - Sync 1 DAGs
[2024-07-24T00:19:36.985+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:19:36.985+0000] {dag.py:3947} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24 00:00:00+00:00, run_after=2024-07-25 00:00:00+00:00
[2024-07-24T00:19:37.003+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.391 seconds
[2024-07-24T00:20:07.186+0000] {processor.py:161} INFO - Started process (PID=3875) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:20:07.187+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T00:20:07.189+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:20:07.188+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:20:07.280+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:20:07.281+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:20:07.323+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:20:07.323+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:20:07.514+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T00:20:07.515+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T00:20:07.522+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:20:07.549+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:20:07.548+0000] {dag.py:3089} INFO - Sync 1 DAGs
[2024-07-24T00:20:07.567+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:20:07.567+0000] {dag.py:3947} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24 00:00:00+00:00, run_after=2024-07-25 00:00:00+00:00
[2024-07-24T00:20:07.589+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.409 seconds
[2024-07-24T00:20:37.767+0000] {processor.py:161} INFO - Started process (PID=3877) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:20:37.768+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T00:20:37.770+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:20:37.769+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:20:37.858+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:20:37.859+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:20:37.902+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:20:37.903+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:20:38.077+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T00:20:38.077+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T00:20:38.084+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:20:38.110+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:20:38.110+0000] {dag.py:3089} INFO - Sync 1 DAGs
[2024-07-24T00:20:38.133+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:20:38.132+0000] {dag.py:3947} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24 00:00:00+00:00, run_after=2024-07-25 00:00:00+00:00
[2024-07-24T00:20:38.151+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.389 seconds
[2024-07-24T00:21:08.333+0000] {processor.py:161} INFO - Started process (PID=3879) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:21:08.335+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T00:21:08.337+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:21:08.336+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:21:08.458+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:21:08.459+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:21:08.513+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:21:08.513+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:21:08.673+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T00:21:08.673+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T00:21:08.680+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:21:08.707+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:21:08.706+0000] {dag.py:3089} INFO - Sync 1 DAGs
[2024-07-24T00:21:08.730+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:21:08.730+0000] {dag.py:3947} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24 00:00:00+00:00, run_after=2024-07-25 00:00:00+00:00
[2024-07-24T00:21:08.752+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.425 seconds
[2024-07-24T00:21:38.927+0000] {processor.py:161} INFO - Started process (PID=3881) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:21:38.928+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T00:21:38.930+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:21:38.929+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:21:39.024+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:21:39.025+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:21:39.066+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:21:39.067+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:21:39.224+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T00:21:39.224+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T00:21:39.231+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:21:39.256+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:21:39.255+0000] {dag.py:3089} INFO - Sync 1 DAGs
[2024-07-24T00:21:39.281+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:21:39.280+0000] {dag.py:3947} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24 00:00:00+00:00, run_after=2024-07-25 00:00:00+00:00
[2024-07-24T00:21:39.297+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.377 seconds
[2024-07-24T00:22:09.476+0000] {processor.py:161} INFO - Started process (PID=3883) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:22:09.477+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T00:22:09.478+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:22:09.478+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:22:09.566+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:22:09.566+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:22:09.607+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:22:09.608+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:22:09.766+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T00:22:09.766+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T00:22:09.772+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:22:09.801+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:22:09.801+0000] {dag.py:3089} INFO - Sync 1 DAGs
[2024-07-24T00:22:09.821+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:22:09.821+0000] {dag.py:3947} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24 00:00:00+00:00, run_after=2024-07-25 00:00:00+00:00
[2024-07-24T00:22:09.839+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.368 seconds
[2024-07-24T00:22:40.024+0000] {processor.py:161} INFO - Started process (PID=3885) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:22:40.025+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T00:22:40.027+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:22:40.026+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:22:40.113+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:22:40.114+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:22:40.158+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:22:40.158+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:22:40.314+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T00:22:40.315+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T00:22:40.321+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:22:40.343+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:22:40.343+0000] {dag.py:3089} INFO - Sync 1 DAGs
[2024-07-24T00:22:40.361+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:22:40.361+0000] {dag.py:3947} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24 00:00:00+00:00, run_after=2024-07-25 00:00:00+00:00
[2024-07-24T00:22:40.375+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.358 seconds
[2024-07-24T00:23:10.561+0000] {processor.py:161} INFO - Started process (PID=3887) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:23:10.562+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T00:23:10.564+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:23:10.564+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:23:10.667+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:23:10.668+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:23:10.713+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:23:10.714+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:23:10.878+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T00:23:10.878+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T00:23:10.886+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:23:10.914+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:23:10.913+0000] {dag.py:3089} INFO - Sync 1 DAGs
[2024-07-24T00:23:10.937+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:23:10.937+0000] {dag.py:3947} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24 00:00:00+00:00, run_after=2024-07-25 00:00:00+00:00
[2024-07-24T00:23:10.955+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.402 seconds
[2024-07-24T00:23:41.173+0000] {processor.py:161} INFO - Started process (PID=3889) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:23:41.174+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T00:23:41.176+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:23:41.175+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:23:41.267+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:23:41.268+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:23:41.309+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:23:41.310+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:23:41.474+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T00:23:41.475+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T00:23:41.482+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:23:41.505+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:23:41.505+0000] {dag.py:3089} INFO - Sync 1 DAGs
[2024-07-24T00:23:41.527+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:23:41.527+0000] {dag.py:3947} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24 00:00:00+00:00, run_after=2024-07-25 00:00:00+00:00
[2024-07-24T00:23:41.549+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.383 seconds
[2024-07-24T00:24:11.740+0000] {processor.py:161} INFO - Started process (PID=3891) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:24:11.741+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T00:24:11.742+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:24:11.742+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:24:11.836+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:24:11.836+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:24:11.878+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:24:11.879+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:24:12.047+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T00:24:12.047+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T00:24:12.056+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:24:12.082+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:24:12.082+0000] {dag.py:3089} INFO - Sync 1 DAGs
[2024-07-24T00:24:12.105+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:24:12.104+0000] {dag.py:3947} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24 00:00:00+00:00, run_after=2024-07-25 00:00:00+00:00
[2024-07-24T00:24:12.129+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.396 seconds
[2024-07-24T00:24:42.313+0000] {processor.py:161} INFO - Started process (PID=3893) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:24:42.314+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T00:24:42.316+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:24:42.315+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:24:42.418+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:24:42.419+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:24:42.464+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:24:42.465+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:24:42.629+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T00:24:42.630+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T00:24:42.637+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:24:42.667+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:24:42.667+0000] {dag.py:3089} INFO - Sync 1 DAGs
[2024-07-24T00:24:42.689+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:24:42.689+0000] {dag.py:3947} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24 00:00:00+00:00, run_after=2024-07-25 00:00:00+00:00
[2024-07-24T00:24:42.706+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.400 seconds
[2024-07-24T00:25:12.877+0000] {processor.py:161} INFO - Started process (PID=3895) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:25:12.878+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T00:25:12.879+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:25:12.879+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:25:12.968+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:25:12.968+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:25:13.013+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:25:13.013+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:25:13.191+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T00:25:13.191+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T00:25:13.200+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:25:13.227+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:25:13.227+0000] {dag.py:3089} INFO - Sync 1 DAGs
[2024-07-24T00:25:13.247+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:25:13.247+0000] {dag.py:3947} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24 00:00:00+00:00, run_after=2024-07-25 00:00:00+00:00
[2024-07-24T00:25:13.264+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.393 seconds
[2024-07-24T00:25:43.433+0000] {processor.py:161} INFO - Started process (PID=3897) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:25:43.434+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T00:25:43.435+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:25:43.435+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:25:43.525+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:25:43.525+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:25:43.567+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:25:43.568+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:25:43.729+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T00:25:43.730+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T00:25:43.736+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:25:43.762+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:25:43.762+0000] {dag.py:3089} INFO - Sync 1 DAGs
[2024-07-24T00:25:43.784+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:25:43.784+0000] {dag.py:3947} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24 00:00:00+00:00, run_after=2024-07-25 00:00:00+00:00
[2024-07-24T00:25:43.803+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.376 seconds
[2024-07-24T00:26:13.975+0000] {processor.py:161} INFO - Started process (PID=3899) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:26:13.976+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T00:26:13.978+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:26:13.977+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:26:14.073+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:26:14.074+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:26:14.123+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:26:14.123+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:26:14.286+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T00:26:14.286+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T00:26:14.292+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:26:14.315+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:26:14.314+0000] {dag.py:3089} INFO - Sync 1 DAGs
[2024-07-24T00:26:14.333+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:26:14.333+0000] {dag.py:3947} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24 00:00:00+00:00, run_after=2024-07-25 00:00:00+00:00
[2024-07-24T00:26:14.356+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.386 seconds
[2024-07-24T00:26:44.532+0000] {processor.py:161} INFO - Started process (PID=3901) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:26:44.534+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T00:26:44.536+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:26:44.536+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:26:44.628+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:26:44.628+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:26:44.668+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:26:44.669+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:26:44.838+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T00:26:44.839+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T00:26:44.846+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:26:44.869+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:26:44.869+0000] {dag.py:3089} INFO - Sync 1 DAGs
[2024-07-24T00:26:44.886+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:26:44.886+0000] {dag.py:3947} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24 00:00:00+00:00, run_after=2024-07-25 00:00:00+00:00
[2024-07-24T00:26:44.902+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.377 seconds
[2024-07-24T00:27:15.080+0000] {processor.py:161} INFO - Started process (PID=3903) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:27:15.081+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T00:27:15.083+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:27:15.083+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:27:15.170+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:27:15.171+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:27:15.207+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:27:15.208+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:27:15.359+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T00:27:15.359+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T00:27:15.365+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:27:15.386+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:27:15.386+0000] {dag.py:3089} INFO - Sync 1 DAGs
[2024-07-24T00:27:15.433+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:27:15.433+0000] {dag.py:3947} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24 00:00:00+00:00, run_after=2024-07-25 00:00:00+00:00
[2024-07-24T00:27:15.460+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.386 seconds
[2024-07-24T00:27:45.633+0000] {processor.py:161} INFO - Started process (PID=3905) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:27:45.634+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T00:27:45.636+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:27:45.635+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:27:45.726+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:27:45.726+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:27:45.769+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:27:45.770+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:27:45.940+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T00:27:45.940+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T00:27:45.947+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:27:45.972+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:27:45.971+0000] {dag.py:3089} INFO - Sync 1 DAGs
[2024-07-24T00:27:45.990+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:27:45.990+0000] {dag.py:3947} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24 00:00:00+00:00, run_after=2024-07-25 00:00:00+00:00
[2024-07-24T00:27:46.012+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.385 seconds
[2024-07-24T00:28:16.193+0000] {processor.py:161} INFO - Started process (PID=3907) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:28:16.194+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T00:28:16.195+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:28:16.195+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:28:16.287+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:28:16.287+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:28:16.331+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:28:16.331+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:28:16.487+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T00:28:16.488+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T00:28:16.495+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:28:16.518+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:28:16.517+0000] {dag.py:3089} INFO - Sync 1 DAGs
[2024-07-24T00:28:16.538+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:28:16.538+0000] {dag.py:3947} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24 00:00:00+00:00, run_after=2024-07-25 00:00:00+00:00
[2024-07-24T00:28:16.560+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.376 seconds
[2024-07-24T00:28:46.737+0000] {processor.py:161} INFO - Started process (PID=3909) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:28:46.738+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T00:28:46.740+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:28:46.739+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:28:46.834+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:28:46.834+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:28:46.872+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:28:46.873+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:28:47.040+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T00:28:47.040+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T00:28:47.046+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:28:47.069+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:28:47.069+0000] {dag.py:3089} INFO - Sync 1 DAGs
[2024-07-24T00:28:47.087+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:28:47.087+0000] {dag.py:3947} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24 00:00:00+00:00, run_after=2024-07-25 00:00:00+00:00
[2024-07-24T00:28:47.104+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.372 seconds
[2024-07-24T00:29:17.272+0000] {processor.py:161} INFO - Started process (PID=3911) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:29:17.273+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T00:29:17.275+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:29:17.274+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:29:17.374+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:29:17.375+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:29:17.417+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:29:17.418+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:29:17.592+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T00:29:17.593+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T00:29:17.599+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:29:17.621+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:29:17.620+0000] {dag.py:3089} INFO - Sync 1 DAGs
[2024-07-24T00:29:17.640+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:29:17.640+0000] {dag.py:3947} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24 00:00:00+00:00, run_after=2024-07-25 00:00:00+00:00
[2024-07-24T00:29:17.658+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.392 seconds
[2024-07-24T00:29:47.846+0000] {processor.py:161} INFO - Started process (PID=3913) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:29:47.847+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T00:29:47.849+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:29:47.849+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:29:47.949+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:29:47.950+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:29:47.997+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:29:47.998+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:29:48.193+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T00:29:48.193+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T00:29:48.202+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:29:48.232+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:29:48.231+0000] {dag.py:3089} INFO - Sync 1 DAGs
[2024-07-24T00:29:48.256+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:29:48.256+0000] {dag.py:3947} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24 00:00:00+00:00, run_after=2024-07-25 00:00:00+00:00
[2024-07-24T00:29:48.275+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.437 seconds
[2024-07-24T00:30:18.457+0000] {processor.py:161} INFO - Started process (PID=3915) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:30:18.458+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T00:30:18.460+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:30:18.459+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:30:18.566+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:30:18.567+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:30:18.611+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:30:18.611+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:30:18.775+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T00:30:18.776+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T00:30:18.784+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:30:18.808+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:30:18.808+0000] {dag.py:3089} INFO - Sync 1 DAGs
[2024-07-24T00:30:18.829+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:30:18.829+0000] {dag.py:3947} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24 00:00:00+00:00, run_after=2024-07-25 00:00:00+00:00
[2024-07-24T00:30:18.844+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.395 seconds
[2024-07-24T00:30:49.014+0000] {processor.py:161} INFO - Started process (PID=3917) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:30:49.015+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T00:30:49.017+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:30:49.017+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:30:49.102+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:30:49.103+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:30:49.140+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:30:49.141+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:30:49.304+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T00:30:49.305+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T00:30:49.311+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:30:49.332+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:30:49.332+0000] {dag.py:3089} INFO - Sync 1 DAGs
[2024-07-24T00:30:49.353+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:30:49.352+0000] {dag.py:3947} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24 00:00:00+00:00, run_after=2024-07-25 00:00:00+00:00
[2024-07-24T00:30:49.368+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.360 seconds
[2024-07-24T00:31:19.539+0000] {processor.py:161} INFO - Started process (PID=3919) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:31:19.540+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T00:31:19.542+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:31:19.542+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:31:19.633+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:31:19.634+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:31:19.672+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:31:19.673+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:31:19.825+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T00:31:19.825+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T00:31:19.832+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:31:19.860+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:31:19.859+0000] {dag.py:3089} INFO - Sync 1 DAGs
[2024-07-24T00:31:19.880+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:31:19.880+0000] {dag.py:3947} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24 00:00:00+00:00, run_after=2024-07-25 00:00:00+00:00
[2024-07-24T00:31:19.897+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.365 seconds
[2024-07-24T00:31:50.076+0000] {processor.py:161} INFO - Started process (PID=3921) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:31:50.077+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T00:31:50.078+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:31:50.078+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:31:50.161+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:31:50.162+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:31:50.202+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:31:50.203+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:31:50.355+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T00:31:50.355+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T00:31:50.363+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:31:50.386+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:31:50.386+0000] {dag.py:3089} INFO - Sync 1 DAGs
[2024-07-24T00:31:50.406+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:31:50.406+0000] {dag.py:3947} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24 00:00:00+00:00, run_after=2024-07-25 00:00:00+00:00
[2024-07-24T00:31:50.429+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.361 seconds
[2024-07-24T00:32:20.611+0000] {processor.py:161} INFO - Started process (PID=3923) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:32:20.612+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T00:32:20.614+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:32:20.613+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:32:20.709+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:32:20.710+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:32:20.753+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:32:20.753+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:32:20.915+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T00:32:20.916+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T00:32:20.922+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:32:20.948+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:32:20.947+0000] {dag.py:3089} INFO - Sync 1 DAGs
[2024-07-24T00:32:20.968+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:32:20.968+0000] {dag.py:3947} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24 00:00:00+00:00, run_after=2024-07-25 00:00:00+00:00
[2024-07-24T00:32:20.990+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.387 seconds
[2024-07-24T00:32:51.173+0000] {processor.py:161} INFO - Started process (PID=3925) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:32:51.175+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T00:32:51.179+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:32:51.178+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:32:51.325+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:32:51.326+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:32:51.416+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:32:51.417+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:32:51.675+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T00:32:51.676+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T00:32:51.684+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:32:51.711+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:32:51.711+0000] {dag.py:3089} INFO - Sync 1 DAGs
[2024-07-24T00:32:51.733+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:32:51.733+0000] {dag.py:3947} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24 00:00:00+00:00, run_after=2024-07-25 00:00:00+00:00
[2024-07-24T00:32:51.750+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.589 seconds
[2024-07-24T00:33:21.944+0000] {processor.py:161} INFO - Started process (PID=3927) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:33:21.946+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T00:33:21.949+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:33:21.949+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:33:22.073+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:33:22.074+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:33:22.128+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:33:22.128+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:33:22.309+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T00:33:22.310+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T00:33:22.321+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:33:22.353+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:33:22.353+0000] {dag.py:3089} INFO - Sync 1 DAGs
[2024-07-24T00:33:22.378+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:33:22.378+0000] {dag.py:3947} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24 00:00:00+00:00, run_after=2024-07-25 00:00:00+00:00
[2024-07-24T00:33:22.398+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.465 seconds
[2024-07-24T00:33:52.669+0000] {processor.py:161} INFO - Started process (PID=3929) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:33:52.670+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T00:33:52.673+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:33:52.673+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:33:52.917+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:33:52.917+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:33:52.988+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:33:52.988+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:33:53.256+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T00:33:53.257+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T00:33:53.272+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:33:53.305+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:33:53.304+0000] {dag.py:3089} INFO - Sync 1 DAGs
[2024-07-24T00:33:53.333+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:33:53.332+0000] {dag.py:3947} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24 00:00:00+00:00, run_after=2024-07-25 00:00:00+00:00
[2024-07-24T00:33:53.352+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.706 seconds
[2024-07-24T00:34:23.529+0000] {processor.py:161} INFO - Started process (PID=3931) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:34:23.530+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T00:34:23.532+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:34:23.531+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:34:23.629+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:34:23.629+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:34:23.674+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:34:23.675+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:34:23.849+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T00:34:23.850+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T00:34:23.857+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:34:23.880+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:34:23.880+0000] {dag.py:3089} INFO - Sync 1 DAGs
[2024-07-24T00:34:23.900+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:34:23.899+0000] {dag.py:3947} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24 00:00:00+00:00, run_after=2024-07-25 00:00:00+00:00
[2024-07-24T00:34:23.915+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.394 seconds
[2024-07-24T00:34:54.086+0000] {processor.py:161} INFO - Started process (PID=3933) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:34:54.087+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T00:34:54.088+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:34:54.088+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:34:54.177+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:34:54.178+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:34:54.223+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:34:54.224+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:34:54.380+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T00:34:54.380+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T00:34:54.389+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:34:54.410+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:34:54.409+0000] {dag.py:3089} INFO - Sync 1 DAGs
[2024-07-24T00:34:54.428+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:34:54.428+0000] {dag.py:3947} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24 00:00:00+00:00, run_after=2024-07-25 00:00:00+00:00
[2024-07-24T00:34:54.442+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.362 seconds
[2024-07-24T00:35:24.656+0000] {processor.py:161} INFO - Started process (PID=3935) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:35:24.662+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T00:35:24.670+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:35:24.670+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:35:24.788+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:35:24.788+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:35:24.848+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:35:24.849+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:35:25.051+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T00:35:25.052+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T00:35:25.060+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:35:25.091+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:35:25.091+0000] {dag.py:3089} INFO - Sync 1 DAGs
[2024-07-24T00:35:25.116+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:35:25.115+0000] {dag.py:3947} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24 00:00:00+00:00, run_after=2024-07-25 00:00:00+00:00
[2024-07-24T00:35:25.140+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.497 seconds
[2024-07-24T00:35:55.340+0000] {processor.py:161} INFO - Started process (PID=3937) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:35:55.342+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T00:35:55.344+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:35:55.344+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:35:55.455+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:35:55.456+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:35:55.508+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:35:55.508+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:35:55.698+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T00:35:55.699+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T00:35:55.707+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:35:55.730+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:35:55.730+0000] {dag.py:3089} INFO - Sync 1 DAGs
[2024-07-24T00:35:55.752+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:35:55.752+0000] {dag.py:3947} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24 00:00:00+00:00, run_after=2024-07-25 00:00:00+00:00
[2024-07-24T00:35:55.769+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.435 seconds
[2024-07-24T00:36:25.956+0000] {processor.py:161} INFO - Started process (PID=3939) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:36:25.957+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T00:36:25.961+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:36:25.960+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:36:26.053+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:36:26.053+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:36:26.108+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:36:26.108+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:36:26.304+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T00:36:26.304+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T00:36:26.314+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:36:26.340+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:36:26.340+0000] {dag.py:3089} INFO - Sync 1 DAGs
[2024-07-24T00:36:26.366+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:36:26.366+0000] {dag.py:3947} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24 00:00:00+00:00, run_after=2024-07-25 00:00:00+00:00
[2024-07-24T00:36:26.384+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.434 seconds
[2024-07-24T00:36:56.570+0000] {processor.py:161} INFO - Started process (PID=3941) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:36:56.571+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T00:36:56.573+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:36:56.572+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:36:56.667+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:36:56.668+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:36:56.711+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:36:56.712+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:36:56.868+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T00:36:56.868+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T00:36:56.874+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:36:56.905+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:36:56.904+0000] {dag.py:3089} INFO - Sync 1 DAGs
[2024-07-24T00:36:56.929+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:36:56.928+0000] {dag.py:3947} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24 00:00:00+00:00, run_after=2024-07-25 00:00:00+00:00
[2024-07-24T00:36:56.947+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.384 seconds
[2024-07-24T00:37:27.131+0000] {processor.py:161} INFO - Started process (PID=3943) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:37:27.132+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T00:37:27.134+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:37:27.134+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:37:27.236+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:37:27.236+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:37:27.284+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:37:27.284+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:37:27.459+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T00:37:27.459+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T00:37:27.468+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:37:27.494+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:37:27.493+0000] {dag.py:3089} INFO - Sync 1 DAGs
[2024-07-24T00:37:27.516+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:37:27.515+0000] {dag.py:3947} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24 00:00:00+00:00, run_after=2024-07-25 00:00:00+00:00
[2024-07-24T00:37:27.534+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.410 seconds
[2024-07-24T00:37:57.735+0000] {processor.py:161} INFO - Started process (PID=3945) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:37:57.736+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T00:37:57.739+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:37:57.738+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:37:57.846+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:37:57.847+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:37:57.898+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:37:57.899+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:37:58.123+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T00:37:58.124+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T00:37:58.133+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:37:58.166+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:37:58.165+0000] {dag.py:3089} INFO - Sync 1 DAGs
[2024-07-24T00:37:58.242+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:37:58.242+0000] {dag.py:3947} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24 00:00:00+00:00, run_after=2024-07-25 00:00:00+00:00
[2024-07-24T00:37:58.277+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.550 seconds
[2024-07-24T00:38:28.728+0000] {processor.py:161} INFO - Started process (PID=3947) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:38:28.729+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T00:38:28.739+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:38:28.737+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:38:29.262+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:38:29.264+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:38:29.610+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:38:29.610+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:38:30.082+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T00:38:30.082+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T00:38:30.110+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:38:30.201+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:38:30.200+0000] {dag.py:3089} INFO - Sync 1 DAGs
[2024-07-24T00:38:30.246+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:38:30.245+0000] {dag.py:3947} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24 00:00:00+00:00, run_after=2024-07-25 00:00:00+00:00
[2024-07-24T00:38:30.276+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 1.573 seconds
[2024-07-24T00:39:00.480+0000] {processor.py:161} INFO - Started process (PID=3949) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:39:00.482+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T00:39:00.485+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:39:00.484+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:39:00.619+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:39:00.620+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:39:00.672+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:39:00.673+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:39:00.890+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T00:39:00.890+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T00:39:00.898+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:39:00.932+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:39:00.931+0000] {dag.py:3089} INFO - Sync 1 DAGs
[2024-07-24T00:39:00.966+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:39:00.966+0000] {dag.py:3947} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24 00:00:00+00:00, run_after=2024-07-25 00:00:00+00:00
[2024-07-24T00:39:00.996+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.526 seconds
[2024-07-24T00:39:31.209+0000] {processor.py:161} INFO - Started process (PID=3951) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:39:31.212+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T00:39:31.215+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:39:31.215+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:39:31.405+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:39:31.406+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:39:31.472+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:39:31.473+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:39:31.705+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T00:39:31.706+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T00:39:31.716+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:39:31.769+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:39:31.768+0000] {dag.py:3089} INFO - Sync 1 DAGs
[2024-07-24T00:39:31.807+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:39:31.806+0000] {dag.py:3947} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24 00:00:00+00:00, run_after=2024-07-25 00:00:00+00:00
[2024-07-24T00:39:31.830+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.639 seconds
[2024-07-24T00:40:02.011+0000] {processor.py:161} INFO - Started process (PID=3953) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:40:02.012+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T00:40:02.014+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:40:02.014+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:40:02.102+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:40:02.103+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:40:02.142+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:40:02.143+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:40:02.300+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T00:40:02.300+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T00:40:02.306+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:40:02.329+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:40:02.328+0000] {dag.py:3089} INFO - Sync 1 DAGs
[2024-07-24T00:40:02.348+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:40:02.348+0000] {dag.py:3947} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24 00:00:00+00:00, run_after=2024-07-25 00:00:00+00:00
[2024-07-24T00:40:02.365+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.359 seconds
[2024-07-24T00:40:32.462+0000] {processor.py:161} INFO - Started process (PID=3955) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:40:32.463+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T00:40:32.465+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:40:32.464+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:40:32.556+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:40:32.557+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:40:32.601+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:40:32.602+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:40:32.763+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T00:40:32.764+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T00:40:32.770+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:40:32.794+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:40:32.794+0000] {dag.py:3089} INFO - Sync 1 DAGs
[2024-07-24T00:40:32.813+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:40:32.813+0000] {dag.py:3947} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24 00:00:00+00:00, run_after=2024-07-25 00:00:00+00:00
[2024-07-24T00:40:32.828+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.372 seconds
[2024-07-24T00:41:02.995+0000] {processor.py:161} INFO - Started process (PID=3957) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:41:02.996+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T00:41:02.998+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:41:02.998+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:41:03.111+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:41:03.111+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:41:03.154+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:41:03.155+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:41:03.338+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T00:41:03.338+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T00:41:03.346+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:41:03.376+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:41:03.375+0000] {dag.py:3089} INFO - Sync 1 DAGs
[2024-07-24T00:41:03.397+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:41:03.397+0000] {dag.py:3947} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24 00:00:00+00:00, run_after=2024-07-25 00:00:00+00:00
[2024-07-24T00:41:03.414+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.427 seconds
[2024-07-24T00:41:33.585+0000] {processor.py:161} INFO - Started process (PID=3959) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:41:33.586+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T00:41:33.588+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:41:33.587+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:41:33.689+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:41:33.690+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:41:33.731+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:41:33.731+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:41:33.902+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T00:41:33.903+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T00:41:33.910+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:41:33.934+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:41:33.934+0000] {dag.py:3089} INFO - Sync 1 DAGs
[2024-07-24T00:41:33.954+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:41:33.954+0000] {dag.py:3947} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24 00:00:00+00:00, run_after=2024-07-25 00:00:00+00:00
[2024-07-24T00:41:33.971+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.392 seconds
[2024-07-24T00:42:04.136+0000] {processor.py:161} INFO - Started process (PID=3961) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:42:04.137+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T00:42:04.138+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:42:04.138+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:42:04.249+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:42:04.249+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:42:04.295+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:42:04.295+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:42:04.463+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T00:42:04.464+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T00:42:04.470+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:42:04.493+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:42:04.492+0000] {dag.py:3089} INFO - Sync 1 DAGs
[2024-07-24T00:42:04.513+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:42:04.512+0000] {dag.py:3947} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24 00:00:00+00:00, run_after=2024-07-25 00:00:00+00:00
[2024-07-24T00:42:04.528+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.398 seconds
[2024-07-24T00:42:34.683+0000] {processor.py:161} INFO - Started process (PID=3963) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:42:34.684+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T00:42:34.685+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:42:34.685+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:42:34.780+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:42:34.780+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:42:34.825+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:42:34.825+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:42:35.060+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T00:42:35.060+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T00:42:35.069+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:42:35.095+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:42:35.094+0000] {dag.py:3089} INFO - Sync 1 DAGs
[2024-07-24T00:42:35.121+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:42:35.121+0000] {dag.py:3947} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24 00:00:00+00:00, run_after=2024-07-25 00:00:00+00:00
[2024-07-24T00:42:35.140+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.463 seconds
[2024-07-24T00:43:05.300+0000] {processor.py:161} INFO - Started process (PID=3965) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:43:05.301+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T00:43:05.303+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:43:05.302+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:43:05.386+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:43:05.387+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:43:05.423+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:43:05.424+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:43:05.576+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T00:43:05.577+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T00:43:05.583+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:43:05.604+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:43:05.603+0000] {dag.py:3089} INFO - Sync 1 DAGs
[2024-07-24T00:43:05.625+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:43:05.625+0000] {dag.py:3947} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24 00:00:00+00:00, run_after=2024-07-25 00:00:00+00:00
[2024-07-24T00:43:05.640+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.346 seconds
[2024-07-24T00:43:35.855+0000] {processor.py:161} INFO - Started process (PID=3967) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:43:35.858+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T00:43:35.862+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:43:35.861+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:43:35.985+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:43:35.986+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:43:36.048+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:43:36.049+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:43:36.272+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T00:43:36.273+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T00:43:36.284+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:43:36.319+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:43:36.318+0000] {dag.py:3089} INFO - Sync 1 DAGs
[2024-07-24T00:43:36.343+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:43:36.342+0000] {dag.py:3947} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24 00:00:00+00:00, run_after=2024-07-25 00:00:00+00:00
[2024-07-24T00:43:36.367+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.521 seconds
[2024-07-24T00:44:06.544+0000] {processor.py:161} INFO - Started process (PID=3969) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:44:06.545+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T00:44:06.546+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:44:06.546+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:44:06.642+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:44:06.643+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:44:06.684+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:44:06.684+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:44:06.846+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T00:44:06.847+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T00:44:06.854+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:44:06.877+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:44:06.877+0000] {dag.py:3089} INFO - Sync 1 DAGs
[2024-07-24T00:44:06.896+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:44:06.895+0000] {dag.py:3947} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24 00:00:00+00:00, run_after=2024-07-25 00:00:00+00:00
[2024-07-24T00:44:06.912+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.375 seconds
[2024-07-24T00:44:37.084+0000] {processor.py:161} INFO - Started process (PID=3971) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:44:37.085+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T00:44:37.087+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:44:37.086+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:44:37.180+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:44:37.181+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:44:37.223+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:44:37.223+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:44:37.404+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T00:44:37.405+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T00:44:37.414+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:44:37.444+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:44:37.444+0000] {dag.py:3089} INFO - Sync 1 DAGs
[2024-07-24T00:44:37.471+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:44:37.470+0000] {dag.py:3947} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24 00:00:00+00:00, run_after=2024-07-25 00:00:00+00:00
[2024-07-24T00:44:37.490+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.412 seconds
[2024-07-24T00:45:07.663+0000] {processor.py:161} INFO - Started process (PID=3973) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:45:07.664+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T00:45:07.666+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:45:07.666+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:45:07.762+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:45:07.763+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:45:07.801+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:45:07.802+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:45:07.980+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T00:45:07.981+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T00:45:07.990+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:45:08.017+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:45:08.016+0000] {dag.py:3089} INFO - Sync 1 DAGs
[2024-07-24T00:45:08.047+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:45:08.046+0000] {dag.py:3947} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24 00:00:00+00:00, run_after=2024-07-25 00:00:00+00:00
[2024-07-24T00:45:08.066+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.411 seconds
[2024-07-24T00:45:38.270+0000] {processor.py:161} INFO - Started process (PID=3975) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:45:38.271+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T00:45:38.274+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:45:38.273+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:45:38.392+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:45:38.393+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:45:38.445+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:45:38.445+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:45:38.636+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T00:45:38.637+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T00:45:38.645+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:45:38.673+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:45:38.673+0000] {dag.py:3089} INFO - Sync 1 DAGs
[2024-07-24T00:45:38.695+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:45:38.695+0000] {dag.py:3947} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24 00:00:00+00:00, run_after=2024-07-25 00:00:00+00:00
[2024-07-24T00:45:38.714+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.453 seconds
[2024-07-24T00:46:08.897+0000] {processor.py:161} INFO - Started process (PID=3977) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:46:08.898+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T00:46:08.900+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:46:08.900+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:46:09.009+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:46:09.009+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:46:09.070+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:46:09.071+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:46:09.320+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T00:46:09.321+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T00:46:09.330+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:46:09.361+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:46:09.360+0000] {dag.py:3089} INFO - Sync 1 DAGs
[2024-07-24T00:46:09.389+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:46:09.388+0000] {dag.py:3947} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24 00:00:00+00:00, run_after=2024-07-25 00:00:00+00:00
[2024-07-24T00:46:09.414+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.525 seconds
[2024-07-24T00:46:39.599+0000] {processor.py:161} INFO - Started process (PID=3979) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:46:39.600+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T00:46:39.602+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:46:39.602+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:46:39.712+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:46:39.712+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:46:39.764+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:46:39.765+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:46:39.968+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T00:46:39.968+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T00:46:39.976+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:46:40.004+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:46:40.004+0000] {dag.py:3089} INFO - Sync 1 DAGs
[2024-07-24T00:46:40.030+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:46:40.029+0000] {dag.py:3947} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24 00:00:00+00:00, run_after=2024-07-25 00:00:00+00:00
[2024-07-24T00:46:40.053+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.460 seconds
[2024-07-24T00:47:10.248+0000] {processor.py:161} INFO - Started process (PID=3981) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:47:10.249+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T00:47:10.251+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:47:10.251+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:47:10.381+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:47:10.382+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:47:10.445+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:47:10.446+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:47:10.647+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T00:47:10.647+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T00:47:10.656+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:47:10.685+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:47:10.684+0000] {dag.py:3089} INFO - Sync 1 DAGs
[2024-07-24T00:47:10.712+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:47:10.711+0000] {dag.py:3947} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24 00:00:00+00:00, run_after=2024-07-25 00:00:00+00:00
[2024-07-24T00:47:10.733+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.493 seconds
[2024-07-24T00:47:23.321+0000] {processor.py:161} INFO - Started process (PID=3983) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:47:23.322+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T00:47:23.323+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:47:23.323+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:47:23.490+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:47:23.491+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:47:23.556+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:47:23.556+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:47:23.779+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T00:47:23.779+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T00:47:23.788+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:47:23.819+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:47:23.818+0000] {dag.py:3089} INFO - Sync 1 DAGs
[2024-07-24T00:47:23.852+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:47:23.852+0000] {dag.py:3947} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24 00:00:00+00:00, run_after=2024-07-25 00:00:00+00:00
[2024-07-24T00:47:23.892+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.580 seconds
[2024-07-24T00:47:32.894+0000] {processor.py:161} INFO - Started process (PID=3985) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:47:32.895+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T00:47:32.897+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:47:32.897+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:47:33.006+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:47:33.006+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:47:33.061+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:47:33.061+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:47:33.238+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T00:47:33.239+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T00:47:33.245+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:47:33.243+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 44, in <module>
    upload_s3 = PythonOperator(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py", line 468, in apply_defaults
    raise AirflowException(f"missing keyword argument {missing_args.pop()!r}")
airflow.exceptions.AirflowException: missing keyword argument 'python_callable'
[2024-07-24T00:47:33.246+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:47:33.282+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.395 seconds
[2024-07-24T00:47:35.993+0000] {processor.py:161} INFO - Started process (PID=3987) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:47:35.994+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T00:47:35.996+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:47:35.995+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:47:36.108+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:47:36.108+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:47:36.156+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:47:36.156+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:47:36.353+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T00:47:36.354+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T00:47:36.359+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:47:36.358+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 46, in <module>
    python_callable=load_data_to_csv,
NameError: name 'load_data_to_csv' is not defined
[2024-07-24T00:47:36.360+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:47:36.382+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.397 seconds
[2024-07-24T00:47:42.368+0000] {processor.py:161} INFO - Started process (PID=3989) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:47:42.370+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T00:47:42.372+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:47:42.372+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:47:42.489+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:47:42.490+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:47:42.541+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:47:42.542+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:47:42.755+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T00:47:42.756+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T00:47:42.760+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:47:42.759+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 46, in <module>
    python_callable=upload_s3_pipeline,
NameError: name 'upload_s3_pipeline' is not defined
[2024-07-24T00:47:42.761+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:47:42.780+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.420 seconds
[2024-07-24T00:47:52.864+0000] {processor.py:161} INFO - Started process (PID=3991) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:47:52.865+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T00:47:52.866+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:47:52.866+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:47:53.000+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:47:53.000+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:47:53.045+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:47:53.046+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:47:53.224+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T00:47:53.224+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T00:47:53.229+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:47:53.228+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 46, in <module>
    python_callable=upload_s3_pipeline,
NameError: name 'upload_s3_pipeline' is not defined
[2024-07-24T00:47:53.230+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:47:53.247+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.389 seconds
[2024-07-24T00:48:23.414+0000] {processor.py:161} INFO - Started process (PID=3993) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:48:23.415+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T00:48:23.417+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:48:23.417+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:48:23.507+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:48:23.508+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:48:23.548+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:48:23.548+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:48:23.705+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T00:48:23.706+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T00:48:23.710+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:48:23.709+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 46, in <module>
    python_callable=upload_s3_pipeline,
NameError: name 'upload_s3_pipeline' is not defined
[2024-07-24T00:48:23.711+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:48:23.728+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.321 seconds
[2024-07-24T00:48:53.937+0000] {processor.py:161} INFO - Started process (PID=3995) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:48:53.938+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T00:48:53.939+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:48:53.939+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:48:54.026+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:48:54.026+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:48:54.065+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:48:54.065+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:48:54.215+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T00:48:54.216+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T00:48:54.221+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:48:54.220+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 46, in <module>
    python_callable=upload_s3_pipeline,
NameError: name 'upload_s3_pipeline' is not defined
[2024-07-24T00:48:54.222+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:48:54.238+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.307 seconds
[2024-07-24T00:49:24.410+0000] {processor.py:161} INFO - Started process (PID=3997) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:49:24.411+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T00:49:24.412+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:49:24.412+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:49:24.502+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:49:24.502+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:49:24.545+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:49:24.545+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:49:24.710+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T00:49:24.711+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T00:49:24.716+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:49:24.715+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 46, in <module>
    python_callable=upload_s3_pipeline,
NameError: name 'upload_s3_pipeline' is not defined
[2024-07-24T00:49:24.717+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:49:24.734+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.330 seconds
[2024-07-24T00:49:54.904+0000] {processor.py:161} INFO - Started process (PID=3999) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:49:54.905+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T00:49:54.905+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:49:54.905+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:49:54.987+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:49:54.988+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:49:55.027+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:49:55.028+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:49:55.177+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T00:49:55.178+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T00:49:55.183+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:49:55.182+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 46, in <module>
    python_callable=upload_s3_pipeline,
NameError: name 'upload_s3_pipeline' is not defined
[2024-07-24T00:49:55.183+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:49:55.198+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.300 seconds
[2024-07-24T00:50:25.363+0000] {processor.py:161} INFO - Started process (PID=4001) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:50:25.364+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T00:50:25.365+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:50:25.365+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:50:25.481+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:50:25.482+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:50:25.523+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:50:25.524+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:50:25.687+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T00:50:25.687+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T00:50:25.693+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:50:25.691+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 46, in <module>
    python_callable=upload_s3_pipeline,
NameError: name 'upload_s3_pipeline' is not defined
[2024-07-24T00:50:25.694+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:50:25.711+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.353 seconds
[2024-07-24T00:50:47.483+0000] {processor.py:161} INFO - Started process (PID=4003) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:50:47.484+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T00:50:47.484+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:50:47.484+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:50:47.590+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:50:47.591+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:50:47.639+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:50:47.640+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:50:47.821+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T00:50:47.821+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T00:50:47.833+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:50:48.053+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:50:48.052+0000] {dag.py:3089} INFO - Sync 1 DAGs
[2024-07-24T00:50:48.072+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:50:48.072+0000] {dag.py:3947} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24 00:00:00+00:00, run_after=2024-07-25 00:00:00+00:00
[2024-07-24T00:50:48.097+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.622 seconds
[2024-07-24T00:50:48.867+0000] {processor.py:161} INFO - Started process (PID=4005) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:50:48.868+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T00:50:48.869+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:50:48.869+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:50:48.965+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:50:48.966+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:50:49.006+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:50:49.006+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:50:49.161+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T00:50:49.162+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T00:50:49.170+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:50:49.180+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:50:49.180+0000] {dag.py:3089} INFO - Sync 1 DAGs
[2024-07-24T00:50:49.202+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:50:49.201+0000] {dag.py:3947} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24 00:00:00+00:00, run_after=2024-07-25 00:00:00+00:00
[2024-07-24T00:50:49.221+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.361 seconds
[2024-07-24T00:51:19.287+0000] {processor.py:161} INFO - Started process (PID=4007) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:51:19.288+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T00:51:19.288+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:51:19.288+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:51:19.372+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:51:19.373+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:51:19.412+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:51:19.413+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:51:19.566+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T00:51:19.566+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T00:51:19.575+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:51:19.599+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:51:19.598+0000] {dag.py:3089} INFO - Sync 1 DAGs
[2024-07-24T00:51:19.619+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:51:19.619+0000] {dag.py:3947} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24 00:00:00+00:00, run_after=2024-07-25 00:00:00+00:00
[2024-07-24T00:51:19.636+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.355 seconds
[2024-07-24T00:51:49.804+0000] {processor.py:161} INFO - Started process (PID=4009) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:51:49.804+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T00:51:49.805+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:51:49.805+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:51:49.899+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:51:49.900+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:51:49.938+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:51:49.938+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:51:50.102+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T00:51:50.102+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T00:51:50.109+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:51:50.131+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:51:50.131+0000] {dag.py:3089} INFO - Sync 1 DAGs
[2024-07-24T00:51:50.150+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:51:50.150+0000] {dag.py:3947} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24 00:00:00+00:00, run_after=2024-07-25 00:00:00+00:00
[2024-07-24T00:51:50.167+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.370 seconds
[2024-07-24T00:52:20.337+0000] {processor.py:161} INFO - Started process (PID=4011) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:52:20.338+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T00:52:20.338+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:52:20.338+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:52:20.423+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:52:20.424+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:52:20.459+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:52:20.460+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:52:20.617+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T00:52:20.617+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T00:52:20.624+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:52:20.648+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:52:20.647+0000] {dag.py:3089} INFO - Sync 1 DAGs
[2024-07-24T00:52:20.667+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:52:20.666+0000] {dag.py:3947} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24 00:00:00+00:00, run_after=2024-07-25 00:00:00+00:00
[2024-07-24T00:52:20.683+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.352 seconds
[2024-07-24T00:52:50.856+0000] {processor.py:161} INFO - Started process (PID=4013) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:52:50.857+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T00:52:50.858+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:52:50.858+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:52:50.952+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:52:50.953+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:52:50.994+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:52:50.995+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:52:51.165+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T00:52:51.165+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T00:52:51.173+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:52:51.197+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:52:51.197+0000] {dag.py:3089} INFO - Sync 1 DAGs
[2024-07-24T00:52:51.215+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:52:51.215+0000] {dag.py:3947} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24 00:00:00+00:00, run_after=2024-07-25 00:00:00+00:00
[2024-07-24T00:52:51.231+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.382 seconds
[2024-07-24T00:53:21.407+0000] {processor.py:161} INFO - Started process (PID=4015) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:53:21.408+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T00:53:21.409+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:53:21.409+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:53:21.532+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:53:21.532+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:53:21.586+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:53:21.586+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:53:21.763+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T00:53:21.764+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T00:53:21.772+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:53:21.799+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:53:21.798+0000] {dag.py:3089} INFO - Sync 1 DAGs
[2024-07-24T00:53:21.818+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:53:21.818+0000] {dag.py:3947} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24 00:00:00+00:00, run_after=2024-07-25 00:00:00+00:00
[2024-07-24T00:53:21.836+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.435 seconds
[2024-07-24T00:53:52.039+0000] {processor.py:161} INFO - Started process (PID=4017) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:53:52.041+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T00:53:52.042+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:53:52.041+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:53:52.137+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:53:52.138+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:53:52.180+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:53:52.180+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:53:52.349+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T00:53:52.350+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T00:53:52.358+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:53:52.381+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:53:52.381+0000] {dag.py:3089} INFO - Sync 1 DAGs
[2024-07-24T00:53:52.402+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:53:52.402+0000] {dag.py:3947} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24 00:00:00+00:00, run_after=2024-07-25 00:00:00+00:00
[2024-07-24T00:53:52.421+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.391 seconds
[2024-07-24T00:54:22.599+0000] {processor.py:161} INFO - Started process (PID=4019) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:54:22.600+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T00:54:22.601+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:54:22.601+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:54:22.692+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:54:22.692+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:54:22.732+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:54:22.733+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:54:22.904+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T00:54:22.904+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T00:54:22.912+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:54:22.935+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:54:22.935+0000] {dag.py:3089} INFO - Sync 1 DAGs
[2024-07-24T00:54:22.957+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:54:22.957+0000] {dag.py:3947} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24 00:00:00+00:00, run_after=2024-07-25 00:00:00+00:00
[2024-07-24T00:54:22.974+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.382 seconds
[2024-07-24T00:54:53.151+0000] {processor.py:161} INFO - Started process (PID=4021) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:54:53.152+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T00:54:53.152+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:54:53.152+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:54:53.246+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:54:53.247+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:54:53.289+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:54:53.290+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:54:53.524+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T00:54:53.524+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T00:54:53.534+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:54:53.567+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:54:53.567+0000] {dag.py:3089} INFO - Sync 1 DAGs
[2024-07-24T00:54:53.602+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:54:53.602+0000] {dag.py:3947} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24 00:00:00+00:00, run_after=2024-07-25 00:00:00+00:00
[2024-07-24T00:54:53.627+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.483 seconds
[2024-07-24T00:55:23.820+0000] {processor.py:161} INFO - Started process (PID=4023) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:55:23.821+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T00:55:23.822+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:55:23.822+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:55:23.915+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:55:23.916+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:55:23.955+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:55:23.956+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:55:24.116+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T00:55:24.117+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T00:55:24.124+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:55:24.146+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:55:24.146+0000] {dag.py:3089} INFO - Sync 1 DAGs
[2024-07-24T00:55:24.165+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:55:24.165+0000] {dag.py:3947} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24 00:00:00+00:00, run_after=2024-07-25 00:00:00+00:00
[2024-07-24T00:55:24.181+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.368 seconds
[2024-07-24T00:55:54.344+0000] {processor.py:161} INFO - Started process (PID=4025) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:55:54.344+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T00:55:54.345+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:55:54.345+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:55:54.428+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:55:54.429+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:55:54.466+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:55:54.466+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:55:54.609+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T00:55:54.609+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T00:55:54.616+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:55:54.639+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:55:54.638+0000] {dag.py:3089} INFO - Sync 1 DAGs
[2024-07-24T00:55:54.658+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:55:54.658+0000] {dag.py:3947} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24 00:00:00+00:00, run_after=2024-07-25 00:00:00+00:00
[2024-07-24T00:55:54.679+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.341 seconds
[2024-07-24T00:56:15.451+0000] {processor.py:161} INFO - Started process (PID=4027) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:56:15.452+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T00:56:15.453+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:56:15.453+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:56:15.563+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:56:15.564+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:56:15.608+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:56:15.608+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:56:15.780+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T00:56:15.780+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T00:56:15.786+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:56:15.785+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 46, in <module>
    python_callable=upload_s3_pipeline,
NameError: name 'upload_s3_pipeline' is not defined
[2024-07-24T00:56:15.787+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:56:15.806+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.362 seconds
[2024-07-24T00:56:38.917+0000] {processor.py:161} INFO - Started process (PID=4029) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:56:38.918+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T00:56:38.918+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:56:38.918+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:56:39.010+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:56:39.011+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:56:39.052+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:56:39.053+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:56:39.210+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T00:56:39.211+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T00:56:39.216+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:56:39.215+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 46, in <module>
    python_callable=upload_s3_pipeline,
NameError: name 'upload_s3_pipeline' is not defined
[2024-07-24T00:56:39.216+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:56:39.231+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.319 seconds
[2024-07-24T00:56:51.989+0000] {processor.py:161} INFO - Started process (PID=4031) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:56:51.990+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T00:56:51.991+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:56:51.991+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:56:52.003+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:56:52.003+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 846, in exec_module
  File "<frozen importlib._bootstrap_external>", line 983, in get_code
  File "<frozen importlib._bootstrap_external>", line 913, in source_to_code
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 12
    from pip
            ^
SyntaxError: invalid syntax
[2024-07-24T00:56:52.004+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:56:52.022+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.039 seconds
[2024-07-24T00:56:56.018+0000] {processor.py:161} INFO - Started process (PID=4032) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:56:56.019+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T00:56:56.020+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:56:56.020+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:56:56.032+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:56:56.032+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 846, in exec_module
  File "<frozen importlib._bootstrap_external>", line 983, in get_code
  File "<frozen importlib._bootstrap_external>", line 913, in source_to_code
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 12
    from pipelines.
                   ^
SyntaxError: invalid syntax
[2024-07-24T00:56:56.033+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:56:56.051+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.038 seconds
[2024-07-24T00:56:59.086+0000] {processor.py:161} INFO - Started process (PID=4033) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:56:59.087+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T00:56:59.088+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:56:59.088+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:56:59.101+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:56:59.100+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 846, in exec_module
  File "<frozen importlib._bootstrap_external>", line 983, in get_code
  File "<frozen importlib._bootstrap_external>", line 913, in source_to_code
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 12
    from pipelines.aws
                      ^
SyntaxError: invalid syntax
[2024-07-24T00:56:59.101+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:56:59.117+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.036 seconds
[2024-07-24T00:57:09.150+0000] {processor.py:161} INFO - Started process (PID=4034) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:57:09.151+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T00:57:09.152+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:57:09.152+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:57:09.247+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:57:09.248+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:57:09.301+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:57:09.301+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:57:09.474+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T00:57:09.474+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T00:57:09.477+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:57:09.475+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 12, in <module>
    from pipelines.aws_pipeline import upload_s3_pipeline
ModuleNotFoundError: No module named 'pipelines.aws_pipeline'
[2024-07-24T00:57:09.478+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:57:09.496+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.351 seconds
[2024-07-24T00:57:12.547+0000] {processor.py:161} INFO - Started process (PID=4036) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:57:12.548+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T00:57:12.549+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:57:12.548+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:57:12.638+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:57:12.638+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:57:12.681+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:57:12.682+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:57:12.837+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T00:57:12.837+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T00:57:12.839+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:57:12.838+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 12, in <module>
    from pipelines.aws_pipeline import upload_s3_pipeline
ModuleNotFoundError: No module named 'pipelines.aws_pipeline'
[2024-07-24T00:57:12.840+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:57:12.856+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.315 seconds
[2024-07-24T00:57:13.898+0000] {processor.py:161} INFO - Started process (PID=4038) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:57:13.899+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T00:57:13.900+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:57:13.900+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:57:13.994+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:57:13.994+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:57:14.032+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:57:14.033+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:57:14.186+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T00:57:14.187+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T00:57:14.189+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:57:14.188+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 12, in <module>
    from pipelines.aws_pipeline import upload_s3_pipeline
ModuleNotFoundError: No module named 'pipelines.aws_pipeline'
[2024-07-24T00:57:14.190+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:57:14.207+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.313 seconds
[2024-07-24T00:57:16.255+0000] {processor.py:161} INFO - Started process (PID=4040) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:57:16.256+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T00:57:16.256+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:57:16.256+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:57:16.353+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:57:16.354+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:57:16.392+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:57:16.392+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:57:16.559+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T00:57:16.559+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T00:57:16.561+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:57:16.560+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 12, in <module>
    from pipelines.aws_pipeline import upload_s3_pipeline
ModuleNotFoundError: No module named 'pipelines.aws_pipeline'
[2024-07-24T00:57:16.562+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:57:16.578+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.329 seconds
[2024-07-24T00:57:19.276+0000] {processor.py:161} INFO - Started process (PID=4042) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:57:19.276+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T00:57:19.277+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:57:19.277+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:57:19.369+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:57:19.370+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:57:19.409+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:57:19.410+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:57:19.563+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T00:57:19.564+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T00:57:19.566+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:57:19.564+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 12, in <module>
    from pipelines.aws_pipeline import upload_s3_pipeline
ModuleNotFoundError: No module named 'pipelines.aws_pipeline'
[2024-07-24T00:57:19.567+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:57:19.583+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.313 seconds
[2024-07-24T00:57:24.309+0000] {processor.py:161} INFO - Started process (PID=4044) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:57:24.309+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T00:57:24.310+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:57:24.310+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:57:24.399+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:57:24.399+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:57:24.435+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:57:24.436+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:57:24.611+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T00:57:24.612+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T00:57:24.614+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:57:24.612+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 12, in <module>
    from pipelines.aws_pipeline import upload_s3_pipeline
ModuleNotFoundError: No module named 'pipelines.aws_pipeline'
[2024-07-24T00:57:24.615+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:57:24.630+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.327 seconds
[2024-07-24T00:57:32.681+0000] {processor.py:161} INFO - Started process (PID=4046) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:57:32.682+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T00:57:32.682+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:57:32.682+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:57:32.778+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:57:32.779+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:57:32.819+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:57:32.820+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:57:32.975+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T00:57:32.976+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T00:57:32.978+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:57:32.976+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 12, in <module>
    from pipelines.aws_pipeline import upload_s3_pipeline
ModuleNotFoundError: No module named 'pipelines.aws_pipeline'
[2024-07-24T00:57:32.979+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:57:33.001+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.325 seconds
[2024-07-24T00:58:03.188+0000] {processor.py:161} INFO - Started process (PID=4048) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:58:03.189+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T00:58:03.190+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:58:03.190+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:58:03.298+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:58:03.299+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:58:03.349+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:58:03.349+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:58:03.535+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T00:58:03.535+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T00:58:03.538+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:58:03.536+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 12, in <module>
    from pipelines.aws_pipeline import upload_s3_pipeline
ModuleNotFoundError: No module named 'pipelines.aws_pipeline'
[2024-07-24T00:58:03.539+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:58:03.559+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.378 seconds
[2024-07-24T00:58:10.227+0000] {processor.py:161} INFO - Started process (PID=4050) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:58:10.228+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T00:58:10.229+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:58:10.229+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:58:10.243+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:58:10.242+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 846, in exec_module
  File "<frozen importlib._bootstrap_external>", line 983, in get_code
  File "<frozen importlib._bootstrap_external>", line 913, in source_to_code
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 12
    from pipelines.
                   ^
SyntaxError: invalid syntax
[2024-07-24T00:58:10.243+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:58:10.260+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.039 seconds
[2024-07-24T00:58:25.333+0000] {processor.py:161} INFO - Started process (PID=4051) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:58:25.334+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T00:58:25.335+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:58:25.335+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:58:25.445+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:58:25.445+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:58:25.488+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:58:25.488+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:58:25.665+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T00:58:25.666+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T00:58:25.671+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:58:25.669+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 46, in <module>
    python_callable=upload_s3_pipeline,
NameError: name 'upload_s3_pipeline' is not defined
[2024-07-24T00:58:25.672+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:58:25.687+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.360 seconds
[2024-07-24T00:58:29.373+0000] {processor.py:161} INFO - Started process (PID=4053) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:58:29.374+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T00:58:29.374+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:58:29.374+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:58:29.412+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T00:58:29.413+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T00:58:29.483+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 12, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:58:29.484+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:58:29.521+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 12, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:58:29.522+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:58:29.725+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:58:29.827+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:58:29.826+0000] {dag.py:3089} INFO - Sync 1 DAGs
[2024-07-24T00:58:29.846+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:58:29.845+0000] {dag.py:3947} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24 00:00:00+00:00, run_after=2024-07-25 00:00:00+00:00
[2024-07-24T00:58:29.868+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.502 seconds
[2024-07-24T00:58:39.789+0000] {processor.py:161} INFO - Started process (PID=4055) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:58:39.789+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T00:58:39.790+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:58:39.790+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:58:39.819+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T00:58:39.820+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T00:58:39.892+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 12, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:58:39.893+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:58:39.931+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 12, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:58:39.931+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:58:40.083+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:58:40.094+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:58:40.093+0000] {dag.py:3089} INFO - Sync 1 DAGs
[2024-07-24T00:58:40.112+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:58:40.112+0000] {dag.py:3947} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24 00:00:00+00:00, run_after=2024-07-25 00:00:00+00:00
[2024-07-24T00:58:40.128+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.344 seconds
[2024-07-24T00:58:44.184+0000] {processor.py:161} INFO - Started process (PID=4057) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:58:44.186+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T00:58:44.187+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:58:44.186+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:58:44.199+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:58:44.199+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 846, in exec_module
  File "<frozen importlib._bootstrap_external>", line 983, in get_code
  File "<frozen importlib._bootstrap_external>", line 913, in source_to_code
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 47
    python_callable=upload_s3_pipeline.,
                                       ^
SyntaxError: invalid syntax
[2024-07-24T00:58:44.200+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:58:44.216+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.039 seconds
[2024-07-24T00:58:45.261+0000] {processor.py:161} INFO - Started process (PID=4058) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:58:45.262+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T00:58:45.263+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:58:45.263+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:58:45.295+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T00:58:45.295+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T00:58:45.368+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 12, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:58:45.369+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:58:45.406+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 12, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:58:45.407+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:58:45.569+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:58:45.567+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 47, in <module>
    python_callable=upload_s3_pipeline.upload_s3_pipeline,
AttributeError: 'function' object has no attribute 'upload_s3_pipeline'
[2024-07-24T00:58:45.570+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:58:45.593+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.337 seconds
[2024-07-24T00:58:48.285+0000] {processor.py:161} INFO - Started process (PID=4060) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:58:48.286+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T00:58:48.287+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:58:48.287+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:58:48.319+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T00:58:48.319+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T00:58:48.392+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 12, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:58:48.392+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:58:48.431+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 12, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:58:48.431+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:58:48.581+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:58:48.592+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:58:48.591+0000] {dag.py:3089} INFO - Sync 1 DAGs
[2024-07-24T00:58:48.611+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:58:48.610+0000] {dag.py:3947} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24 00:00:00+00:00, run_after=2024-07-25 00:00:00+00:00
[2024-07-24T00:58:48.628+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.349 seconds
[2024-07-24T00:59:18.795+0000] {processor.py:161} INFO - Started process (PID=4062) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:59:18.796+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T00:59:18.797+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:59:18.796+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:59:18.827+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T00:59:18.827+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T00:59:18.898+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 12, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:59:18.898+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:59:18.936+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 12, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T00:59:18.936+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T00:59:19.094+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T00:59:19.119+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:59:19.119+0000] {dag.py:3089} INFO - Sync 1 DAGs
[2024-07-24T00:59:19.141+0000] {logging_mixin.py:188} INFO - [2024-07-24T00:59:19.141+0000] {dag.py:3947} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24 00:00:00+00:00, run_after=2024-07-25 00:00:00+00:00
[2024-07-24T00:59:19.161+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.371 seconds
[2024-07-24T01:00:16.474+0000] {processor.py:161} INFO - Started process (PID=37) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:00:16.476+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T01:00:16.478+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:00:16.478+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:00:16.509+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T01:00:16.510+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T01:00:16.642+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 556, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 12, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T01:00:16.643+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T01:00:16.716+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 556, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 12, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T01:00:16.717+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T01:00:17.583+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:00:18.256+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:00:18.255+0000] {dag.py:3089} INFO - Sync 1 DAGs
[2024-07-24T01:00:18.312+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:00:18.312+0000] {dag.py:3947} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24 00:00:00+00:00, run_after=2024-07-25 00:00:00+00:00
[2024-07-24T01:00:18.349+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 1.883 seconds
[2024-07-24T01:00:48.618+0000] {processor.py:161} INFO - Started process (PID=39) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:00:48.619+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T01:00:48.625+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:00:48.624+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:00:48.659+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T01:00:48.660+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T01:00:48.775+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 12, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T01:00:48.776+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T01:00:48.830+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 12, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T01:00:48.831+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T01:00:49.001+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:00:49.019+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:00:49.018+0000] {dag.py:3089} INFO - Sync 1 DAGs
[2024-07-24T01:00:49.044+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:00:49.044+0000] {dag.py:3947} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24 00:00:00+00:00, run_after=2024-07-25 00:00:00+00:00
[2024-07-24T01:00:49.062+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.465 seconds
[2024-07-24T01:01:19.233+0000] {processor.py:161} INFO - Started process (PID=41) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:01:19.234+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T01:01:19.236+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:01:19.236+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:01:19.265+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T01:01:19.265+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T01:01:19.352+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 12, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T01:01:19.353+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T01:01:19.414+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 12, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T01:01:19.415+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T01:01:19.590+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:01:19.742+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:01:19.741+0000] {dag.py:3089} INFO - Sync 1 DAGs
[2024-07-24T01:01:19.760+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:01:19.760+0000] {dag.py:3947} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24 00:00:00+00:00, run_after=2024-07-25 00:00:00+00:00
[2024-07-24T01:01:19.777+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.554 seconds
[2024-07-24T01:01:49.944+0000] {processor.py:161} INFO - Started process (PID=43) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:01:49.945+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T01:01:49.947+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:01:49.946+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:01:49.975+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T01:01:49.976+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T01:01:50.064+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 12, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T01:01:50.065+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T01:01:50.131+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 12, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T01:01:50.132+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T01:01:50.296+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:01:50.323+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:01:50.322+0000] {dag.py:3089} INFO - Sync 1 DAGs
[2024-07-24T01:01:50.346+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:01:50.346+0000] {dag.py:3947} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24 00:00:00+00:00, run_after=2024-07-25 00:00:00+00:00
[2024-07-24T01:01:50.363+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.426 seconds
[2024-07-24T01:02:20.544+0000] {processor.py:161} INFO - Started process (PID=45) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:02:20.546+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T01:02:20.549+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:02:20.548+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:02:20.581+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T01:02:20.582+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T01:02:20.680+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 12, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T01:02:20.680+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T01:02:20.736+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 12, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T01:02:20.736+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T01:02:20.922+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:02:20.946+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:02:20.946+0000] {dag.py:3089} INFO - Sync 1 DAGs
[2024-07-24T01:02:20.968+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:02:20.968+0000] {dag.py:3947} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24 00:00:00+00:00, run_after=2024-07-25 00:00:00+00:00
[2024-07-24T01:02:20.988+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.451 seconds
[2024-07-24T01:02:51.191+0000] {processor.py:161} INFO - Started process (PID=47) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:02:51.193+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T01:02:51.196+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:02:51.195+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:02:51.232+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T01:02:51.233+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T01:02:51.351+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 12, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T01:02:51.352+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T01:02:51.418+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 12, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T01:02:51.419+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T01:02:51.631+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:02:51.658+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:02:51.657+0000] {dag.py:3089} INFO - Sync 1 DAGs
[2024-07-24T01:02:51.679+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:02:51.679+0000] {dag.py:3947} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24 00:00:00+00:00, run_after=2024-07-25 00:00:00+00:00
[2024-07-24T01:02:51.697+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.516 seconds
[2024-07-24T01:03:21.864+0000] {processor.py:161} INFO - Started process (PID=49) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:03:21.866+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T01:03:21.871+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:03:21.870+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:03:21.907+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T01:03:21.907+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T01:03:22.050+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 12, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T01:03:22.051+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T01:03:22.139+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 12, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T01:03:22.139+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T01:03:22.477+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:03:22.539+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:03:22.538+0000] {dag.py:3089} INFO - Sync 1 DAGs
[2024-07-24T01:03:22.589+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:03:22.589+0000] {dag.py:3947} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24 00:00:00+00:00, run_after=2024-07-25 00:00:00+00:00
[2024-07-24T01:03:22.636+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.789 seconds
[2024-07-24T01:03:52.838+0000] {processor.py:161} INFO - Started process (PID=51) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:03:52.849+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T01:03:52.852+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:03:52.852+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:03:52.880+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T01:03:52.881+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T01:03:52.997+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 12, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T01:03:52.997+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T01:03:53.116+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 12, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T01:03:53.117+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T01:03:53.413+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:03:53.454+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:03:53.453+0000] {dag.py:3089} INFO - Sync 1 DAGs
[2024-07-24T01:03:53.497+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:03:53.497+0000] {dag.py:3947} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24 00:00:00+00:00, run_after=2024-07-25 00:00:00+00:00
[2024-07-24T01:03:53.524+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.700 seconds
[2024-07-24T01:04:23.744+0000] {processor.py:161} INFO - Started process (PID=53) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:04:23.751+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T01:04:23.758+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:04:23.757+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:04:23.797+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T01:04:23.798+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T01:04:24.007+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 12, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T01:04:24.008+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T01:04:24.090+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 12, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T01:04:24.091+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T01:04:24.317+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:04:24.343+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:04:24.343+0000] {dag.py:3089} INFO - Sync 1 DAGs
[2024-07-24T01:04:24.366+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:04:24.365+0000] {dag.py:3947} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24 00:00:00+00:00, run_after=2024-07-25 00:00:00+00:00
[2024-07-24T01:04:24.384+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.687 seconds
[2024-07-24T01:04:54.572+0000] {processor.py:161} INFO - Started process (PID=55) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:04:54.573+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T01:04:54.576+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:04:54.575+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:04:54.605+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T01:04:54.605+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T01:04:54.706+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 12, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T01:04:54.707+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T01:04:54.782+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 12, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T01:04:54.783+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T01:04:54.960+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:04:54.993+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:04:54.993+0000] {dag.py:3089} INFO - Sync 1 DAGs
[2024-07-24T01:04:55.015+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:04:55.015+0000] {dag.py:3947} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24 00:00:00+00:00, run_after=2024-07-25 00:00:00+00:00
[2024-07-24T01:04:55.038+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.475 seconds
[2024-07-24T01:05:25.229+0000] {processor.py:161} INFO - Started process (PID=57) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:05:25.230+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T01:05:25.232+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:05:25.231+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:05:25.284+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T01:05:25.285+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T01:05:25.380+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 12, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T01:05:25.380+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T01:05:25.430+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 12, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T01:05:25.431+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T01:05:25.613+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:05:25.638+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:05:25.638+0000] {dag.py:3089} INFO - Sync 1 DAGs
[2024-07-24T01:05:25.661+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:05:25.661+0000] {dag.py:3947} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24 00:00:00+00:00, run_after=2024-07-25 00:00:00+00:00
[2024-07-24T01:05:25.678+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.458 seconds
[2024-07-24T01:05:55.847+0000] {processor.py:161} INFO - Started process (PID=59) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:05:55.848+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T01:05:55.850+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:05:55.850+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:05:55.890+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T01:05:55.891+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T01:05:55.972+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 12, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T01:05:55.972+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T01:05:56.020+0000] {logging_mixin.py:188} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.1 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 842, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 168, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 241, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 476, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 619, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1087, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 238, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 12, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-07-24T01:05:56.020+0000] {logging_mixin.py:188} WARNING - AttributeError: _ARRAY_API not found
[2024-07-24T01:05:56.199+0000] {processor.py:840} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:05:56.227+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:05:56.226+0000] {dag.py:3089} INFO - Sync 1 DAGs
[2024-07-24T01:05:56.251+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:05:56.251+0000] {dag.py:3947} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24 00:00:00+00:00, run_after=2024-07-25 00:00:00+00:00
[2024-07-24T01:05:56.270+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.429 seconds
[2024-07-24T01:06:26.439+0000] {processor.py:161} INFO - Started process (PID=61) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:06:26.440+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T01:06:26.442+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:06:26.442+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:06:26.489+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:06:26.478+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 1, in <module>
    import s3fs
ModuleNotFoundError: No module named 's3fs'
[2024-07-24T01:06:26.491+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:06:26.514+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.081 seconds
[2024-07-24T01:06:56.587+0000] {processor.py:161} INFO - Started process (PID=62) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:06:56.588+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T01:06:56.590+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:06:56.590+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:06:56.633+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:06:56.625+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 1, in <module>
    import s3fs
ModuleNotFoundError: No module named 's3fs'
[2024-07-24T01:06:56.636+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:06:56.657+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.077 seconds
[2024-07-24T01:07:26.829+0000] {processor.py:161} INFO - Started process (PID=63) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:07:26.830+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T01:07:26.831+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:07:26.831+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:07:26.873+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:07:26.865+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 1, in <module>
    import s3fs
ModuleNotFoundError: No module named 's3fs'
[2024-07-24T01:07:26.875+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:07:26.896+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.075 seconds
[2024-07-24T01:07:56.984+0000] {processor.py:161} INFO - Started process (PID=64) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:07:56.985+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T01:07:56.986+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:07:56.986+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:07:57.015+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:07:57.010+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 43
    s3.put(file_path, bucket_name +'/rawr', object_name)
                                                        ^
SyntaxError: unexpected EOF while parsing
[2024-07-24T01:07:57.017+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:07:57.033+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.056 seconds
[2024-07-24T01:08:27.204+0000] {processor.py:161} INFO - Started process (PID=65) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:08:27.205+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T01:08:27.206+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:08:27.206+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:08:27.235+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:08:27.231+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 43
    s3.put(file_path, bucket_name +'/raw/' + s3 , object_name)
                                                              ^
SyntaxError: unexpected EOF while parsing
[2024-07-24T01:08:27.237+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:08:27.255+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.059 seconds
[2024-07-24T01:08:57.365+0000] {processor.py:161} INFO - Started process (PID=66) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:08:57.367+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T01:08:57.368+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:08:57.368+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:08:57.403+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:08:57.397+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 45
    except 
           ^
SyntaxError: invalid syntax
[2024-07-24T01:08:57.404+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:08:57.423+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.068 seconds
[2024-07-24T01:09:27.551+0000] {processor.py:161} INFO - Started process (PID=67) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:09:27.552+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T01:09:27.553+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:09:27.552+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:09:27.594+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:09:27.586+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 1, in <module>
    import s3fs
ModuleNotFoundError: No module named 's3fs'
[2024-07-24T01:09:27.595+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:09:27.614+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.068 seconds
[2024-07-24T01:09:57.719+0000] {processor.py:161} INFO - Started process (PID=68) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:09:57.720+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T01:09:57.721+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:09:57.721+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:09:57.755+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:09:57.748+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 1, in <module>
    import s3fs
ModuleNotFoundError: No module named 's3fs'
[2024-07-24T01:09:57.757+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:09:57.776+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.063 seconds
[2024-07-24T01:10:27.958+0000] {processor.py:161} INFO - Started process (PID=69) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:10:27.959+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T01:10:27.961+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:10:27.960+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:10:28.000+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:10:27.992+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 1, in <module>
    import s3fs
ModuleNotFoundError: No module named 's3fs'
[2024-07-24T01:10:28.002+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:10:28.027+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.074 seconds
[2024-07-24T01:10:58.125+0000] {processor.py:161} INFO - Started process (PID=70) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:10:58.126+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T01:10:58.127+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:10:58.127+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:10:58.162+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:10:58.154+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 1, in <module>
    import s3fs
ModuleNotFoundError: No module named 's3fs'
[2024-07-24T01:10:58.163+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:10:58.183+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.065 seconds
[2024-07-24T01:11:28.340+0000] {processor.py:161} INFO - Started process (PID=71) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:11:28.341+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T01:11:28.342+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:11:28.341+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:11:28.372+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:11:28.365+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 1, in <module>
    import s3fs
ModuleNotFoundError: No module named 's3fs'
[2024-07-24T01:11:28.373+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:11:28.391+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.058 seconds
[2024-07-24T01:11:58.489+0000] {processor.py:161} INFO - Started process (PID=72) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:11:58.490+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T01:11:58.490+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:11:58.490+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:11:58.526+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T01:11:58.527+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T01:11:58.533+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:11:58.527+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 40, in <module>
    def upload_to_s3(s3: s3fs.S3FileSystem, file_path: str, bucket:str, s3_file_name: str):
NameError: name 's3fs' is not defined
[2024-07-24T01:11:58.535+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:11:58.549+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.066 seconds
[2024-07-24T01:12:28.686+0000] {processor.py:161} INFO - Started process (PID=73) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:12:28.687+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T01:12:28.688+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:12:28.687+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:12:28.719+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T01:12:28.720+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T01:12:28.726+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:12:28.720+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 40, in <module>
    def upload_to_s3(s3: s3fs.S3FileSystem, file_path: str, bucket:str, s3_file_name: str):
NameError: name 's3fs' is not defined
[2024-07-24T01:12:28.728+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:12:28.744+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.064 seconds
[2024-07-24T01:12:58.855+0000] {processor.py:161} INFO - Started process (PID=74) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:12:58.856+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T01:12:58.857+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:12:58.857+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:12:58.907+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:12:58.898+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 1, in <module>
    import s3
ModuleNotFoundError: No module named 's3'
[2024-07-24T01:12:58.909+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:12:58.928+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.079 seconds
[2024-07-24T01:13:29.058+0000] {processor.py:161} INFO - Started process (PID=75) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:13:29.060+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T01:13:29.060+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:13:29.060+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:13:29.089+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:13:29.081+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 1, in <module>
    import s3
ModuleNotFoundError: No module named 's3'
[2024-07-24T01:13:29.091+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:13:29.107+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.055 seconds
[2024-07-24T01:13:59.256+0000] {processor.py:161} INFO - Started process (PID=76) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:13:59.260+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T01:13:59.262+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:13:59.261+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:13:59.300+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:13:59.293+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 1
    import 
           ^
SyntaxError: invalid syntax
[2024-07-24T01:13:59.302+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:13:59.344+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.105 seconds
[2024-07-24T01:14:29.423+0000] {processor.py:161} INFO - Started process (PID=77) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:14:29.424+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T01:14:29.425+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:14:29.424+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:14:29.457+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:14:29.452+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 1
    import 
           ^
SyntaxError: invalid syntax
[2024-07-24T01:14:29.460+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:14:29.482+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.066 seconds
[2024-07-24T01:14:59.634+0000] {processor.py:161} INFO - Started process (PID=78) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:14:59.635+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T01:14:59.636+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:14:59.636+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:14:59.665+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:14:59.660+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 1
    import 
           ^
SyntaxError: invalid syntax
[2024-07-24T01:14:59.667+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:14:59.684+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.054 seconds
[2024-07-24T01:15:29.815+0000] {processor.py:161} INFO - Started process (PID=79) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:15:29.816+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T01:15:29.818+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:15:29.817+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:15:29.851+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:15:29.845+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 1
    import 
           ^
SyntaxError: invalid syntax
[2024-07-24T01:15:29.853+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:15:29.873+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.063 seconds
[2024-07-24T01:15:59.971+0000] {processor.py:161} INFO - Started process (PID=80) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:15:59.972+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T01:15:59.973+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:15:59.973+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:16:00.004+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:15:59.999+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 1
    import 
           ^
SyntaxError: invalid syntax
[2024-07-24T01:16:00.006+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:16:00.027+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.061 seconds
[2024-07-24T01:16:30.150+0000] {processor.py:161} INFO - Started process (PID=81) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:16:30.151+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T01:16:30.152+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:16:30.151+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:16:30.181+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:16:30.176+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 1
    import 
           ^
SyntaxError: invalid syntax
[2024-07-24T01:16:30.183+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:16:30.198+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.054 seconds
[2024-07-24T01:17:00.304+0000] {processor.py:161} INFO - Started process (PID=82) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:17:00.305+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T01:17:00.306+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:17:00.306+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:17:00.337+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:17:00.332+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 1
    import 
           ^
SyntaxError: invalid syntax
[2024-07-24T01:17:00.338+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:17:00.354+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.055 seconds
[2024-07-24T01:17:30.487+0000] {processor.py:161} INFO - Started process (PID=83) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:17:30.488+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T01:17:30.489+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:17:30.488+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:17:30.520+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:17:30.515+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 1
    import 
           ^
SyntaxError: invalid syntax
[2024-07-24T01:17:30.521+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:17:30.537+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.056 seconds
[2024-07-24T01:18:00.646+0000] {processor.py:161} INFO - Started process (PID=84) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:18:00.647+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T01:18:00.648+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:18:00.648+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:18:00.677+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:18:00.672+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 1
    import 
           ^
SyntaxError: invalid syntax
[2024-07-24T01:18:00.678+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:18:00.694+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.054 seconds
[2024-07-24T01:18:30.819+0000] {processor.py:161} INFO - Started process (PID=85) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:18:30.820+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T01:18:30.820+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:18:30.820+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:18:30.848+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:18:30.844+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 1
    import 
           ^
SyntaxError: invalid syntax
[2024-07-24T01:18:30.850+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:18:30.866+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.053 seconds
[2024-07-24T01:19:00.970+0000] {processor.py:161} INFO - Started process (PID=86) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:19:00.971+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T01:19:00.971+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:19:00.971+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:19:01.000+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:19:00.995+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 1
    import 
           ^
SyntaxError: invalid syntax
[2024-07-24T01:19:01.001+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:19:01.017+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.054 seconds
[2024-07-24T01:19:31.150+0000] {processor.py:161} INFO - Started process (PID=87) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:19:31.151+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T01:19:31.152+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:19:31.152+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:19:31.531+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T01:19:31.532+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T01:19:31.538+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:19:31.532+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 41, in <module>
    def upload_to_s3(s3: s3fs.S3FileSystem, file_path: str, bucket:str, s3_file_name: str):
NameError: name 's3fs' is not defined
[2024-07-24T01:19:31.540+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:19:31.559+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.416 seconds
[2024-07-24T01:20:01.705+0000] {processor.py:161} INFO - Started process (PID=88) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:20:01.706+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T01:20:01.708+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:20:01.707+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:20:01.746+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T01:20:01.747+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T01:20:01.754+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:20:01.747+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 40, in <module>
    def upload_to_s3(s3: s3fs.S3FileSystem, file_path: str, bucket:str, s3_file_name: str):
NameError: name 's3fs' is not defined
[2024-07-24T01:20:01.756+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:20:01.781+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.082 seconds
[2024-07-24T01:20:31.868+0000] {processor.py:161} INFO - Started process (PID=89) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:20:31.869+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T01:20:31.869+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:20:31.869+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:20:31.900+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T01:20:31.900+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T01:20:31.907+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:20:31.901+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 40, in <module>
    def upload_to_s3(s3: s3fs.S3FileSystem, file_path: str, bucket:str, s3_file_name: str):
NameError: name 's3fs' is not defined
[2024-07-24T01:20:31.909+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:20:31.926+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.063 seconds
[2024-07-24T01:21:02.055+0000] {processor.py:161} INFO - Started process (PID=90) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:21:02.056+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T01:21:02.056+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:21:02.056+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:21:02.085+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T01:21:02.086+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T01:21:02.093+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:21:02.086+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 40, in <module>
    def upload_to_s3(s3: s3fs.S3FileSystem, file_path: str, bucket:str, s3_file_name: str):
NameError: name 's3fs' is not defined
[2024-07-24T01:21:02.094+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:21:02.111+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.062 seconds
[2024-07-24T01:21:32.206+0000] {processor.py:161} INFO - Started process (PID=91) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:21:32.207+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T01:21:32.208+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:21:32.207+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:21:32.238+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T01:21:32.239+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T01:21:32.245+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:21:32.239+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 40, in <module>
    def upload_to_s3(s3: s3fs.S3FileSystem, file_path: str, bucket:str, s3_file_name: str):
NameError: name 's3fs' is not defined
[2024-07-24T01:21:32.247+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:21:32.272+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.071 seconds
[2024-07-24T01:22:02.395+0000] {processor.py:161} INFO - Started process (PID=92) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:22:02.397+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T01:22:02.398+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:22:02.398+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:22:02.432+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T01:22:02.433+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T01:22:02.440+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:22:02.433+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 40, in <module>
    def upload_to_s3(s3: s3fs.S3FileSystem, file_path: str, bucket:str, s3_file_name: str):
NameError: name 's3fs' is not defined
[2024-07-24T01:22:02.442+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:22:02.457+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.068 seconds
[2024-07-24T01:22:32.571+0000] {processor.py:161} INFO - Started process (PID=93) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:22:32.572+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T01:22:32.573+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:22:32.573+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:22:32.605+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T01:22:32.606+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T01:22:32.613+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:22:32.606+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 40, in <module>
    def upload_to_s3(s3: s3fs.S3FileSystem, file_path: str, bucket:str, s3_file_name: str):
NameError: name 's3fs' is not defined
[2024-07-24T01:22:32.615+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:22:32.631+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.067 seconds
[2024-07-24T01:23:02.741+0000] {processor.py:161} INFO - Started process (PID=94) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:23:02.741+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T01:23:02.742+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:23:02.742+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:23:02.773+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T01:23:02.774+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T01:23:02.781+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:23:02.774+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 40, in <module>
    def upload_to_s3(s3: s3fs.S3FileSystem, file_path: str, bucket:str, s3_file_name: str):
NameError: name 's3fs' is not defined
[2024-07-24T01:23:02.782+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:23:02.799+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.064 seconds
[2024-07-24T01:23:32.941+0000] {processor.py:161} INFO - Started process (PID=95) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:23:32.942+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T01:23:32.943+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:23:32.943+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:23:32.977+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T01:23:32.978+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T01:23:32.984+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:23:32.978+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 40, in <module>
    def upload_to_s3(s3: s3fs.S3FileSystem, file_path: str, bucket:str, s3_file_name: str):
NameError: name 's3fs' is not defined
[2024-07-24T01:23:32.986+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:23:33.003+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.068 seconds
[2024-07-24T01:24:03.130+0000] {processor.py:161} INFO - Started process (PID=96) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:24:03.131+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T01:24:03.132+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:24:03.132+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:24:03.164+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T01:24:03.165+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T01:24:03.172+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:24:03.165+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 40, in <module>
    def upload_to_s3(s3: s3fs.S3FileSystem, file_path: str, bucket:str, s3_file_name: str):
NameError: name 's3fs' is not defined
[2024-07-24T01:24:03.174+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:24:03.194+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.083 seconds
[2024-07-24T01:24:33.304+0000] {processor.py:161} INFO - Started process (PID=97) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:24:33.308+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T01:24:33.309+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:24:33.309+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:24:33.345+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T01:24:33.346+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T01:24:33.352+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:24:33.346+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 40, in <module>
    def upload_to_s3(s3: s3fs.S3FileSystem, file_path: str, bucket:str, s3_file_name: str):
NameError: name 's3fs' is not defined
[2024-07-24T01:24:33.354+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:24:33.372+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.078 seconds
[2024-07-24T01:25:03.518+0000] {processor.py:161} INFO - Started process (PID=98) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:25:03.519+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T01:25:03.520+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:25:03.520+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:25:03.556+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T01:25:03.557+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T01:25:03.565+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:25:03.557+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 40, in <module>
    def upload_to_s3(s3: s3fs.S3FileSystem, file_path: str, bucket:str, s3_file_name: str):
NameError: name 's3fs' is not defined
[2024-07-24T01:25:03.567+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:25:03.595+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.083 seconds
[2024-07-24T01:25:33.751+0000] {processor.py:161} INFO - Started process (PID=99) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:25:33.752+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T01:25:33.753+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:25:33.753+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:25:33.790+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T01:25:33.791+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T01:25:33.799+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:25:33.791+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 40, in <module>
    def upload_to_s3(s3: s3fs.S3FileSystem, file_path: str, bucket:str, s3_file_name: str):
NameError: name 's3fs' is not defined
[2024-07-24T01:25:33.801+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:25:33.823+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.082 seconds
[2024-07-24T01:26:03.932+0000] {processor.py:161} INFO - Started process (PID=100) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:26:03.933+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T01:26:03.934+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:26:03.934+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:26:03.970+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T01:26:03.970+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T01:26:03.977+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:26:03.970+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 40, in <module>
    def upload_to_s3(s3: s3fs.S3FileSystem, file_path: str, bucket:str, s3_file_name: str):
NameError: name 's3fs' is not defined
[2024-07-24T01:26:03.979+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:26:04.000+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.073 seconds
[2024-07-24T01:26:34.137+0000] {processor.py:161} INFO - Started process (PID=101) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:26:34.138+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T01:26:34.139+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:26:34.139+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:26:34.171+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T01:26:34.172+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T01:26:34.180+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:26:34.172+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 40, in <module>
    def upload_to_s3(s3: s3fs.S3FileSystem, file_path: str, bucket:str, s3_file_name: str):
NameError: name 's3fs' is not defined
[2024-07-24T01:26:34.182+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:26:34.205+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.073 seconds
[2024-07-24T01:27:04.310+0000] {processor.py:161} INFO - Started process (PID=102) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:27:04.312+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T01:27:04.314+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:27:04.313+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:27:04.346+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T01:27:04.347+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T01:27:04.357+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:27:04.347+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 40, in <module>
    def upload_to_s3(s3: s3fs.S3FileSystem, file_path: str, bucket:str, s3_file_name: str):
NameError: name 's3fs' is not defined
[2024-07-24T01:27:04.358+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:27:04.378+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.079 seconds
[2024-07-24T01:27:34.512+0000] {processor.py:161} INFO - Started process (PID=103) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:27:34.512+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T01:27:34.513+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:27:34.513+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:27:34.541+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T01:27:34.542+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T01:27:34.549+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:27:34.542+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 40, in <module>
    def upload_to_s3(s3: s3fs.S3FileSystem, file_path: str, bucket:str, s3_file_name: str):
NameError: name 's3fs' is not defined
[2024-07-24T01:27:34.550+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:27:34.567+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.060 seconds
[2024-07-24T01:28:04.678+0000] {processor.py:161} INFO - Started process (PID=104) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:28:04.680+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T01:28:04.681+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:28:04.680+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:28:04.714+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T01:28:04.714+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T01:28:04.721+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:28:04.715+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 40, in <module>
    def upload_to_s3(s3: s3fs.S3FileSystem, file_path: str, bucket:str, s3_file_name: str):
NameError: name 's3fs' is not defined
[2024-07-24T01:28:04.723+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:28:04.738+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.067 seconds
[2024-07-24T01:28:34.882+0000] {processor.py:161} INFO - Started process (PID=105) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:28:34.884+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T01:28:34.887+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:28:34.886+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:28:34.924+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T01:28:34.924+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T01:28:34.932+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:28:34.925+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 40, in <module>
    def upload_to_s3(s3: s3fs.S3FileSystem, file_path: str, bucket:str, s3_file_name: str):
NameError: name 's3fs' is not defined
[2024-07-24T01:28:34.934+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:28:34.959+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.090 seconds
[2024-07-24T01:29:05.062+0000] {processor.py:161} INFO - Started process (PID=106) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:29:05.063+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T01:29:05.064+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:29:05.064+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:29:05.145+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T01:29:05.146+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T01:29:05.155+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:29:05.146+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 40, in <module>
    def upload_to_s3(s3: s3fs.S3FileSystem, file_path: str, bucket:str, s3_file_name: str):
NameError: name 's3fs' is not defined
[2024-07-24T01:29:05.157+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:29:05.217+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.164 seconds
[2024-07-24T01:29:35.328+0000] {processor.py:161} INFO - Started process (PID=107) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:29:35.348+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T01:29:35.349+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:29:35.349+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:29:35.386+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T01:29:35.387+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T01:29:35.395+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:29:35.387+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 40, in <module>
    def upload_to_s3(s3: s3fs.S3FileSystem, file_path: str, bucket:str, s3_file_name: str):
NameError: name 's3fs' is not defined
[2024-07-24T01:29:35.397+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:29:35.430+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.130 seconds
[2024-07-24T01:30:05.579+0000] {processor.py:161} INFO - Started process (PID=108) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:30:05.583+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T01:30:05.587+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:30:05.586+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:30:05.642+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T01:30:05.642+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T01:30:05.650+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:30:05.642+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 40, in <module>
    def upload_to_s3(s3: s3fs.S3FileSystem, file_path: str, bucket:str, s3_file_name: str):
NameError: name 's3fs' is not defined
[2024-07-24T01:30:05.651+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:30:05.754+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.212 seconds
[2024-07-24T01:30:35.932+0000] {processor.py:161} INFO - Started process (PID=109) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:30:35.934+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T01:30:35.935+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:30:35.935+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:30:35.969+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T01:30:35.970+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T01:30:35.981+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:30:35.970+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 40, in <module>
    def upload_to_s3(s3: s3fs.S3FileSystem, file_path: str, bucket:str, s3_file_name: str):
NameError: name 's3fs' is not defined
[2024-07-24T01:30:35.983+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:30:36.009+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.083 seconds
[2024-07-24T01:31:06.125+0000] {processor.py:161} INFO - Started process (PID=110) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:31:06.127+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T01:31:06.131+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:31:06.131+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:31:06.213+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T01:31:06.214+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T01:31:06.226+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:31:06.215+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 40, in <module>
    def upload_to_s3(s3: s3fs.S3FileSystem, file_path: str, bucket:str, s3_file_name: str):
NameError: name 's3fs' is not defined
[2024-07-24T01:31:06.229+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:31:06.300+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.200 seconds
[2024-07-24T01:31:36.354+0000] {processor.py:161} INFO - Started process (PID=111) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:31:36.356+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T01:31:36.358+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:31:36.357+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:31:36.396+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T01:31:36.396+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T01:31:36.405+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:31:36.397+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 40, in <module>
    def upload_to_s3(s3: s3fs.S3FileSystem, file_path: str, bucket:str, s3_file_name: str):
NameError: name 's3fs' is not defined
[2024-07-24T01:31:36.407+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:31:36.430+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.090 seconds
[2024-07-24T01:32:06.499+0000] {processor.py:161} INFO - Started process (PID=112) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:32:06.501+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T01:32:06.502+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:32:06.502+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:32:06.534+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T01:32:06.535+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T01:32:06.542+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:32:06.535+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 40, in <module>
    def upload_to_s3(s3: s3fs.S3FileSystem, file_path: str, bucket:str, s3_file_name: str):
NameError: name 's3fs' is not defined
[2024-07-24T01:32:06.543+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:32:06.560+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.070 seconds
[2024-07-24T01:32:36.763+0000] {processor.py:161} INFO - Started process (PID=113) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:32:36.764+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T01:32:36.765+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:32:36.765+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:32:36.814+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T01:32:36.814+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T01:32:36.826+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:32:36.815+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 40, in <module>
    def upload_to_s3(s3: s3fs.S3FileSystem, file_path: str, bucket:str, s3_file_name: str):
NameError: name 's3fs' is not defined
[2024-07-24T01:32:36.828+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:32:36.866+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.112 seconds
[2024-07-24T01:33:06.934+0000] {processor.py:161} INFO - Started process (PID=114) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:33:06.935+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T01:33:06.936+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:33:06.936+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:33:06.968+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T01:33:06.969+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T01:33:06.977+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:33:06.969+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 40, in <module>
    def upload_to_s3(s3: s3fs.S3FileSystem, file_path: str, bucket:str, s3_file_name: str):
NameError: name 's3fs' is not defined
[2024-07-24T01:33:06.979+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:33:07.003+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.076 seconds
[2024-07-24T01:33:37.096+0000] {processor.py:161} INFO - Started process (PID=115) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:33:37.097+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T01:33:37.098+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:33:37.097+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:33:37.134+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T01:33:37.135+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T01:33:37.142+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:33:37.135+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 40, in <module>
    def upload_to_s3(s3: s3fs.S3FileSystem, file_path: str, bucket:str, s3_file_name: str):
NameError: name 's3fs' is not defined
[2024-07-24T01:33:37.144+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:33:37.174+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.090 seconds
[2024-07-24T01:34:07.353+0000] {processor.py:161} INFO - Started process (PID=116) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:34:07.354+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T01:34:07.356+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:34:07.355+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:34:07.390+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T01:34:07.391+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T01:34:07.399+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:34:07.391+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 40, in <module>
    def upload_to_s3(s3: s3fs.S3FileSystem, file_path: str, bucket:str, s3_file_name: str):
NameError: name 's3fs' is not defined
[2024-07-24T01:34:07.400+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:34:07.425+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.081 seconds
[2024-07-24T01:34:37.507+0000] {processor.py:161} INFO - Started process (PID=117) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:34:37.508+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T01:34:37.509+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:34:37.509+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:34:37.541+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T01:34:37.541+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T01:34:37.550+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:34:37.542+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 40, in <module>
    def upload_to_s3(s3: s3fs.S3FileSystem, file_path: str, bucket:str, s3_file_name: str):
NameError: name 's3fs' is not defined
[2024-07-24T01:34:37.551+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:34:37.578+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.076 seconds
[2024-07-24T01:35:07.722+0000] {processor.py:161} INFO - Started process (PID=118) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:35:07.724+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T01:35:07.725+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:35:07.724+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:35:07.757+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T01:35:07.757+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T01:35:07.764+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:35:07.757+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 40, in <module>
    def upload_to_s3(s3: s3fs.S3FileSystem, file_path: str, bucket:str, s3_file_name: str):
NameError: name 's3fs' is not defined
[2024-07-24T01:35:07.766+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:35:07.786+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.070 seconds
[2024-07-24T01:35:37.920+0000] {processor.py:161} INFO - Started process (PID=119) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:35:37.921+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T01:35:37.922+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:35:37.922+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:35:37.966+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T01:35:37.967+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T01:35:37.977+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:35:37.967+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 40, in <module>
    def upload_to_s3(s3: s3fs.S3FileSystem, file_path: str, bucket:str, s3_file_name: str):
NameError: name 's3fs' is not defined
[2024-07-24T01:35:37.979+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:35:38.021+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.109 seconds
[2024-07-24T01:36:08.077+0000] {processor.py:161} INFO - Started process (PID=120) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:36:08.078+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T01:36:08.079+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:36:08.079+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:36:08.113+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T01:36:08.113+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T01:36:08.121+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:36:08.114+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 40, in <module>
    def upload_to_s3(s3: s3fs.S3FileSystem, file_path: str, bucket:str, s3_file_name: str):
NameError: name 's3fs' is not defined
[2024-07-24T01:36:08.123+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:36:08.145+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.076 seconds
[2024-07-24T01:36:38.313+0000] {processor.py:161} INFO - Started process (PID=121) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:36:38.314+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T01:36:38.315+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:36:38.315+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:36:38.350+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T01:36:38.350+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T01:36:38.359+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:36:38.351+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 40, in <module>
    def upload_to_s3(s3: s3fs.S3FileSystem, file_path: str, bucket:str, s3_file_name: str):
NameError: name 's3fs' is not defined
[2024-07-24T01:36:38.362+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:36:38.407+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.103 seconds
[2024-07-24T01:37:08.466+0000] {processor.py:161} INFO - Started process (PID=122) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:37:08.467+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T01:37:08.468+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:37:08.467+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:37:08.505+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T01:37:08.506+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T01:37:08.514+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:37:08.506+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 40, in <module>
    def upload_to_s3(s3: s3fs.S3FileSystem, file_path: str, bucket:str, s3_file_name: str):
NameError: name 's3fs' is not defined
[2024-07-24T01:37:08.516+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:37:08.563+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.103 seconds
[2024-07-24T01:37:38.702+0000] {processor.py:161} INFO - Started process (PID=123) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:37:38.703+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T01:37:38.704+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:37:38.704+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:37:38.733+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T01:37:38.733+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T01:37:38.740+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:37:38.733+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 40, in <module>
    def upload_to_s3(s3: s3fs.S3FileSystem, file_path: str, bucket:str, s3_file_name: str):
NameError: name 's3fs' is not defined
[2024-07-24T01:37:38.741+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:37:38.761+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.063 seconds
[2024-07-24T01:38:08.855+0000] {processor.py:161} INFO - Started process (PID=124) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:38:08.856+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T01:38:08.858+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:38:08.857+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:38:08.890+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T01:38:08.890+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T01:38:08.897+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:38:08.890+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 40, in <module>
    def upload_to_s3(s3: s3fs.S3FileSystem, file_path: str, bucket:str, s3_file_name: str):
NameError: name 's3fs' is not defined
[2024-07-24T01:38:08.899+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:38:08.918+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.069 seconds
[2024-07-24T01:38:39.045+0000] {processor.py:161} INFO - Started process (PID=125) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:38:39.046+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T01:38:39.047+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:38:39.047+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:38:39.077+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T01:38:39.077+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T01:38:39.085+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:38:39.078+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 40, in <module>
    def upload_to_s3(s3: s3fs.S3FileSystem, file_path: str, bucket:str, s3_file_name: str):
NameError: name 's3fs' is not defined
[2024-07-24T01:38:39.086+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:38:39.109+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.069 seconds
[2024-07-24T01:39:09.204+0000] {processor.py:161} INFO - Started process (PID=126) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:39:09.204+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T01:39:09.205+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:39:09.205+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:39:09.245+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T01:39:09.246+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T01:39:09.253+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:39:09.246+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 40, in <module>
    def upload_to_s3(s3: s3fs.S3FileSystem, file_path: str, bucket:str, s3_file_name: str):
NameError: name 's3fs' is not defined
[2024-07-24T01:39:09.254+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:39:09.271+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.074 seconds
[2024-07-24T01:39:39.403+0000] {processor.py:161} INFO - Started process (PID=127) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:39:39.404+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T01:39:39.405+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:39:39.405+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:39:39.438+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T01:39:39.439+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T01:39:39.446+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:39:39.439+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 40, in <module>
    def upload_to_s3(s3: s3fs.S3FileSystem, file_path: str, bucket:str, s3_file_name: str):
NameError: name 's3fs' is not defined
[2024-07-24T01:39:39.448+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:39:39.466+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.070 seconds
[2024-07-24T01:40:09.557+0000] {processor.py:161} INFO - Started process (PID=128) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:40:09.558+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T01:40:09.559+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:40:09.559+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:40:09.588+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T01:40:09.589+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T01:40:09.596+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:40:09.589+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 40, in <module>
    def upload_to_s3(s3: s3fs.S3FileSystem, file_path: str, bucket:str, s3_file_name: str):
NameError: name 's3fs' is not defined
[2024-07-24T01:40:09.598+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:40:09.615+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.068 seconds
[2024-07-24T01:40:39.793+0000] {processor.py:161} INFO - Started process (PID=129) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:40:39.795+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T01:40:39.795+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:40:39.795+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:40:39.868+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T01:40:39.869+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T01:40:39.889+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:40:39.869+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 40, in <module>
    def upload_to_s3(s3: s3fs.S3FileSystem, file_path: str, bucket:str, s3_file_name: str):
NameError: name 's3fs' is not defined
[2024-07-24T01:40:39.891+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:40:39.909+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.123 seconds
[2024-07-24T01:41:10.048+0000] {processor.py:161} INFO - Started process (PID=130) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:41:10.049+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T01:41:10.050+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:41:10.050+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:41:10.080+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T01:41:10.081+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T01:41:10.087+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:41:10.081+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 40, in <module>
    def upload_to_s3(s3: s3fs.S3FileSystem, file_path: str, bucket:str, s3_file_name: str):
NameError: name 's3fs' is not defined
[2024-07-24T01:41:10.089+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:41:10.106+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.063 seconds
[2024-07-24T01:41:40.193+0000] {processor.py:161} INFO - Started process (PID=131) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:41:40.194+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T01:41:40.195+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:41:40.195+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:41:40.227+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T01:41:40.227+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T01:41:40.234+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:41:40.228+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 40, in <module>
    def upload_to_s3(s3: s3fs.S3FileSystem, file_path: str, bucket:str, s3_file_name: str):
NameError: name 's3fs' is not defined
[2024-07-24T01:41:40.236+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:41:40.253+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.065 seconds
[2024-07-24T01:42:10.375+0000] {processor.py:161} INFO - Started process (PID=132) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:42:10.376+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T01:42:10.377+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:42:10.377+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:42:10.407+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T01:42:10.407+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T01:42:10.414+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:42:10.408+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 40, in <module>
    def upload_to_s3(s3: s3fs.S3FileSystem, file_path: str, bucket:str, s3_file_name: str):
NameError: name 's3fs' is not defined
[2024-07-24T01:42:10.416+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:42:10.431+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.061 seconds
[2024-07-24T01:42:40.531+0000] {processor.py:161} INFO - Started process (PID=133) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:42:40.532+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T01:42:40.532+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:42:40.532+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:42:40.566+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T01:42:40.567+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T01:42:40.574+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:42:40.567+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 40, in <module>
    def upload_to_s3(s3: s3fs.S3FileSystem, file_path: str, bucket:str, s3_file_name: str):
NameError: name 's3fs' is not defined
[2024-07-24T01:42:40.576+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:42:40.595+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.072 seconds
[2024-07-24T01:43:10.706+0000] {processor.py:161} INFO - Started process (PID=134) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:43:10.707+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T01:43:10.708+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:43:10.708+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:43:10.738+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T01:43:10.738+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T01:43:10.745+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:43:10.738+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 40, in <module>
    def upload_to_s3(s3: s3fs.S3FileSystem, file_path: str, bucket:str, s3_file_name: str):
NameError: name 's3fs' is not defined
[2024-07-24T01:43:10.746+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:43:10.761+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.060 seconds
[2024-07-24T01:43:40.869+0000] {processor.py:161} INFO - Started process (PID=135) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:43:40.870+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T01:43:40.871+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:43:40.870+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:43:40.904+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T01:43:40.905+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T01:43:40.913+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:43:40.905+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 40, in <module>
    def upload_to_s3(s3: s3fs.S3FileSystem, file_path: str, bucket:str, s3_file_name: str):
NameError: name 's3fs' is not defined
[2024-07-24T01:43:40.915+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:43:40.933+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.069 seconds
[2024-07-24T01:44:11.046+0000] {processor.py:161} INFO - Started process (PID=136) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:44:11.047+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T01:44:11.048+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:44:11.047+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:44:11.085+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T01:44:11.086+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T01:44:11.095+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:44:11.086+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 40, in <module>
    def upload_to_s3(s3: s3fs.S3FileSystem, file_path: str, bucket:str, s3_file_name: str):
NameError: name 's3fs' is not defined
[2024-07-24T01:44:11.097+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:44:11.119+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.080 seconds
[2024-07-24T01:44:41.229+0000] {processor.py:161} INFO - Started process (PID=137) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:44:41.230+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T01:44:41.231+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:44:41.230+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:44:41.264+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T01:44:41.264+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T01:44:41.271+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:44:41.264+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 40, in <module>
    def upload_to_s3(s3: s3fs.S3FileSystem, file_path: str, bucket:str, s3_file_name: str):
NameError: name 's3fs' is not defined
[2024-07-24T01:44:41.273+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:44:41.292+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.072 seconds
[2024-07-24T01:45:11.417+0000] {processor.py:161} INFO - Started process (PID=138) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:45:11.418+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T01:45:11.419+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:45:11.419+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:45:11.451+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T01:45:11.452+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T01:45:11.458+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:45:11.452+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 40, in <module>
    def upload_to_s3(s3: s3fs.S3FileSystem, file_path: str, bucket:str, s3_file_name: str):
NameError: name 's3fs' is not defined
[2024-07-24T01:45:11.460+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:45:11.479+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.068 seconds
[2024-07-24T01:45:41.609+0000] {processor.py:161} INFO - Started process (PID=139) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:45:41.610+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T01:45:41.611+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:45:41.610+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:45:41.661+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T01:45:41.662+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T01:45:41.669+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:45:41.662+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 40, in <module>
    def upload_to_s3(s3: s3fs.S3FileSystem, file_path: str, bucket:str, s3_file_name: str):
NameError: name 's3fs' is not defined
[2024-07-24T01:45:41.671+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:45:41.687+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.085 seconds
[2024-07-24T01:46:11.767+0000] {processor.py:161} INFO - Started process (PID=140) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:46:11.768+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T01:46:11.769+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:46:11.768+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:46:11.801+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T01:46:11.801+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T01:46:11.809+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:46:11.802+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 40, in <module>
    def upload_to_s3(s3: s3fs.S3FileSystem, file_path: str, bucket:str, s3_file_name: str):
NameError: name 's3fs' is not defined
[2024-07-24T01:46:11.811+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:46:11.831+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.070 seconds
[2024-07-24T01:46:42.012+0000] {processor.py:161} INFO - Started process (PID=141) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:46:42.025+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T01:46:42.033+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:46:42.031+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:46:42.107+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T01:46:42.108+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T01:46:42.120+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:46:42.109+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 40, in <module>
    def upload_to_s3(s3: s3fs.S3FileSystem, file_path: str, bucket:str, s3_file_name: str):
NameError: name 's3fs' is not defined
[2024-07-24T01:46:42.122+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:46:42.183+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.192 seconds
[2024-07-24T01:47:12.336+0000] {processor.py:161} INFO - Started process (PID=142) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:47:12.337+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T01:47:12.338+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:47:12.338+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:47:12.379+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T01:47:12.380+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T01:47:12.388+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:47:12.380+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 40, in <module>
    def upload_to_s3(s3: s3fs.S3FileSystem, file_path: str, bucket:str, s3_file_name: str):
NameError: name 's3fs' is not defined
[2024-07-24T01:47:12.390+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:47:12.414+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.095 seconds
[2024-07-24T01:47:42.483+0000] {processor.py:161} INFO - Started process (PID=143) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:47:42.485+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T01:47:42.486+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:47:42.486+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:47:42.569+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T01:47:42.571+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T01:47:42.584+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:47:42.572+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 40, in <module>
    def upload_to_s3(s3: s3fs.S3FileSystem, file_path: str, bucket:str, s3_file_name: str):
NameError: name 's3fs' is not defined
[2024-07-24T01:47:42.586+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:47:42.636+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.161 seconds
[2024-07-24T01:48:12.774+0000] {processor.py:161} INFO - Started process (PID=144) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:48:12.776+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T01:48:12.776+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:48:12.776+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:48:12.808+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T01:48:12.808+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T01:48:12.815+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:48:12.809+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 40, in <module>
    def upload_to_s3(s3: s3fs.S3FileSystem, file_path: str, bucket:str, s3_file_name: str):
NameError: name 's3fs' is not defined
[2024-07-24T01:48:12.818+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:48:12.841+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.087 seconds
[2024-07-24T01:48:42.949+0000] {processor.py:161} INFO - Started process (PID=145) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:48:42.951+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T01:48:42.951+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:48:42.951+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:48:42.998+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T01:48:42.998+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T01:48:43.005+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:48:42.998+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 40, in <module>
    def upload_to_s3(s3: s3fs.S3FileSystem, file_path: str, bucket:str, s3_file_name: str):
NameError: name 's3fs' is not defined
[2024-07-24T01:48:43.006+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:48:43.078+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.138 seconds
[2024-07-24T01:49:13.132+0000] {processor.py:161} INFO - Started process (PID=146) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:49:13.133+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T01:49:13.134+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:49:13.133+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:49:13.168+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T01:49:13.169+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T01:49:13.175+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:49:13.169+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 40, in <module>
    def upload_to_s3(s3: s3fs.S3FileSystem, file_path: str, bucket:str, s3_file_name: str):
NameError: name 's3fs' is not defined
[2024-07-24T01:49:13.177+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:49:13.199+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.073 seconds
[2024-07-24T01:49:43.275+0000] {processor.py:161} INFO - Started process (PID=147) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:49:43.276+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T01:49:43.276+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:49:43.276+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:49:43.306+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T01:49:43.307+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T01:49:43.314+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:49:43.307+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 40, in <module>
    def upload_to_s3(s3: s3fs.S3FileSystem, file_path: str, bucket:str, s3_file_name: str):
NameError: name 's3fs' is not defined
[2024-07-24T01:49:43.315+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:49:43.334+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.065 seconds
[2024-07-24T01:50:13.474+0000] {processor.py:161} INFO - Started process (PID=148) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:50:13.475+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T01:50:13.476+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:50:13.476+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:50:13.506+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T01:50:13.506+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T01:50:13.512+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:50:13.506+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 40, in <module>
    def upload_to_s3(s3: s3fs.S3FileSystem, file_path: str, bucket:str, s3_file_name: str):
NameError: name 's3fs' is not defined
[2024-07-24T01:50:13.514+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:50:13.531+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.062 seconds
[2024-07-24T01:50:43.708+0000] {processor.py:161} INFO - Started process (PID=149) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:50:43.723+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T01:50:43.727+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:50:43.726+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:50:43.794+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T01:50:43.794+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T01:50:43.805+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:50:43.795+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 40, in <module>
    def upload_to_s3(s3: s3fs.S3FileSystem, file_path: str, bucket:str, s3_file_name: str):
NameError: name 's3fs' is not defined
[2024-07-24T01:50:43.806+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:50:43.863+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.195 seconds
[2024-07-24T01:51:14.012+0000] {processor.py:161} INFO - Started process (PID=150) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:51:14.013+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T01:51:14.014+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:51:14.014+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:51:14.055+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T01:51:14.056+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T01:51:14.067+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:51:14.056+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 40, in <module>
    def upload_to_s3(s3: s3fs.S3FileSystem, file_path: str, bucket:str, s3_file_name: str):
NameError: name 's3fs' is not defined
[2024-07-24T01:51:14.069+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:51:14.104+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.099 seconds
[2024-07-24T01:51:44.163+0000] {processor.py:161} INFO - Started process (PID=151) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:51:44.164+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T01:51:44.165+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:51:44.165+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:51:44.198+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T01:51:44.199+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T01:51:44.205+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:51:44.199+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 40, in <module>
    def upload_to_s3(s3: s3fs.S3FileSystem, file_path: str, bucket:str, s3_file_name: str):
NameError: name 's3fs' is not defined
[2024-07-24T01:51:44.207+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:51:44.241+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.087 seconds
[2024-07-24T01:52:14.393+0000] {processor.py:161} INFO - Started process (PID=152) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:52:14.394+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T01:52:14.395+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:52:14.394+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:52:14.427+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T01:52:14.428+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T01:52:14.435+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:52:14.428+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 40, in <module>
    def upload_to_s3(s3: s3fs.S3FileSystem, file_path: str, bucket:str, s3_file_name: str):
NameError: name 's3fs' is not defined
[2024-07-24T01:52:14.437+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:52:14.456+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.074 seconds
[2024-07-24T01:52:44.540+0000] {processor.py:161} INFO - Started process (PID=153) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:52:44.541+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T01:52:44.542+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:52:44.541+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:52:44.571+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T01:52:44.572+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T01:52:44.578+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:52:44.572+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 40, in <module>
    def upload_to_s3(s3: s3fs.S3FileSystem, file_path: str, bucket:str, s3_file_name: str):
NameError: name 's3fs' is not defined
[2024-07-24T01:52:44.580+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:52:44.595+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.061 seconds
[2024-07-24T01:53:14.728+0000] {processor.py:161} INFO - Started process (PID=154) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:53:14.729+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T01:53:14.730+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:53:14.730+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:53:14.762+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T01:53:14.763+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T01:53:14.769+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:53:14.763+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 40, in <module>
    def upload_to_s3(s3: s3fs.S3FileSystem, file_path: str, bucket:str, s3_file_name: str):
NameError: name 's3fs' is not defined
[2024-07-24T01:53:14.771+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:53:14.788+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.065 seconds
[2024-07-24T01:53:44.891+0000] {processor.py:161} INFO - Started process (PID=155) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:53:44.892+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T01:53:44.893+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:53:44.893+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:53:44.930+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T01:53:44.930+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T01:53:44.939+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:53:44.931+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 40, in <module>
    def upload_to_s3(s3: s3fs.S3FileSystem, file_path: str, bucket:str, s3_file_name: str):
NameError: name 's3fs' is not defined
[2024-07-24T01:53:44.941+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:53:44.959+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.075 seconds
[2024-07-24T01:54:15.118+0000] {processor.py:161} INFO - Started process (PID=156) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:54:15.121+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T01:54:15.123+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:54:15.122+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:54:15.160+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T01:54:15.160+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T01:54:15.168+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:54:15.161+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 40, in <module>
    def upload_to_s3(s3: s3fs.S3FileSystem, file_path: str, bucket:str, s3_file_name: str):
NameError: name 's3fs' is not defined
[2024-07-24T01:54:15.169+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:54:15.186+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.086 seconds
[2024-07-24T01:54:45.272+0000] {processor.py:161} INFO - Started process (PID=157) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:54:45.272+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T01:54:45.273+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:54:45.273+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:54:45.300+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T01:54:45.301+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T01:54:45.308+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:54:45.301+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 40, in <module>
    def upload_to_s3(s3: s3fs.S3FileSystem, file_path: str, bucket:str, s3_file_name: str):
NameError: name 's3fs' is not defined
[2024-07-24T01:54:45.309+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:54:45.328+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.061 seconds
[2024-07-24T01:55:15.473+0000] {processor.py:161} INFO - Started process (PID=158) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:55:15.474+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T01:55:15.475+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:55:15.474+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:55:15.504+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T01:55:15.504+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T01:55:15.512+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:55:15.505+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 40, in <module>
    def upload_to_s3(s3: s3fs.S3FileSystem, file_path: str, bucket:str, s3_file_name: str):
NameError: name 's3fs' is not defined
[2024-07-24T01:55:15.513+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:55:15.535+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.067 seconds
[2024-07-24T01:55:45.679+0000] {processor.py:161} INFO - Started process (PID=159) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:55:45.680+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T01:55:45.681+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:55:45.681+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:55:45.713+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T01:55:45.714+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T01:55:45.721+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:55:45.714+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 40, in <module>
    def upload_to_s3(s3: s3fs.S3FileSystem, file_path: str, bucket:str, s3_file_name: str):
NameError: name 's3fs' is not defined
[2024-07-24T01:55:45.722+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:55:45.740+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.071 seconds
[2024-07-24T01:56:15.840+0000] {processor.py:161} INFO - Started process (PID=160) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:56:15.841+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T01:56:15.842+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:56:15.841+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:56:15.874+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T01:56:15.875+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T01:56:15.882+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:56:15.875+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 40, in <module>
    def upload_to_s3(s3: s3fs.S3FileSystem, file_path: str, bucket:str, s3_file_name: str):
NameError: name 's3fs' is not defined
[2024-07-24T01:56:15.883+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:56:15.898+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.064 seconds
[2024-07-24T01:56:46.013+0000] {processor.py:161} INFO - Started process (PID=161) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:56:46.013+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T01:56:46.014+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:56:46.014+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:56:46.044+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T01:56:46.044+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T01:56:46.051+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:56:46.045+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 40, in <module>
    def upload_to_s3(s3: s3fs.S3FileSystem, file_path: str, bucket:str, s3_file_name: str):
NameError: name 's3fs' is not defined
[2024-07-24T01:56:46.053+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:56:46.071+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.063 seconds
[2024-07-24T01:57:16.170+0000] {processor.py:161} INFO - Started process (PID=162) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:57:16.171+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T01:57:16.172+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:57:16.172+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:57:16.201+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T01:57:16.202+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T01:57:16.209+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:57:16.202+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 40, in <module>
    def upload_to_s3(s3: s3fs.S3FileSystem, file_path: str, bucket:str, s3_file_name: str):
NameError: name 's3fs' is not defined
[2024-07-24T01:57:16.210+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:57:16.226+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.061 seconds
[2024-07-24T01:57:46.349+0000] {processor.py:161} INFO - Started process (PID=163) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:57:46.350+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T01:57:46.351+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:57:46.351+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:57:46.380+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T01:57:46.381+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T01:57:46.387+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:57:46.381+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 40, in <module>
    def upload_to_s3(s3: s3fs.S3FileSystem, file_path: str, bucket:str, s3_file_name: str):
NameError: name 's3fs' is not defined
[2024-07-24T01:57:46.389+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:57:46.404+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.060 seconds
[2024-07-24T01:58:16.498+0000] {processor.py:161} INFO - Started process (PID=164) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:58:16.499+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T01:58:16.500+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:58:16.499+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:58:16.530+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T01:58:16.530+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T01:58:16.537+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:58:16.531+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 40, in <module>
    def upload_to_s3(s3: s3fs.S3FileSystem, file_path: str, bucket:str, s3_file_name: str):
NameError: name 's3fs' is not defined
[2024-07-24T01:58:16.538+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:58:16.555+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.062 seconds
[2024-07-24T01:58:46.669+0000] {processor.py:161} INFO - Started process (PID=165) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:58:46.669+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T01:58:46.670+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:58:46.670+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:58:46.699+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T01:58:46.700+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T01:58:46.706+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:58:46.700+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 40, in <module>
    def upload_to_s3(s3: s3fs.S3FileSystem, file_path: str, bucket:str, s3_file_name: str):
NameError: name 's3fs' is not defined
[2024-07-24T01:58:46.708+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:58:46.722+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.059 seconds
[2024-07-24T01:59:16.821+0000] {processor.py:161} INFO - Started process (PID=166) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:59:16.822+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T01:59:16.823+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:59:16.823+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:59:16.852+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T01:59:16.853+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T01:59:16.860+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:59:16.853+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 40, in <module>
    def upload_to_s3(s3: s3fs.S3FileSystem, file_path: str, bucket:str, s3_file_name: str):
NameError: name 's3fs' is not defined
[2024-07-24T01:59:16.862+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:59:16.878+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.063 seconds
[2024-07-24T01:59:46.993+0000] {processor.py:161} INFO - Started process (PID=167) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:59:46.994+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T01:59:46.995+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:59:46.994+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:59:47.025+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T01:59:47.025+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T01:59:47.032+0000] {logging_mixin.py:188} INFO - [2024-07-24T01:59:47.026+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 40, in <module>
    def upload_to_s3(s3: s3fs.S3FileSystem, file_path: str, bucket:str, s3_file_name: str):
NameError: name 's3fs' is not defined
[2024-07-24T01:59:47.034+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T01:59:47.049+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.063 seconds
[2024-07-24T02:00:17.154+0000] {processor.py:161} INFO - Started process (PID=168) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:00:17.155+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T02:00:17.155+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:00:17.155+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:00:17.184+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T02:00:17.184+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T02:00:17.191+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:00:17.185+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 40, in <module>
    def upload_to_s3(s3: s3fs.S3FileSystem, file_path: str, bucket:str, s3_file_name: str):
NameError: name 's3fs' is not defined
[2024-07-24T02:00:17.193+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:00:17.207+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.058 seconds
[2024-07-24T02:00:47.341+0000] {processor.py:161} INFO - Started process (PID=169) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:00:47.342+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T02:00:47.342+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:00:47.342+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:00:47.371+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T02:00:47.371+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T02:00:47.377+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:00:47.371+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 40, in <module>
    def upload_to_s3(s3: s3fs.S3FileSystem, file_path: str, bucket:str, s3_file_name: str):
NameError: name 's3fs' is not defined
[2024-07-24T02:00:47.379+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:00:47.393+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.057 seconds
[2024-07-24T02:01:17.482+0000] {processor.py:161} INFO - Started process (PID=170) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:01:17.482+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T02:01:17.483+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:01:17.483+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:01:17.511+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T02:01:17.512+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T02:01:17.518+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:01:17.512+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 40, in <module>
    def upload_to_s3(s3: s3fs.S3FileSystem, file_path: str, bucket:str, s3_file_name: str):
NameError: name 's3fs' is not defined
[2024-07-24T02:01:17.519+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:01:17.534+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.057 seconds
[2024-07-24T02:01:47.665+0000] {processor.py:161} INFO - Started process (PID=171) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:01:47.666+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T02:01:47.667+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:01:47.667+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:01:47.697+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T02:01:47.697+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T02:01:47.703+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:01:47.697+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 40, in <module>
    def upload_to_s3(s3: s3fs.S3FileSystem, file_path: str, bucket:str, s3_file_name: str):
NameError: name 's3fs' is not defined
[2024-07-24T02:01:47.705+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:01:47.720+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.060 seconds
[2024-07-24T02:02:17.813+0000] {processor.py:161} INFO - Started process (PID=172) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:02:17.814+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T02:02:17.815+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:02:17.814+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:02:17.845+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T02:02:17.845+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T02:02:17.853+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:02:17.846+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 40, in <module>
    def upload_to_s3(s3: s3fs.S3FileSystem, file_path: str, bucket:str, s3_file_name: str):
NameError: name 's3fs' is not defined
[2024-07-24T02:02:17.854+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:02:17.869+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.061 seconds
[2024-07-24T02:02:48.001+0000] {processor.py:161} INFO - Started process (PID=173) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:02:48.002+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T02:02:48.003+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:02:48.003+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:02:48.033+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T02:02:48.033+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T02:02:48.041+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:02:48.034+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 40, in <module>
    def upload_to_s3(s3: s3fs.S3FileSystem, file_path: str, bucket:str, s3_file_name: str):
NameError: name 's3fs' is not defined
[2024-07-24T02:02:48.043+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:02:48.059+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.062 seconds
[2024-07-24T02:03:18.153+0000] {processor.py:161} INFO - Started process (PID=174) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:03:18.154+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T02:03:18.154+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:03:18.154+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:03:18.184+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T02:03:18.184+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T02:03:18.191+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:03:18.185+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 40, in <module>
    def upload_to_s3(s3: s3fs.S3FileSystem, file_path: str, bucket:str, s3_file_name: str):
NameError: name 's3fs' is not defined
[2024-07-24T02:03:18.192+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:03:18.207+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.060 seconds
[2024-07-24T02:03:48.342+0000] {processor.py:161} INFO - Started process (PID=175) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:03:48.343+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T02:03:48.344+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:03:48.344+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:03:48.375+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T02:03:48.376+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T02:03:48.382+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:03:48.376+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 40, in <module>
    def upload_to_s3(s3: s3fs.S3FileSystem, file_path: str, bucket:str, s3_file_name: str):
NameError: name 's3fs' is not defined
[2024-07-24T02:03:48.384+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:03:48.399+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.062 seconds
[2024-07-24T02:04:18.498+0000] {processor.py:161} INFO - Started process (PID=176) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:04:18.499+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T02:04:18.500+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:04:18.499+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:04:18.531+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T02:04:18.532+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T02:04:18.539+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:04:18.532+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 40, in <module>
    def upload_to_s3(s3: s3fs.S3FileSystem, file_path: str, bucket:str, s3_file_name: str):
NameError: name 's3fs' is not defined
[2024-07-24T02:04:18.541+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:04:18.558+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.066 seconds
[2024-07-24T02:04:48.676+0000] {processor.py:161} INFO - Started process (PID=177) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:04:48.677+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T02:04:48.677+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:04:48.677+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:04:48.706+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T02:04:48.706+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T02:04:48.713+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:04:48.707+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 40, in <module>
    def upload_to_s3(s3: s3fs.S3FileSystem, file_path: str, bucket:str, s3_file_name: str):
NameError: name 's3fs' is not defined
[2024-07-24T02:04:48.714+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:04:48.730+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.059 seconds
[2024-07-24T02:05:18.837+0000] {processor.py:161} INFO - Started process (PID=178) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:05:18.838+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T02:05:18.838+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:05:18.838+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:05:18.870+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T02:05:18.871+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T02:05:18.878+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:05:18.871+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 40, in <module>
    def upload_to_s3(s3: s3fs.S3FileSystem, file_path: str, bucket:str, s3_file_name: str):
NameError: name 's3fs' is not defined
[2024-07-24T02:05:18.880+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:05:18.898+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.066 seconds
[2024-07-24T02:05:49.035+0000] {processor.py:161} INFO - Started process (PID=179) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:05:49.035+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T02:05:49.036+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:05:49.036+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:05:49.065+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T02:05:49.066+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T02:05:49.072+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:05:49.066+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 40, in <module>
    def upload_to_s3(s3: s3fs.S3FileSystem, file_path: str, bucket:str, s3_file_name: str):
NameError: name 's3fs' is not defined
[2024-07-24T02:05:49.074+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:05:49.089+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.059 seconds
[2024-07-24T02:06:19.196+0000] {processor.py:161} INFO - Started process (PID=180) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:06:19.197+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T02:06:19.199+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:06:19.198+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:06:19.236+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T02:06:19.236+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T02:06:19.243+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:06:19.237+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 40, in <module>
    def upload_to_s3(s3: s3fs.S3FileSystem, file_path: str, bucket:str, s3_file_name: str):
NameError: name 's3fs' is not defined
[2024-07-24T02:06:19.245+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:06:19.260+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.071 seconds
[2024-07-24T02:06:49.377+0000] {processor.py:161} INFO - Started process (PID=181) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:06:49.378+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T02:06:49.379+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:06:49.379+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:06:49.408+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T02:06:49.409+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T02:06:49.415+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:06:49.409+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 40, in <module>
    def upload_to_s3(s3: s3fs.S3FileSystem, file_path: str, bucket:str, s3_file_name: str):
NameError: name 's3fs' is not defined
[2024-07-24T02:06:49.417+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:06:49.432+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.060 seconds
[2024-07-24T02:07:19.535+0000] {processor.py:161} INFO - Started process (PID=182) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:07:19.536+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T02:07:19.537+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:07:19.537+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:07:19.566+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T02:07:19.566+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T02:07:19.572+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:07:19.566+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 40, in <module>
    def upload_to_s3(s3: s3fs.S3FileSystem, file_path: str, bucket:str, s3_file_name: str):
NameError: name 's3fs' is not defined
[2024-07-24T02:07:19.574+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:07:19.588+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.058 seconds
[2024-07-24T02:07:49.700+0000] {processor.py:161} INFO - Started process (PID=183) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:07:49.701+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T02:07:49.702+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:07:49.701+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:07:49.732+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T02:07:49.733+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T02:07:49.739+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:07:49.733+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 40, in <module>
    def upload_to_s3(s3: s3fs.S3FileSystem, file_path: str, bucket:str, s3_file_name: str):
NameError: name 's3fs' is not defined
[2024-07-24T02:07:49.741+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:07:49.763+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.068 seconds
[2024-07-24T02:08:19.862+0000] {processor.py:161} INFO - Started process (PID=184) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:08:19.864+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T02:08:19.864+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:08:19.864+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:08:19.894+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T02:08:19.895+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T02:08:19.902+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:08:19.895+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 40, in <module>
    def upload_to_s3(s3: s3fs.S3FileSystem, file_path: str, bucket:str, s3_file_name: str):
NameError: name 's3fs' is not defined
[2024-07-24T02:08:19.904+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:08:19.920+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.064 seconds
[2024-07-24T02:08:50.039+0000] {processor.py:161} INFO - Started process (PID=185) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:08:50.040+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T02:08:50.040+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:08:50.040+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:08:50.070+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T02:08:50.070+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T02:08:50.077+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:08:50.070+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 40, in <module>
    def upload_to_s3(s3: s3fs.S3FileSystem, file_path: str, bucket:str, s3_file_name: str):
NameError: name 's3fs' is not defined
[2024-07-24T02:08:50.079+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:08:50.096+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.062 seconds
[2024-07-24T02:09:20.194+0000] {processor.py:161} INFO - Started process (PID=186) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:09:20.195+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T02:09:20.196+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:09:20.196+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:09:20.226+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T02:09:20.227+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T02:09:20.233+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:09:20.227+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 40, in <module>
    def upload_to_s3(s3: s3fs.S3FileSystem, file_path: str, bucket:str, s3_file_name: str):
NameError: name 's3fs' is not defined
[2024-07-24T02:09:20.235+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:09:20.251+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.061 seconds
[2024-07-24T02:09:50.369+0000] {processor.py:161} INFO - Started process (PID=187) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:09:50.370+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T02:09:50.371+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:09:50.370+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:09:50.401+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T02:09:50.401+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T02:09:50.408+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:09:50.402+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 40, in <module>
    def upload_to_s3(s3: s3fs.S3FileSystem, file_path: str, bucket:str, s3_file_name: str):
NameError: name 's3fs' is not defined
[2024-07-24T02:09:50.410+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:09:50.426+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.062 seconds
[2024-07-24T02:10:20.526+0000] {processor.py:161} INFO - Started process (PID=188) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:10:20.527+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T02:10:20.528+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:10:20.528+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:10:20.568+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T02:10:20.569+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T02:10:20.577+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:10:20.569+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 40, in <module>
    def upload_to_s3(s3: s3fs.S3FileSystem, file_path: str, bucket:str, s3_file_name: str):
NameError: name 's3fs' is not defined
[2024-07-24T02:10:20.579+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:10:20.600+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.081 seconds
[2024-07-24T02:10:50.719+0000] {processor.py:161} INFO - Started process (PID=189) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:10:50.720+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T02:10:50.721+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:10:50.721+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:10:50.751+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T02:10:50.752+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T02:10:50.759+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:10:50.752+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 40, in <module>
    def upload_to_s3(s3: s3fs.S3FileSystem, file_path: str, bucket:str, s3_file_name: str):
NameError: name 's3fs' is not defined
[2024-07-24T02:10:50.760+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:10:50.775+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.061 seconds
[2024-07-24T02:11:20.875+0000] {processor.py:161} INFO - Started process (PID=190) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:11:20.876+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T02:11:20.877+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:11:20.877+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:11:20.909+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T02:11:20.910+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T02:11:20.917+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:11:20.910+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 40, in <module>
    def upload_to_s3(s3: s3fs.S3FileSystem, file_path: str, bucket:str, s3_file_name: str):
NameError: name 's3fs' is not defined
[2024-07-24T02:11:20.919+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:11:20.936+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.069 seconds
[2024-07-24T02:11:51.042+0000] {processor.py:161} INFO - Started process (PID=191) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:11:51.042+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T02:11:51.043+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:11:51.043+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:11:51.075+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T02:11:51.075+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T02:11:51.083+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:11:51.076+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 40, in <module>
    def upload_to_s3(s3: s3fs.S3FileSystem, file_path: str, bucket:str, s3_file_name: str):
NameError: name 's3fs' is not defined
[2024-07-24T02:11:51.085+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:11:51.101+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.065 seconds
[2024-07-24T02:12:21.208+0000] {processor.py:161} INFO - Started process (PID=192) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:12:21.209+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T02:12:21.210+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:12:21.210+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:12:21.240+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T02:12:21.240+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T02:12:21.247+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:12:21.240+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 40, in <module>
    def upload_to_s3(s3: s3fs.S3FileSystem, file_path: str, bucket:str, s3_file_name: str):
NameError: name 's3fs' is not defined
[2024-07-24T02:12:21.249+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:12:21.267+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.064 seconds
[2024-07-24T02:12:51.363+0000] {processor.py:161} INFO - Started process (PID=193) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:12:51.364+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T02:12:51.365+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:12:51.365+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:12:51.394+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T02:12:51.395+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T02:12:51.402+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:12:51.395+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 40, in <module>
    def upload_to_s3(s3: s3fs.S3FileSystem, file_path: str, bucket:str, s3_file_name: str):
NameError: name 's3fs' is not defined
[2024-07-24T02:12:51.404+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:12:51.424+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.066 seconds
[2024-07-24T02:13:21.538+0000] {processor.py:161} INFO - Started process (PID=194) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:13:21.539+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T02:13:21.540+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:13:21.540+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:13:21.571+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T02:13:21.571+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T02:13:21.578+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:13:21.571+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 40, in <module>
    def upload_to_s3(s3: s3fs.S3FileSystem, file_path: str, bucket:str, s3_file_name: str):
NameError: name 's3fs' is not defined
[2024-07-24T02:13:21.579+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:13:21.594+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.061 seconds
[2024-07-24T02:13:51.706+0000] {processor.py:161} INFO - Started process (PID=195) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:13:51.707+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T02:13:51.708+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:13:51.708+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:13:51.739+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T02:13:51.739+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T02:13:51.746+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:13:51.739+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 40, in <module>
    def upload_to_s3(s3: s3fs.S3FileSystem, file_path: str, bucket:str, s3_file_name: str):
NameError: name 's3fs' is not defined
[2024-07-24T02:13:51.748+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:13:51.764+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.065 seconds
[2024-07-24T02:14:21.873+0000] {processor.py:161} INFO - Started process (PID=196) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:14:21.874+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T02:14:21.875+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:14:21.875+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:14:21.908+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T02:14:21.908+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T02:14:21.916+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:14:21.909+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 40, in <module>
    def upload_to_s3(s3: s3fs.S3FileSystem, file_path: str, bucket:str, s3_file_name: str):
NameError: name 's3fs' is not defined
[2024-07-24T02:14:21.917+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:14:21.933+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.066 seconds
[2024-07-24T02:14:52.050+0000] {processor.py:161} INFO - Started process (PID=197) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:14:52.051+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T02:14:52.053+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:14:52.052+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:14:52.084+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T02:14:52.084+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T02:14:52.091+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:14:52.085+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 40, in <module>
    def upload_to_s3(s3: s3fs.S3FileSystem, file_path: str, bucket:str, s3_file_name: str):
NameError: name 's3fs' is not defined
[2024-07-24T02:14:52.093+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:14:52.109+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.064 seconds
[2024-07-24T02:15:22.219+0000] {processor.py:161} INFO - Started process (PID=198) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:15:22.220+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T02:15:22.221+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:15:22.220+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:15:22.251+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T02:15:22.252+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T02:15:22.260+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:15:22.252+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 40, in <module>
    def upload_to_s3(s3: s3fs.S3FileSystem, file_path: str, bucket:str, s3_file_name: str):
NameError: name 's3fs' is not defined
[2024-07-24T02:15:22.261+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:15:22.277+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.063 seconds
[2024-07-24T02:15:52.418+0000] {processor.py:161} INFO - Started process (PID=199) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:15:52.419+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T02:15:52.420+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:15:52.420+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:15:52.456+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T02:15:52.457+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T02:15:52.466+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:15:52.458+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 40, in <module>
    def upload_to_s3(s3: s3fs.S3FileSystem, file_path: str, bucket:str, s3_file_name: str):
NameError: name 's3fs' is not defined
[2024-07-24T02:15:52.468+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:15:52.495+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.084 seconds
[2024-07-24T02:16:22.579+0000] {processor.py:161} INFO - Started process (PID=200) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:16:22.581+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T02:16:22.582+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:16:22.581+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:16:22.615+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T02:16:22.615+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T02:16:22.622+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:16:22.615+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 40, in <module>
    def upload_to_s3(s3: s3fs.S3FileSystem, file_path: str, bucket:str, s3_file_name: str):
NameError: name 's3fs' is not defined
[2024-07-24T02:16:22.624+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:16:22.641+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.068 seconds
[2024-07-24T02:16:52.787+0000] {processor.py:161} INFO - Started process (PID=201) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:16:52.788+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T02:16:52.789+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:16:52.789+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:16:52.820+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T02:16:52.821+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T02:16:52.828+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:16:52.821+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 40, in <module>
    def upload_to_s3(s3: s3fs.S3FileSystem, file_path: str, bucket:str, s3_file_name: str):
NameError: name 's3fs' is not defined
[2024-07-24T02:16:52.829+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:16:52.846+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.063 seconds
[2024-07-24T02:17:22.947+0000] {processor.py:161} INFO - Started process (PID=202) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:17:22.948+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T02:17:22.949+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:17:22.949+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:17:22.983+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T02:17:22.983+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T02:17:22.991+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:17:22.984+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 40, in <module>
    def upload_to_s3(s3: s3fs.S3FileSystem, file_path: str, bucket:str, s3_file_name: str):
NameError: name 's3fs' is not defined
[2024-07-24T02:17:22.992+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:17:23.010+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.070 seconds
[2024-07-24T02:17:53.162+0000] {processor.py:161} INFO - Started process (PID=203) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:17:53.164+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T02:17:53.165+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:17:53.164+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:17:53.197+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T02:17:53.198+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T02:17:53.205+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:17:53.198+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 40, in <module>
    def upload_to_s3(s3: s3fs.S3FileSystem, file_path: str, bucket:str, s3_file_name: str):
NameError: name 's3fs' is not defined
[2024-07-24T02:17:53.207+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:17:53.223+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.067 seconds
[2024-07-24T02:18:23.321+0000] {processor.py:161} INFO - Started process (PID=204) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:18:23.322+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T02:18:23.323+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:18:23.322+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:18:23.357+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T02:18:23.357+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T02:18:23.365+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:18:23.358+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 40, in <module>
    def upload_to_s3(s3: s3fs.S3FileSystem, file_path: str, bucket:str, s3_file_name: str):
NameError: name 's3fs' is not defined
[2024-07-24T02:18:23.367+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:18:23.384+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.069 seconds
[2024-07-24T02:18:53.521+0000] {processor.py:161} INFO - Started process (PID=205) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:18:53.522+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T02:18:53.523+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:18:53.522+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:18:53.555+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T02:18:53.555+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T02:18:53.562+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:18:53.556+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 40, in <module>
    def upload_to_s3(s3: s3fs.S3FileSystem, file_path: str, bucket:str, s3_file_name: str):
NameError: name 's3fs' is not defined
[2024-07-24T02:18:53.564+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:18:53.583+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.069 seconds
[2024-07-24T02:19:23.673+0000] {processor.py:161} INFO - Started process (PID=206) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:19:23.674+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T02:19:23.675+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:19:23.675+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:19:23.704+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T02:19:23.705+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T02:19:23.712+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:19:23.705+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 40, in <module>
    def upload_to_s3(s3: s3fs.S3FileSystem, file_path: str, bucket:str, s3_file_name: str):
NameError: name 's3fs' is not defined
[2024-07-24T02:19:23.713+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:19:23.729+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.061 seconds
[2024-07-24T02:19:53.873+0000] {processor.py:161} INFO - Started process (PID=207) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:19:53.874+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T02:19:53.874+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:19:53.874+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:19:53.903+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T02:19:53.904+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T02:19:53.912+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:19:53.904+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 40, in <module>
    def upload_to_s3(s3: s3fs.S3FileSystem, file_path: str, bucket:str, s3_file_name: str):
NameError: name 's3fs' is not defined
[2024-07-24T02:19:53.914+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:19:53.932+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.063 seconds
[2024-07-24T02:20:24.047+0000] {processor.py:161} INFO - Started process (PID=208) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:20:24.048+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T02:20:24.049+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:20:24.048+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:20:24.078+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T02:20:24.078+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T02:20:24.085+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:20:24.079+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 40, in <module>
    def upload_to_s3(s3: s3fs.S3FileSystem, file_path: str, bucket:str, s3_file_name: str):
NameError: name 's3fs' is not defined
[2024-07-24T02:20:24.087+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:20:24.102+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.061 seconds
[2024-07-24T02:20:54.223+0000] {processor.py:161} INFO - Started process (PID=209) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:20:54.224+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T02:20:54.225+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:20:54.225+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:20:54.259+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T02:20:54.260+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T02:20:54.268+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:20:54.261+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 40, in <module>
    def upload_to_s3(s3: s3fs.S3FileSystem, file_path: str, bucket:str, s3_file_name: str):
NameError: name 's3fs' is not defined
[2024-07-24T02:20:54.270+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:20:54.286+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.067 seconds
[2024-07-24T02:21:24.394+0000] {processor.py:161} INFO - Started process (PID=210) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:21:24.395+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T02:21:24.395+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:21:24.395+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:21:24.425+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T02:21:24.425+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T02:21:24.432+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:21:24.426+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 40, in <module>
    def upload_to_s3(s3: s3fs.S3FileSystem, file_path: str, bucket:str, s3_file_name: str):
NameError: name 's3fs' is not defined
[2024-07-24T02:21:24.434+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:21:24.450+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.063 seconds
[2024-07-24T02:21:54.585+0000] {processor.py:161} INFO - Started process (PID=211) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:21:54.586+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T02:21:54.587+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:21:54.587+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:21:54.618+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T02:21:54.618+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T02:21:54.625+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:21:54.619+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 40, in <module>
    def upload_to_s3(s3: s3fs.S3FileSystem, file_path: str, bucket:str, s3_file_name: str):
NameError: name 's3fs' is not defined
[2024-07-24T02:21:54.626+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:21:54.644+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.066 seconds
[2024-07-24T02:22:24.750+0000] {processor.py:161} INFO - Started process (PID=212) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:22:24.751+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T02:22:24.753+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:22:24.752+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:22:24.781+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T02:22:24.782+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T02:22:24.789+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:22:24.782+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 40, in <module>
    def upload_to_s3(s3: s3fs.S3FileSystem, file_path: str, bucket:str, s3_file_name: str):
NameError: name 's3fs' is not defined
[2024-07-24T02:22:24.791+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:22:24.817+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.073 seconds
[2024-07-24T02:22:54.936+0000] {processor.py:161} INFO - Started process (PID=213) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:22:54.937+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T02:22:54.938+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:22:54.938+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:22:54.967+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T02:22:54.968+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T02:22:54.974+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:22:54.968+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 40, in <module>
    def upload_to_s3(s3: s3fs.S3FileSystem, file_path: str, bucket:str, s3_file_name: str):
NameError: name 's3fs' is not defined
[2024-07-24T02:22:54.976+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:22:54.996+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.066 seconds
[2024-07-24T02:23:25.107+0000] {processor.py:161} INFO - Started process (PID=214) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:23:25.108+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T02:23:25.109+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:23:25.109+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:23:25.137+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T02:23:25.138+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T02:23:25.144+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:23:25.138+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 40, in <module>
    def upload_to_s3(s3: s3fs.S3FileSystem, file_path: str, bucket:str, s3_file_name: str):
NameError: name 's3fs' is not defined
[2024-07-24T02:23:25.145+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:23:25.161+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.060 seconds
[2024-07-24T02:23:55.290+0000] {processor.py:161} INFO - Started process (PID=215) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:23:55.291+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T02:23:55.293+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:23:55.292+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:23:55.323+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T02:23:55.323+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T02:23:55.330+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:23:55.324+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 40, in <module>
    def upload_to_s3(s3: s3fs.S3FileSystem, file_path: str, bucket:str, s3_file_name: str):
NameError: name 's3fs' is not defined
[2024-07-24T02:23:55.332+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:23:55.353+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.069 seconds
[2024-07-24T02:24:25.442+0000] {processor.py:161} INFO - Started process (PID=216) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:24:25.443+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T02:24:25.445+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:24:25.444+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:24:25.473+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T02:24:25.473+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T02:24:25.480+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:24:25.474+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 40, in <module>
    def upload_to_s3(s3: s3fs.S3FileSystem, file_path: str, bucket:str, s3_file_name: str):
NameError: name 's3fs' is not defined
[2024-07-24T02:24:25.482+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:24:25.508+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.070 seconds
[2024-07-24T02:24:55.652+0000] {processor.py:161} INFO - Started process (PID=217) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:24:55.655+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T02:24:55.657+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:24:55.656+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:24:55.690+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T02:24:55.690+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T02:24:55.697+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:24:55.691+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 40, in <module>
    def upload_to_s3(s3: s3fs.S3FileSystem, file_path: str, bucket:str, s3_file_name: str):
NameError: name 's3fs' is not defined
[2024-07-24T02:24:55.698+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:24:55.715+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.072 seconds
[2024-07-24T02:25:25.848+0000] {processor.py:161} INFO - Started process (PID=218) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:25:25.852+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T02:25:25.854+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:25:25.853+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:25:25.890+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T02:25:25.890+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T02:25:25.900+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:25:25.890+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 40, in <module>
    def upload_to_s3(s3: s3fs.S3FileSystem, file_path: str, bucket:str, s3_file_name: str):
NameError: name 's3fs' is not defined
[2024-07-24T02:25:25.902+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:25:25.938+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.109 seconds
[2024-07-24T02:25:56.028+0000] {processor.py:161} INFO - Started process (PID=219) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:25:56.030+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T02:25:56.032+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:25:56.031+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:25:56.071+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T02:25:56.071+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T02:25:56.079+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:25:56.072+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 40, in <module>
    def upload_to_s3(s3: s3fs.S3FileSystem, file_path: str, bucket:str, s3_file_name: str):
NameError: name 's3fs' is not defined
[2024-07-24T02:25:56.081+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:25:56.106+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.088 seconds
[2024-07-24T02:26:26.236+0000] {processor.py:161} INFO - Started process (PID=220) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:26:26.237+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T02:26:26.238+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:26:26.237+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:26:26.274+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T02:26:26.275+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T02:26:26.283+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:26:26.276+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 40, in <module>
    def upload_to_s3(s3: s3fs.S3FileSystem, file_path: str, bucket:str, s3_file_name: str):
NameError: name 's3fs' is not defined
[2024-07-24T02:26:26.285+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:26:26.306+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.079 seconds
[2024-07-24T02:26:56.403+0000] {processor.py:161} INFO - Started process (PID=221) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:26:56.405+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T02:26:56.406+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:26:56.406+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:26:56.442+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T02:26:56.442+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T02:26:56.450+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:26:56.443+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 40, in <module>
    def upload_to_s3(s3: s3fs.S3FileSystem, file_path: str, bucket:str, s3_file_name: str):
NameError: name 's3fs' is not defined
[2024-07-24T02:26:56.452+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:26:56.484+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.088 seconds
[2024-07-24T02:27:26.625+0000] {processor.py:161} INFO - Started process (PID=222) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:27:26.626+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T02:27:26.627+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:27:26.627+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:27:26.662+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T02:27:26.663+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T02:27:26.673+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:27:26.664+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 40, in <module>
    def upload_to_s3(s3: s3fs.S3FileSystem, file_path: str, bucket:str, s3_file_name: str):
NameError: name 's3fs' is not defined
[2024-07-24T02:27:26.675+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:27:26.694+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.082 seconds
[2024-07-24T02:27:56.794+0000] {processor.py:161} INFO - Started process (PID=223) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:27:56.796+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T02:27:56.797+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:27:56.797+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:27:56.837+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T02:27:56.838+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T02:27:56.846+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:27:56.838+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 40, in <module>
    def upload_to_s3(s3: s3fs.S3FileSystem, file_path: str, bucket:str, s3_file_name: str):
NameError: name 's3fs' is not defined
[2024-07-24T02:27:56.848+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:27:56.872+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.088 seconds
[2024-07-24T02:28:27.005+0000] {processor.py:161} INFO - Started process (PID=224) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:28:27.006+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T02:28:27.007+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:28:27.007+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:28:27.042+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T02:28:27.042+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T02:28:27.051+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:28:27.043+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 40, in <module>
    def upload_to_s3(s3: s3fs.S3FileSystem, file_path: str, bucket:str, s3_file_name: str):
NameError: name 's3fs' is not defined
[2024-07-24T02:28:27.053+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:28:27.075+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.078 seconds
[2024-07-24T02:28:57.172+0000] {processor.py:161} INFO - Started process (PID=225) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:28:57.174+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T02:28:57.175+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:28:57.174+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:28:57.210+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T02:28:57.211+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T02:28:57.219+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:28:57.211+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 40, in <module>
    def upload_to_s3(s3: s3fs.S3FileSystem, file_path: str, bucket:str, s3_file_name: str):
NameError: name 's3fs' is not defined
[2024-07-24T02:28:57.221+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:28:57.239+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.075 seconds
[2024-07-24T02:29:27.379+0000] {processor.py:161} INFO - Started process (PID=226) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:29:27.380+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T02:29:27.381+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:29:27.381+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:29:27.413+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T02:29:27.414+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T02:29:27.422+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:29:27.414+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 40, in <module>
    def upload_to_s3(s3: s3fs.S3FileSystem, file_path: str, bucket:str, s3_file_name: str):
NameError: name 's3fs' is not defined
[2024-07-24T02:29:27.424+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:29:27.446+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.074 seconds
[2024-07-24T02:29:57.546+0000] {processor.py:161} INFO - Started process (PID=227) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:29:57.547+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T02:29:57.548+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:29:57.548+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:29:57.586+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T02:29:57.587+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T02:29:57.595+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:29:57.587+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 40, in <module>
    def upload_to_s3(s3: s3fs.S3FileSystem, file_path: str, bucket:str, s3_file_name: str):
NameError: name 's3fs' is not defined
[2024-07-24T02:29:57.597+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:29:57.617+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.079 seconds
[2024-07-24T02:30:27.783+0000] {processor.py:161} INFO - Started process (PID=228) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:30:27.784+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T02:30:27.785+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:30:27.784+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:30:27.819+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T02:30:27.820+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T02:30:27.827+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:30:27.820+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 40, in <module>
    def upload_to_s3(s3: s3fs.S3FileSystem, file_path: str, bucket:str, s3_file_name: str):
NameError: name 's3fs' is not defined
[2024-07-24T02:30:27.829+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:30:27.849+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.076 seconds
[2024-07-24T02:30:57.944+0000] {processor.py:161} INFO - Started process (PID=229) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:30:57.945+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T02:30:57.946+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:30:57.946+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:30:57.977+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T02:30:57.978+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T02:30:57.986+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:30:57.979+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 40, in <module>
    def upload_to_s3(s3: s3fs.S3FileSystem, file_path: str, bucket:str, s3_file_name: str):
NameError: name 's3fs' is not defined
[2024-07-24T02:30:57.988+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:30:58.007+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.069 seconds
[2024-07-24T02:31:28.154+0000] {processor.py:161} INFO - Started process (PID=230) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:31:28.155+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T02:31:28.156+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:31:28.155+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:31:28.195+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T02:31:28.196+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T02:31:28.206+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:31:28.197+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 40, in <module>
    def upload_to_s3(s3: s3fs.S3FileSystem, file_path: str, bucket:str, s3_file_name: str):
NameError: name 's3fs' is not defined
[2024-07-24T02:31:28.208+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:31:28.233+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.087 seconds
[2024-07-24T02:31:58.313+0000] {processor.py:161} INFO - Started process (PID=231) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:31:58.314+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T02:31:58.315+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:31:58.315+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:31:58.346+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T02:31:58.347+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T02:31:58.354+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:31:58.347+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 40, in <module>
    def upload_to_s3(s3: s3fs.S3FileSystem, file_path: str, bucket:str, s3_file_name: str):
NameError: name 's3fs' is not defined
[2024-07-24T02:31:58.356+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:31:58.373+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.068 seconds
[2024-07-24T02:32:28.535+0000] {processor.py:161} INFO - Started process (PID=232) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:32:28.536+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T02:32:28.536+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:32:28.536+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:32:28.569+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T02:32:28.569+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T02:32:28.578+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:32:28.570+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 40, in <module>
    def upload_to_s3(s3: s3fs.S3FileSystem, file_path: str, bucket:str, s3_file_name: str):
NameError: name 's3fs' is not defined
[2024-07-24T02:32:28.580+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:32:28.600+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.071 seconds
[2024-07-24T02:32:59.105+0000] {processor.py:161} INFO - Started process (PID=233) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:32:59.121+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T02:32:59.127+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:32:59.126+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:32:59.239+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T02:32:59.240+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T02:32:59.253+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:32:59.242+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 40, in <module>
    def upload_to_s3(s3: s3fs.S3FileSystem, file_path: str, bucket:str, s3_file_name: str):
NameError: name 's3fs' is not defined
[2024-07-24T02:32:59.256+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:32:59.470+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.532 seconds
[2024-07-24T02:33:29.707+0000] {processor.py:161} INFO - Started process (PID=234) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:33:29.710+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T02:33:29.712+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:33:29.712+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:33:29.761+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T02:33:29.761+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T02:33:29.770+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:33:29.762+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 40, in <module>
    def upload_to_s3(s3: s3fs.S3FileSystem, file_path: str, bucket:str, s3_file_name: str):
NameError: name 's3fs' is not defined
[2024-07-24T02:33:29.772+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:33:29.849+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.161 seconds
[2024-07-24T02:33:59.903+0000] {processor.py:161} INFO - Started process (PID=235) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:33:59.904+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T02:33:59.905+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:33:59.905+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:33:59.945+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T02:33:59.946+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T02:33:59.957+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:33:59.950+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 2, in <module>
    import s3fs
ModuleNotFoundError: No module named 's3fs'
[2024-07-24T02:33:59.959+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:33:59.980+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.087 seconds
[2024-07-24T02:34:30.060+0000] {processor.py:161} INFO - Started process (PID=236) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:34:30.061+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T02:34:30.062+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:34:30.062+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:34:30.090+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T02:34:30.091+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T02:34:30.098+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:34:30.092+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 2, in <module>
    import s3fs
ModuleNotFoundError: No module named 's3fs'
[2024-07-24T02:34:30.100+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:34:30.122+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.076 seconds
[2024-07-24T02:35:00.303+0000] {processor.py:161} INFO - Started process (PID=237) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:35:00.306+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T02:35:00.308+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:35:00.307+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:35:00.347+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T02:35:00.348+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T02:35:00.357+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:35:00.349+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 2, in <module>
    import s3fs
ModuleNotFoundError: No module named 's3fs'
[2024-07-24T02:35:00.359+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:35:00.393+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.105 seconds
[2024-07-24T02:35:30.519+0000] {processor.py:161} INFO - Started process (PID=238) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:35:30.520+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T02:35:30.521+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:35:30.521+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:35:30.573+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T02:35:30.573+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T02:35:30.583+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:35:30.575+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 2, in <module>
    import s3fs
ModuleNotFoundError: No module named 's3fs'
[2024-07-24T02:35:30.585+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:35:30.605+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.094 seconds
[2024-07-24T02:36:00.701+0000] {processor.py:161} INFO - Started process (PID=239) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:36:00.702+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T02:36:00.703+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:36:00.702+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:36:00.885+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T02:36:00.886+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T02:36:00.894+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:36:00.887+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 2, in <module>
    import s3fs
ModuleNotFoundError: No module named 's3fs'
[2024-07-24T02:36:00.896+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:36:00.917+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.224 seconds
[2024-07-24T02:36:31.078+0000] {processor.py:161} INFO - Started process (PID=240) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:36:31.079+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T02:36:31.080+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:36:31.080+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:36:31.112+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T02:36:31.112+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T02:36:31.120+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:36:31.113+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 2, in <module>
    import s3fs
ModuleNotFoundError: No module named 's3fs'
[2024-07-24T02:36:31.122+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:36:31.143+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.071 seconds
[2024-07-24T02:37:01.233+0000] {processor.py:161} INFO - Started process (PID=241) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:37:01.234+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T02:37:01.235+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:37:01.235+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:37:01.273+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T02:37:01.273+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T02:37:01.280+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:37:01.274+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 39, in <module>
    def upload_to_s3(s3: s3fs.S3FileSystem, file_path: str, bucket:str, s3_file_name: str):
NameError: name 's3fs' is not defined
[2024-07-24T02:37:01.282+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:37:01.298+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.070 seconds
[2024-07-24T02:37:31.492+0000] {processor.py:161} INFO - Started process (PID=242) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:37:31.495+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T02:37:31.497+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:37:31.497+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:37:31.550+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T02:37:31.550+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T02:37:31.560+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:37:31.552+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 2, in <module>
    import boto
ModuleNotFoundError: No module named 'boto'
[2024-07-24T02:37:31.561+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:37:31.603+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.140 seconds
[2024-07-24T02:38:01.754+0000] {processor.py:161} INFO - Started process (PID=243) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:38:01.756+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T02:38:01.756+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:38:01.756+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:38:01.796+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T02:38:01.797+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T02:38:01.804+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:38:01.797+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 39, in <module>
    def upload_to_s3(s3: s3fs.S3FileSystem, file_path: str, bucket:str, s3_file_name: str):
NameError: name 's3fs' is not defined
[2024-07-24T02:38:01.805+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:38:01.823+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.074 seconds
[2024-07-24T02:38:31.899+0000] {processor.py:161} INFO - Started process (PID=244) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:38:31.900+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T02:38:31.901+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:38:31.901+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:38:31.934+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T02:38:31.935+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T02:38:31.941+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:38:31.935+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 39, in <module>
    def upload_to_s3(s3: s3fs.S3FileSystem, file_path: str, bucket:str, s3_file_name: str):
NameError: name 's3fs' is not defined
[2024-07-24T02:38:31.943+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:38:31.960+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.067 seconds
[2024-07-24T02:39:02.173+0000] {processor.py:161} INFO - Started process (PID=245) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:39:02.174+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T02:39:02.175+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:39:02.175+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:39:02.208+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T02:39:02.209+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T02:39:02.216+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:39:02.209+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 39, in <module>
    def upload_to_s3(s3: s3fs.S3FileSystem, file_path: str, bucket:str, s3_file_name: str):
NameError: name 's3fs' is not defined
[2024-07-24T02:39:02.218+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:39:02.236+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.093 seconds
[2024-07-24T02:39:32.300+0000] {processor.py:161} INFO - Started process (PID=246) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:39:32.301+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T02:39:32.305+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:39:32.303+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:39:32.340+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T02:39:32.341+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T02:39:32.348+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:39:32.341+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 39, in <module>
    def upload_to_s3(s3: s3fs.S3FileSystem, file_path: str, bucket:str, s3_file_name: str):
NameError: name 's3fs' is not defined
[2024-07-24T02:39:32.349+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:39:32.372+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.078 seconds
[2024-07-24T02:40:02.533+0000] {processor.py:161} INFO - Started process (PID=247) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:40:02.535+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T02:40:02.541+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:40:02.539+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:40:02.582+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T02:40:02.583+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T02:40:02.590+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:40:02.583+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 39, in <module>
    def upload_to_s3(s3: s3fs.S3FileSystem, file_path: str, bucket:str, s3_file_name: str):
NameError: name 's3fs' is not defined
[2024-07-24T02:40:02.591+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:40:02.611+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.084 seconds
[2024-07-24T02:40:32.746+0000] {processor.py:161} INFO - Started process (PID=248) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:40:32.750+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T02:40:32.755+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:40:32.754+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:40:32.795+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T02:40:32.796+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T02:40:32.805+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:40:32.796+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 39, in <module>
    def upload_to_s3(s3: s3fs.S3FileSystem, file_path: str, bucket:str, s3_file_name: str):
NameError: name 's3fs' is not defined
[2024-07-24T02:40:32.807+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:40:32.829+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.103 seconds
[2024-07-24T02:41:02.930+0000] {processor.py:161} INFO - Started process (PID=249) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:41:02.931+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T02:41:02.932+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:41:02.932+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:41:02.961+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T02:41:02.961+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T02:41:02.968+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:41:02.961+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 39, in <module>
    def upload_to_s3(s3: s3fs.S3FileSystem, file_path: str, bucket:str, s3_file_name: str):
NameError: name 's3fs' is not defined
[2024-07-24T02:41:02.969+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:41:02.992+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.067 seconds
[2024-07-24T02:41:33.130+0000] {processor.py:161} INFO - Started process (PID=250) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:41:33.131+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T02:41:33.133+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:41:33.133+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:41:33.163+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T02:41:33.163+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T02:41:33.170+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:41:33.164+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 39, in <module>
    def upload_to_s3(s3: s3fs.S3FileSystem, file_path: str, bucket:str, s3_file_name: str):
NameError: name 's3fs' is not defined
[2024-07-24T02:41:33.171+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:41:33.189+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.064 seconds
[2024-07-24T02:42:03.289+0000] {processor.py:161} INFO - Started process (PID=251) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:42:03.290+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T02:42:03.291+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:42:03.290+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:42:03.321+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T02:42:03.321+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T02:42:03.328+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:42:03.322+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 39, in <module>
    def upload_to_s3(s3: s3fs.S3FileSystem, file_path: str, bucket:str, s3_file_name: str):
NameError: name 's3fs' is not defined
[2024-07-24T02:42:03.330+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:42:03.351+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.067 seconds
[2024-07-24T02:42:33.465+0000] {processor.py:161} INFO - Started process (PID=252) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:42:33.465+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T02:42:33.466+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:42:33.466+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:42:33.496+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T02:42:33.497+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T02:42:33.505+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:42:33.497+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 39, in <module>
    def upload_to_s3(s3: s3fs.S3FileSystem, file_path: str, bucket:str, s3_file_name: str):
NameError: name 's3fs' is not defined
[2024-07-24T02:42:33.507+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:42:33.521+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.061 seconds
[2024-07-24T02:43:03.740+0000] {processor.py:161} INFO - Started process (PID=253) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:43:03.744+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T02:43:03.746+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:43:03.745+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:43:03.848+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T02:43:03.849+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T02:43:03.876+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:43:03.850+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 39, in <module>
    def upload_to_s3(s3: s3fs.S3FileSystem, file_path: str, bucket:str, s3_file_name: str):
NameError: name 's3fs' is not defined
[2024-07-24T02:43:03.880+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:43:03.911+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.196 seconds
[2024-07-24T02:43:34.149+0000] {processor.py:161} INFO - Started process (PID=254) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:43:34.150+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T02:43:34.151+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:43:34.150+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:43:34.194+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T02:43:34.194+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T02:43:34.201+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:43:34.195+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 39, in <module>
    def upload_to_s3(s3: s3fs.S3FileSystem, file_path: str, bucket:str, s3_file_name: str):
NameError: name 's3fs' is not defined
[2024-07-24T02:43:34.203+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:43:34.219+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.139 seconds
[2024-07-24T02:44:04.376+0000] {processor.py:161} INFO - Started process (PID=255) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:44:04.377+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T02:44:04.378+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:44:04.378+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:44:04.408+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T02:44:04.408+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T02:44:04.415+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:44:04.409+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 39, in <module>
    def upload_to_s3(s3: s3fs.S3FileSystem, file_path: str, bucket:str, s3_file_name: str):
NameError: name 's3fs' is not defined
[2024-07-24T02:44:04.417+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:44:04.435+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.064 seconds
[2024-07-24T02:44:34.528+0000] {processor.py:161} INFO - Started process (PID=256) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:44:34.529+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T02:44:34.530+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:44:34.529+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:44:34.561+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T02:44:34.562+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T02:44:34.568+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:44:34.562+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 39, in <module>
    def upload_to_s3(s3: s3fs.S3FileSystem, file_path: str, bucket:str, s3_file_name: str):
NameError: name 's3fs' is not defined
[2024-07-24T02:44:34.570+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:44:34.587+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.065 seconds
[2024-07-24T02:45:04.742+0000] {processor.py:161} INFO - Started process (PID=257) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:45:04.743+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T02:45:04.744+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:45:04.744+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:45:04.772+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T02:45:04.773+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T02:45:04.779+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:45:04.773+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 39, in <module>
    def upload_to_s3(s3: s3fs.S3FileSystem, file_path: str, bucket:str, s3_file_name: str):
NameError: name 's3fs' is not defined
[2024-07-24T02:45:04.781+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:45:04.798+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.061 seconds
[2024-07-24T02:45:34.917+0000] {processor.py:161} INFO - Started process (PID=258) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:45:34.918+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T02:45:34.919+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:45:34.919+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:45:34.950+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T02:45:34.950+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T02:45:34.957+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:45:34.951+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 39, in <module>
    def upload_to_s3(s3: s3fs.S3FileSystem, file_path: str, bucket:str, s3_file_name: str):
NameError: name 's3fs' is not defined
[2024-07-24T02:45:34.959+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:45:34.976+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.066 seconds
[2024-07-24T02:46:05.101+0000] {processor.py:161} INFO - Started process (PID=259) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:46:05.103+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T02:46:05.104+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:46:05.104+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:46:05.145+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T02:46:05.146+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T02:46:05.155+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:46:05.146+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 39, in <module>
    def upload_to_s3(s3: s3fs.S3FileSystem, file_path: str, bucket:str, s3_file_name: str):
NameError: name 's3fs' is not defined
[2024-07-24T02:46:05.158+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:46:05.180+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.086 seconds
[2024-07-24T02:46:35.295+0000] {processor.py:161} INFO - Started process (PID=260) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:46:35.296+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T02:46:35.298+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:46:35.297+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:46:35.337+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T02:46:35.338+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T02:46:35.346+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:46:35.338+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 39, in <module>
    def upload_to_s3(s3: s3fs.S3FileSystem, file_path: str, bucket:str, s3_file_name: str):
NameError: name 's3fs' is not defined
[2024-07-24T02:46:35.347+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:46:35.367+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.079 seconds
[2024-07-24T02:47:05.485+0000] {processor.py:161} INFO - Started process (PID=261) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:47:05.487+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T02:47:05.488+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:47:05.487+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:47:05.519+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T02:47:05.520+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T02:47:05.528+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:47:05.520+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 39, in <module>
    def upload_to_s3(s3: s3fs.S3FileSystem, file_path: str, bucket:str, s3_file_name: str):
NameError: name 's3fs' is not defined
[2024-07-24T02:47:05.530+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:47:05.549+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.069 seconds
[2024-07-24T02:47:35.743+0000] {processor.py:161} INFO - Started process (PID=262) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:47:35.749+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T02:47:35.754+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:47:35.752+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:47:35.858+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T02:47:35.859+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T02:47:35.868+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:47:35.859+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 40, in <module>
    def upload_to_s3(s3: s3fs.S3FileSystem, file_path: str, bucket:str, s3_file_name: str):
NameError: name 's3fs' is not defined
[2024-07-24T02:47:35.872+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:47:36.021+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.357 seconds
[2024-07-24T02:48:06.204+0000] {processor.py:161} INFO - Started process (PID=263) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:48:06.206+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T02:48:06.207+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:48:06.207+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:48:06.244+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T02:48:06.244+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T02:48:06.253+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:48:06.245+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 40, in <module>
    def upload_to_s3(s3: s3fs.S3FileSystem, file_path: str, bucket:str, s3_file_name: str):
NameError: name 's3fs' is not defined
[2024-07-24T02:48:06.255+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:48:06.278+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.088 seconds
[2024-07-24T02:48:36.343+0000] {processor.py:161} INFO - Started process (PID=264) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:48:36.344+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T02:48:36.345+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:48:36.344+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:48:36.380+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T02:48:36.380+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T02:48:36.388+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:48:36.382+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 2, in <module>
    import s3fs
ModuleNotFoundError: No module named 's3fs'
[2024-07-24T02:48:36.390+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:48:36.408+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.070 seconds
[2024-07-24T02:49:06.581+0000] {processor.py:161} INFO - Started process (PID=265) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:49:06.582+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T02:49:06.583+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:49:06.583+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:49:06.612+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T02:49:06.612+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T02:49:06.620+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:49:06.614+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 2, in <module>
    import s3fs
ModuleNotFoundError: No module named 's3fs'
[2024-07-24T02:49:06.622+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:49:06.640+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.063 seconds
[2024-07-24T02:49:36.736+0000] {processor.py:161} INFO - Started process (PID=266) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:49:36.738+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T02:49:36.738+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:49:36.738+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:49:36.768+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T02:49:36.768+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T02:49:36.775+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:49:36.769+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 2, in <module>
    import s3fs
ModuleNotFoundError: No module named 's3fs'
[2024-07-24T02:49:36.777+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:49:36.794+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.065 seconds
[2024-07-24T02:50:06.939+0000] {processor.py:161} INFO - Started process (PID=267) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:50:06.951+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T02:50:06.961+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:50:06.957+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:50:07.058+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T02:50:07.059+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T02:50:07.074+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:50:07.063+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 2, in <module>
    import s3fs
ModuleNotFoundError: No module named 's3fs'
[2024-07-24T02:50:07.077+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:50:07.166+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.233 seconds
[2024-07-24T02:50:37.387+0000] {processor.py:161} INFO - Started process (PID=268) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:50:37.388+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T02:50:37.389+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:50:37.388+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:50:37.430+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T02:50:37.431+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T02:50:37.439+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:50:37.432+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 2, in <module>
    import s3fs
ModuleNotFoundError: No module named 's3fs'
[2024-07-24T02:50:37.441+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:50:37.543+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.183 seconds
[2024-07-24T02:51:07.738+0000] {processor.py:161} INFO - Started process (PID=269) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:51:07.743+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T02:51:07.745+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:51:07.745+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:51:07.787+0000] {logging_mixin.py:188} INFO - PARSER IS ALIVE
[2024-07-24T02:51:07.788+0000] {logging_mixin.py:188} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T02:51:07.796+0000] {logging_mixin.py:188} INFO - [2024-07-24T02:51:07.789+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 11, in <module>
    from pipelines.aws_s3_pipeline import upload_s3_pipeline
  File "/opt/airflow/pipelines/aws_s3_pipeline.py", line 2, in <module>
    from etls.aws_etl import connect_to_s3, create_s3_bucket_if_not_exists, upload_to_s3
  File "/opt/airflow/etls/aws_etl.py", line 2, in <module>
    import s3fs
ModuleNotFoundError: No module named 's3fs'
[2024-07-24T02:51:07.798+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T02:51:07.832+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.120 seconds
[2024-07-24T03:31:23.973+0000] {processor.py:157} INFO - Started process (PID=37) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T03:31:23.974+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T03:31:23.977+0000] {logging_mixin.py:151} INFO - [2024-07-24T03:31:23.976+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T03:31:24.037+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T03:31:24.039+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T03:31:27.640+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T03:31:27.894+0000] {logging_mixin.py:151} INFO - [2024-07-24T03:31:27.894+0000] {manager.py:499} INFO - Created Permission View: %s
[2024-07-24T03:31:27.906+0000] {logging_mixin.py:151} INFO - [2024-07-24T03:31:27.905+0000] {manager.py:499} INFO - Created Permission View: %s
[2024-07-24T03:31:27.914+0000] {logging_mixin.py:151} INFO - [2024-07-24T03:31:27.914+0000] {manager.py:499} INFO - Created Permission View: %s
[2024-07-24T03:31:27.915+0000] {logging_mixin.py:151} INFO - [2024-07-24T03:31:27.915+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T03:31:27.929+0000] {logging_mixin.py:151} INFO - [2024-07-24T03:31:27.928+0000] {dag.py:2963} INFO - Creating ORM DAG for etl_reddit_pipeline
[2024-07-24T03:31:27.942+0000] {logging_mixin.py:151} INFO - [2024-07-24T03:31:27.941+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-23T00:00:00+00:00, run_after=2024-07-24T00:00:00+00:00
[2024-07-24T03:31:27.965+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 3.996 seconds
[2024-07-24T03:31:58.193+0000] {processor.py:157} INFO - Started process (PID=39) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T03:31:58.196+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T03:31:58.207+0000] {logging_mixin.py:151} INFO - [2024-07-24T03:31:58.205+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T03:31:58.280+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T03:31:58.281+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T03:31:59.484+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T03:31:59.499+0000] {logging_mixin.py:151} INFO - [2024-07-24T03:31:59.498+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T03:31:59.530+0000] {logging_mixin.py:151} INFO - [2024-07-24T03:31:59.529+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-23T00:00:00+00:00, run_after=2024-07-24T00:00:00+00:00
[2024-07-24T03:31:59.551+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 1.367 seconds
[2024-07-24T03:32:29.742+0000] {processor.py:157} INFO - Started process (PID=41) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T03:32:29.744+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T03:32:29.748+0000] {logging_mixin.py:151} INFO - [2024-07-24T03:32:29.747+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T03:32:29.792+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T03:32:29.793+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T03:32:30.621+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T03:32:30.650+0000] {logging_mixin.py:151} INFO - [2024-07-24T03:32:30.649+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T03:32:30.678+0000] {logging_mixin.py:151} INFO - [2024-07-24T03:32:30.678+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T03:32:30.698+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.961 seconds
[2024-07-24T03:33:00.898+0000] {processor.py:157} INFO - Started process (PID=43) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T03:33:00.899+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T03:33:00.902+0000] {logging_mixin.py:151} INFO - [2024-07-24T03:33:00.901+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T03:33:00.936+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T03:33:00.936+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T03:33:01.620+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T03:33:01.648+0000] {logging_mixin.py:151} INFO - [2024-07-24T03:33:01.648+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T03:33:01.675+0000] {logging_mixin.py:151} INFO - [2024-07-24T03:33:01.675+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T03:33:01.694+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.799 seconds
[2024-07-24T03:33:31.742+0000] {processor.py:157} INFO - Started process (PID=45) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T03:33:31.743+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T03:33:31.746+0000] {logging_mixin.py:151} INFO - [2024-07-24T03:33:31.746+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T03:33:31.782+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T03:33:31.782+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T03:33:32.489+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T03:33:32.522+0000] {logging_mixin.py:151} INFO - [2024-07-24T03:33:32.521+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T03:33:32.549+0000] {logging_mixin.py:151} INFO - [2024-07-24T03:33:32.548+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T03:33:32.565+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.827 seconds
[2024-07-24T03:34:02.762+0000] {processor.py:157} INFO - Started process (PID=47) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T03:34:02.763+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T03:34:02.765+0000] {logging_mixin.py:151} INFO - [2024-07-24T03:34:02.765+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T03:34:02.797+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T03:34:02.797+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T03:34:03.398+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T03:34:03.429+0000] {logging_mixin.py:151} INFO - [2024-07-24T03:34:03.428+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T03:34:03.453+0000] {logging_mixin.py:151} INFO - [2024-07-24T03:34:03.452+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T03:34:03.469+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.712 seconds
[2024-07-24T03:34:33.831+0000] {processor.py:157} INFO - Started process (PID=49) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T03:34:33.832+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T03:34:33.834+0000] {logging_mixin.py:151} INFO - [2024-07-24T03:34:33.834+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T03:34:33.869+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T03:34:33.870+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T03:34:34.317+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T03:34:34.338+0000] {logging_mixin.py:151} INFO - [2024-07-24T03:34:34.338+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T03:34:34.360+0000] {logging_mixin.py:151} INFO - [2024-07-24T03:34:34.360+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T03:34:34.377+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.550 seconds
[2024-07-24T03:35:04.547+0000] {processor.py:157} INFO - Started process (PID=51) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T03:35:04.549+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T03:35:04.557+0000] {logging_mixin.py:151} INFO - [2024-07-24T03:35:04.556+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T03:35:04.606+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T03:35:04.607+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T03:35:05.237+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T03:35:05.265+0000] {logging_mixin.py:151} INFO - [2024-07-24T03:35:05.265+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T03:35:05.293+0000] {logging_mixin.py:151} INFO - [2024-07-24T03:35:05.293+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T03:35:05.312+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.769 seconds
[2024-07-24T03:35:35.466+0000] {processor.py:157} INFO - Started process (PID=53) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T03:35:35.467+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T03:35:35.469+0000] {logging_mixin.py:151} INFO - [2024-07-24T03:35:35.468+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T03:35:35.510+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T03:35:35.510+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T03:35:36.013+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T03:35:36.039+0000] {logging_mixin.py:151} INFO - [2024-07-24T03:35:36.038+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T03:35:36.071+0000] {logging_mixin.py:151} INFO - [2024-07-24T03:35:36.071+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T03:35:36.097+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.633 seconds
[2024-07-24T03:36:06.294+0000] {processor.py:157} INFO - Started process (PID=55) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T03:36:06.295+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T03:36:06.297+0000] {logging_mixin.py:151} INFO - [2024-07-24T03:36:06.297+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T03:36:06.330+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T03:36:06.331+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T03:36:06.890+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T03:36:06.939+0000] {logging_mixin.py:151} INFO - [2024-07-24T03:36:06.938+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T03:36:06.971+0000] {logging_mixin.py:151} INFO - [2024-07-24T03:36:06.970+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T03:36:06.990+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.699 seconds
[2024-07-24T03:36:37.210+0000] {processor.py:157} INFO - Started process (PID=57) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T03:36:37.214+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T03:36:37.216+0000] {logging_mixin.py:151} INFO - [2024-07-24T03:36:37.215+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T03:36:37.267+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T03:36:37.267+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T03:36:37.853+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T03:36:37.889+0000] {logging_mixin.py:151} INFO - [2024-07-24T03:36:37.888+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T03:36:37.920+0000] {logging_mixin.py:151} INFO - [2024-07-24T03:36:37.920+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T03:36:37.947+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.741 seconds
[2024-07-24T03:37:08.131+0000] {processor.py:157} INFO - Started process (PID=59) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T03:37:08.132+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T03:37:08.134+0000] {logging_mixin.py:151} INFO - [2024-07-24T03:37:08.134+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T03:37:08.164+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T03:37:08.165+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T03:37:08.668+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T03:37:08.693+0000] {logging_mixin.py:151} INFO - [2024-07-24T03:37:08.692+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T03:37:08.718+0000] {logging_mixin.py:151} INFO - [2024-07-24T03:37:08.718+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T03:37:08.736+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.607 seconds
[2024-07-24T03:37:38.951+0000] {processor.py:157} INFO - Started process (PID=61) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T03:37:38.953+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T03:37:38.955+0000] {logging_mixin.py:151} INFO - [2024-07-24T03:37:38.954+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T03:37:39.005+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T03:37:39.005+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T03:37:39.555+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T03:37:39.579+0000] {logging_mixin.py:151} INFO - [2024-07-24T03:37:39.579+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T03:37:39.602+0000] {logging_mixin.py:151} INFO - [2024-07-24T03:37:39.602+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T03:37:39.620+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.673 seconds
[2024-07-24T03:38:09.798+0000] {processor.py:157} INFO - Started process (PID=63) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T03:38:09.799+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T03:38:09.801+0000] {logging_mixin.py:151} INFO - [2024-07-24T03:38:09.801+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T03:38:09.836+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T03:38:09.837+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T03:38:10.371+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T03:38:10.411+0000] {logging_mixin.py:151} INFO - [2024-07-24T03:38:10.411+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T03:38:10.444+0000] {logging_mixin.py:151} INFO - [2024-07-24T03:38:10.443+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T03:38:10.466+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.672 seconds
[2024-07-24T03:38:40.622+0000] {processor.py:157} INFO - Started process (PID=65) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T03:38:40.622+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T03:38:40.624+0000] {logging_mixin.py:151} INFO - [2024-07-24T03:38:40.624+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T03:38:40.659+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T03:38:40.660+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T03:38:41.139+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T03:38:41.163+0000] {logging_mixin.py:151} INFO - [2024-07-24T03:38:41.162+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T03:38:41.187+0000] {logging_mixin.py:151} INFO - [2024-07-24T03:38:41.186+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T03:38:41.210+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.591 seconds
[2024-07-24T03:39:11.404+0000] {processor.py:157} INFO - Started process (PID=67) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T03:39:11.406+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T03:39:11.409+0000] {logging_mixin.py:151} INFO - [2024-07-24T03:39:11.409+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T03:39:11.444+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T03:39:11.445+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T03:39:11.939+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T03:39:11.960+0000] {logging_mixin.py:151} INFO - [2024-07-24T03:39:11.959+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T03:39:11.983+0000] {logging_mixin.py:151} INFO - [2024-07-24T03:39:11.983+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T03:39:12.003+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.605 seconds
[2024-07-24T03:39:42.175+0000] {processor.py:157} INFO - Started process (PID=69) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T03:39:42.176+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T03:39:42.178+0000] {logging_mixin.py:151} INFO - [2024-07-24T03:39:42.178+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T03:39:42.209+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T03:39:42.210+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T03:39:42.790+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T03:39:42.836+0000] {logging_mixin.py:151} INFO - [2024-07-24T03:39:42.835+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T03:39:42.870+0000] {logging_mixin.py:151} INFO - [2024-07-24T03:39:42.870+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T03:39:42.893+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.721 seconds
[2024-07-24T03:40:13.069+0000] {processor.py:157} INFO - Started process (PID=71) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T03:40:13.070+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T03:40:13.073+0000] {logging_mixin.py:151} INFO - [2024-07-24T03:40:13.072+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T03:40:13.105+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T03:40:13.105+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T03:40:13.561+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T03:40:13.583+0000] {logging_mixin.py:151} INFO - [2024-07-24T03:40:13.582+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T03:40:13.604+0000] {logging_mixin.py:151} INFO - [2024-07-24T03:40:13.604+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T03:40:13.620+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.554 seconds
[2024-07-24T03:40:43.788+0000] {processor.py:157} INFO - Started process (PID=73) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T03:40:43.790+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T03:40:43.792+0000] {logging_mixin.py:151} INFO - [2024-07-24T03:40:43.792+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T03:40:43.834+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T03:40:43.834+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T03:40:44.438+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T03:40:44.465+0000] {logging_mixin.py:151} INFO - [2024-07-24T03:40:44.464+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T03:40:44.489+0000] {logging_mixin.py:151} INFO - [2024-07-24T03:40:44.489+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T03:40:44.507+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.722 seconds
[2024-07-24T03:41:14.627+0000] {processor.py:157} INFO - Started process (PID=75) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T03:41:14.628+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T03:41:14.630+0000] {logging_mixin.py:151} INFO - [2024-07-24T03:41:14.630+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T03:41:14.664+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T03:41:14.665+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T03:41:15.115+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T03:41:15.134+0000] {logging_mixin.py:151} INFO - [2024-07-24T03:41:15.133+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T03:41:15.156+0000] {logging_mixin.py:151} INFO - [2024-07-24T03:41:15.156+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T03:41:15.181+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.558 seconds
[2024-07-24T03:41:45.392+0000] {processor.py:157} INFO - Started process (PID=77) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T03:41:45.393+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T03:41:45.395+0000] {logging_mixin.py:151} INFO - [2024-07-24T03:41:45.395+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T03:41:45.429+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T03:41:45.430+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T03:41:45.965+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T03:41:45.990+0000] {logging_mixin.py:151} INFO - [2024-07-24T03:41:45.990+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T03:41:46.016+0000] {logging_mixin.py:151} INFO - [2024-07-24T03:41:46.016+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T03:41:46.032+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.644 seconds
[2024-07-24T03:42:16.201+0000] {processor.py:157} INFO - Started process (PID=79) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T03:42:16.203+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T03:42:16.206+0000] {logging_mixin.py:151} INFO - [2024-07-24T03:42:16.205+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T03:42:16.242+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T03:42:16.243+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T03:42:16.736+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T03:42:16.759+0000] {logging_mixin.py:151} INFO - [2024-07-24T03:42:16.759+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T03:42:16.783+0000] {logging_mixin.py:151} INFO - [2024-07-24T03:42:16.783+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T03:42:16.802+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.604 seconds
[2024-07-24T03:42:46.986+0000] {processor.py:157} INFO - Started process (PID=81) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T03:42:46.987+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T03:42:46.989+0000] {logging_mixin.py:151} INFO - [2024-07-24T03:42:46.989+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T03:42:47.022+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T03:42:47.022+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T03:42:47.516+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T03:42:47.544+0000] {logging_mixin.py:151} INFO - [2024-07-24T03:42:47.543+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T03:42:47.568+0000] {logging_mixin.py:151} INFO - [2024-07-24T03:42:47.568+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T03:42:47.589+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.607 seconds
[2024-07-24T03:43:17.765+0000] {processor.py:157} INFO - Started process (PID=83) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T03:43:17.767+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T03:43:17.774+0000] {logging_mixin.py:151} INFO - [2024-07-24T03:43:17.773+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T03:43:17.805+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T03:43:17.805+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T03:43:18.284+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T03:43:18.309+0000] {logging_mixin.py:151} INFO - [2024-07-24T03:43:18.308+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T03:43:18.332+0000] {logging_mixin.py:151} INFO - [2024-07-24T03:43:18.331+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T03:43:18.349+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.587 seconds
[2024-07-24T03:43:48.538+0000] {processor.py:157} INFO - Started process (PID=85) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T03:43:48.539+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T03:43:48.541+0000] {logging_mixin.py:151} INFO - [2024-07-24T03:43:48.541+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T03:43:48.574+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T03:43:48.574+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T03:43:49.028+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T03:43:49.054+0000] {logging_mixin.py:151} INFO - [2024-07-24T03:43:49.053+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T03:43:49.083+0000] {logging_mixin.py:151} INFO - [2024-07-24T03:43:49.083+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T03:43:49.099+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.565 seconds
[2024-07-24T03:44:19.270+0000] {processor.py:157} INFO - Started process (PID=87) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T03:44:19.271+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T03:44:19.273+0000] {logging_mixin.py:151} INFO - [2024-07-24T03:44:19.273+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T03:44:19.305+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T03:44:19.306+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T03:44:19.817+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T03:44:19.842+0000] {logging_mixin.py:151} INFO - [2024-07-24T03:44:19.841+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T03:44:19.866+0000] {logging_mixin.py:151} INFO - [2024-07-24T03:44:19.865+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T03:44:20.482+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 1.217 seconds
[2024-07-24T03:44:50.656+0000] {processor.py:157} INFO - Started process (PID=89) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T03:44:50.657+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T03:44:50.660+0000] {logging_mixin.py:151} INFO - [2024-07-24T03:44:50.660+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T03:44:50.692+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T03:44:50.692+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T03:44:51.147+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T03:44:51.171+0000] {logging_mixin.py:151} INFO - [2024-07-24T03:44:51.170+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T03:44:51.192+0000] {logging_mixin.py:151} INFO - [2024-07-24T03:44:51.192+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T03:44:51.208+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.557 seconds
[2024-07-24T03:45:21.396+0000] {processor.py:157} INFO - Started process (PID=91) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T03:45:21.397+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T03:45:21.399+0000] {logging_mixin.py:151} INFO - [2024-07-24T03:45:21.399+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T03:45:21.433+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T03:45:21.433+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T03:45:21.993+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T03:45:22.027+0000] {logging_mixin.py:151} INFO - [2024-07-24T03:45:22.026+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T03:45:22.057+0000] {logging_mixin.py:151} INFO - [2024-07-24T03:45:22.057+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T03:45:22.084+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.694 seconds
[2024-07-24T03:45:52.265+0000] {processor.py:157} INFO - Started process (PID=93) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T03:45:52.266+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T03:45:52.268+0000] {logging_mixin.py:151} INFO - [2024-07-24T03:45:52.267+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T03:45:52.301+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T03:45:52.302+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T03:45:52.783+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T03:45:52.810+0000] {logging_mixin.py:151} INFO - [2024-07-24T03:45:52.809+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T03:45:52.832+0000] {logging_mixin.py:151} INFO - [2024-07-24T03:45:52.832+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T03:45:52.849+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.588 seconds
[2024-07-24T03:46:23.036+0000] {processor.py:157} INFO - Started process (PID=95) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T03:46:23.038+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T03:46:23.041+0000] {logging_mixin.py:151} INFO - [2024-07-24T03:46:23.040+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T03:46:23.072+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T03:46:23.073+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T03:46:23.563+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T03:46:23.588+0000] {logging_mixin.py:151} INFO - [2024-07-24T03:46:23.587+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T03:46:23.616+0000] {logging_mixin.py:151} INFO - [2024-07-24T03:46:23.615+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T03:46:23.639+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.608 seconds
[2024-07-24T03:46:53.816+0000] {processor.py:157} INFO - Started process (PID=97) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T03:46:53.817+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T03:46:53.820+0000] {logging_mixin.py:151} INFO - [2024-07-24T03:46:53.820+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T03:46:53.853+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T03:46:53.853+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T03:46:54.335+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T03:46:54.355+0000] {logging_mixin.py:151} INFO - [2024-07-24T03:46:54.355+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T03:46:54.374+0000] {logging_mixin.py:151} INFO - [2024-07-24T03:46:54.374+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T03:46:54.391+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.581 seconds
[2024-07-24T03:47:24.572+0000] {processor.py:157} INFO - Started process (PID=99) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T03:47:24.574+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T03:47:24.576+0000] {logging_mixin.py:151} INFO - [2024-07-24T03:47:24.575+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T03:47:24.605+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T03:47:24.606+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T03:47:25.055+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T03:47:25.078+0000] {logging_mixin.py:151} INFO - [2024-07-24T03:47:25.077+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T03:47:25.099+0000] {logging_mixin.py:151} INFO - [2024-07-24T03:47:25.099+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T03:47:25.122+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.553 seconds
[2024-07-24T03:47:55.280+0000] {processor.py:157} INFO - Started process (PID=101) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T03:47:55.282+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T03:47:55.283+0000] {logging_mixin.py:151} INFO - [2024-07-24T03:47:55.283+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T03:47:55.315+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T03:47:55.315+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T03:47:55.770+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T03:47:55.792+0000] {logging_mixin.py:151} INFO - [2024-07-24T03:47:55.791+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T03:47:55.816+0000] {logging_mixin.py:151} INFO - [2024-07-24T03:47:55.816+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T03:47:55.835+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.560 seconds
[2024-07-24T03:48:26.020+0000] {processor.py:157} INFO - Started process (PID=103) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T03:48:26.022+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T03:48:26.024+0000] {logging_mixin.py:151} INFO - [2024-07-24T03:48:26.023+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T03:48:26.062+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T03:48:26.063+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T03:48:26.598+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T03:48:26.635+0000] {logging_mixin.py:151} INFO - [2024-07-24T03:48:26.634+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T03:48:26.680+0000] {logging_mixin.py:151} INFO - [2024-07-24T03:48:26.680+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T03:48:26.702+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.688 seconds
[2024-07-24T03:48:56.909+0000] {processor.py:157} INFO - Started process (PID=105) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T03:48:56.910+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T03:48:56.912+0000] {logging_mixin.py:151} INFO - [2024-07-24T03:48:56.912+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T03:48:56.943+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T03:48:56.944+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T03:48:57.410+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T03:48:57.439+0000] {logging_mixin.py:151} INFO - [2024-07-24T03:48:57.438+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T03:48:57.465+0000] {logging_mixin.py:151} INFO - [2024-07-24T03:48:57.464+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T03:48:57.481+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.577 seconds
[2024-07-24T03:49:27.672+0000] {processor.py:157} INFO - Started process (PID=107) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T03:49:27.673+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T03:49:27.676+0000] {logging_mixin.py:151} INFO - [2024-07-24T03:49:27.676+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T03:49:27.708+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T03:49:27.708+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T03:49:28.239+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T03:49:28.263+0000] {logging_mixin.py:151} INFO - [2024-07-24T03:49:28.263+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T03:49:28.287+0000] {logging_mixin.py:151} INFO - [2024-07-24T03:49:28.286+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T03:49:28.305+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.636 seconds
[2024-07-24T03:49:58.470+0000] {processor.py:157} INFO - Started process (PID=109) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T03:49:58.471+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T03:49:58.474+0000] {logging_mixin.py:151} INFO - [2024-07-24T03:49:58.474+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T03:49:58.506+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T03:49:58.507+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T03:49:58.946+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T03:49:58.967+0000] {logging_mixin.py:151} INFO - [2024-07-24T03:49:58.967+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T03:49:58.991+0000] {logging_mixin.py:151} INFO - [2024-07-24T03:49:58.991+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T03:49:59.010+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.543 seconds
[2024-07-24T03:50:29.194+0000] {processor.py:157} INFO - Started process (PID=111) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T03:50:29.195+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T03:50:29.197+0000] {logging_mixin.py:151} INFO - [2024-07-24T03:50:29.197+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T03:50:29.231+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T03:50:29.231+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T03:50:29.749+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T03:50:29.773+0000] {logging_mixin.py:151} INFO - [2024-07-24T03:50:29.772+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T03:50:29.804+0000] {logging_mixin.py:151} INFO - [2024-07-24T03:50:29.804+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T03:50:29.827+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.638 seconds
[2024-07-24T03:51:00.003+0000] {processor.py:157} INFO - Started process (PID=113) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T03:51:00.004+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T03:51:00.005+0000] {logging_mixin.py:151} INFO - [2024-07-24T03:51:00.005+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T03:51:00.042+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T03:51:00.043+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T03:51:00.646+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T03:51:00.676+0000] {logging_mixin.py:151} INFO - [2024-07-24T03:51:00.675+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T03:51:00.711+0000] {logging_mixin.py:151} INFO - [2024-07-24T03:51:00.711+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T03:51:00.741+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.744 seconds
[2024-07-24T03:51:30.975+0000] {processor.py:157} INFO - Started process (PID=115) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T03:51:30.978+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T03:51:30.985+0000] {logging_mixin.py:151} INFO - [2024-07-24T03:51:30.985+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T03:51:31.031+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T03:51:31.031+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T03:51:31.722+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T03:51:31.753+0000] {logging_mixin.py:151} INFO - [2024-07-24T03:51:31.753+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T03:51:31.781+0000] {logging_mixin.py:151} INFO - [2024-07-24T03:51:31.780+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T03:51:31.813+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.847 seconds
[2024-07-24T03:52:02.020+0000] {processor.py:157} INFO - Started process (PID=117) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T03:52:02.021+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T03:52:02.024+0000] {logging_mixin.py:151} INFO - [2024-07-24T03:52:02.023+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T03:52:02.061+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T03:52:02.061+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T03:52:02.553+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T03:52:02.574+0000] {logging_mixin.py:151} INFO - [2024-07-24T03:52:02.573+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T03:52:02.596+0000] {logging_mixin.py:151} INFO - [2024-07-24T03:52:02.596+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T03:52:02.618+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.602 seconds
[2024-07-24T03:52:32.790+0000] {processor.py:157} INFO - Started process (PID=119) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T03:52:32.790+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T03:52:32.793+0000] {logging_mixin.py:151} INFO - [2024-07-24T03:52:32.793+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T03:52:32.826+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T03:52:32.827+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T03:52:33.302+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T03:52:33.324+0000] {logging_mixin.py:151} INFO - [2024-07-24T03:52:33.323+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T03:52:33.350+0000] {logging_mixin.py:151} INFO - [2024-07-24T03:52:33.350+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T03:52:33.368+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.582 seconds
[2024-07-24T03:53:03.557+0000] {processor.py:157} INFO - Started process (PID=121) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T03:53:03.558+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T03:53:03.561+0000] {logging_mixin.py:151} INFO - [2024-07-24T03:53:03.560+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T03:53:03.590+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T03:53:03.590+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T03:53:04.026+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T03:53:04.050+0000] {logging_mixin.py:151} INFO - [2024-07-24T03:53:04.049+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T03:53:04.080+0000] {logging_mixin.py:151} INFO - [2024-07-24T03:53:04.080+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T03:53:04.560+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 1.006 seconds
[2024-07-24T03:53:34.721+0000] {processor.py:157} INFO - Started process (PID=123) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T03:53:34.721+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T03:53:34.723+0000] {logging_mixin.py:151} INFO - [2024-07-24T03:53:34.723+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T03:53:34.754+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T03:53:34.754+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T03:53:35.211+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T03:53:35.237+0000] {logging_mixin.py:151} INFO - [2024-07-24T03:53:35.236+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T03:53:35.268+0000] {logging_mixin.py:151} INFO - [2024-07-24T03:53:35.268+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T03:53:35.289+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.571 seconds
[2024-07-24T03:54:05.468+0000] {processor.py:157} INFO - Started process (PID=125) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T03:54:05.469+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T03:54:05.472+0000] {logging_mixin.py:151} INFO - [2024-07-24T03:54:05.471+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T03:54:05.504+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T03:54:05.504+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T03:54:05.963+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T03:54:05.987+0000] {logging_mixin.py:151} INFO - [2024-07-24T03:54:05.987+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T03:54:06.017+0000] {logging_mixin.py:151} INFO - [2024-07-24T03:54:06.017+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T03:54:06.041+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.576 seconds
[2024-07-24T03:54:36.210+0000] {processor.py:157} INFO - Started process (PID=127) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T03:54:36.211+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T03:54:36.213+0000] {logging_mixin.py:151} INFO - [2024-07-24T03:54:36.212+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T03:54:36.247+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T03:54:36.247+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T03:54:36.716+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T03:54:36.741+0000] {logging_mixin.py:151} INFO - [2024-07-24T03:54:36.741+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T03:54:36.767+0000] {logging_mixin.py:151} INFO - [2024-07-24T03:54:36.766+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T03:54:36.785+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.578 seconds
[2024-07-24T03:55:06.970+0000] {processor.py:157} INFO - Started process (PID=129) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T03:55:06.971+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T03:55:06.973+0000] {logging_mixin.py:151} INFO - [2024-07-24T03:55:06.972+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T03:55:07.006+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T03:55:07.006+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T03:55:07.496+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T03:55:07.521+0000] {logging_mixin.py:151} INFO - [2024-07-24T03:55:07.520+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T03:55:07.545+0000] {logging_mixin.py:151} INFO - [2024-07-24T03:55:07.545+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T03:55:07.562+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.594 seconds
[2024-07-24T03:55:37.741+0000] {processor.py:157} INFO - Started process (PID=131) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T03:55:37.742+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T03:55:37.745+0000] {logging_mixin.py:151} INFO - [2024-07-24T03:55:37.745+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T03:55:37.785+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T03:55:37.785+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T03:55:38.343+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T03:55:38.366+0000] {logging_mixin.py:151} INFO - [2024-07-24T03:55:38.365+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T03:55:38.392+0000] {logging_mixin.py:151} INFO - [2024-07-24T03:55:38.392+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T03:55:38.414+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.677 seconds
[2024-07-24T03:56:08.597+0000] {processor.py:157} INFO - Started process (PID=133) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T03:56:08.598+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T03:56:08.600+0000] {logging_mixin.py:151} INFO - [2024-07-24T03:56:08.600+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T03:56:08.629+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T03:56:08.629+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T03:56:09.052+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T03:56:09.077+0000] {logging_mixin.py:151} INFO - [2024-07-24T03:56:09.076+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T03:56:09.105+0000] {logging_mixin.py:151} INFO - [2024-07-24T03:56:09.104+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T03:56:09.361+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.767 seconds
[2024-07-24T03:56:39.541+0000] {processor.py:157} INFO - Started process (PID=135) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T03:56:39.542+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T03:56:39.544+0000] {logging_mixin.py:151} INFO - [2024-07-24T03:56:39.544+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T03:56:39.577+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T03:56:39.578+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T03:56:40.066+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T03:56:40.089+0000] {logging_mixin.py:151} INFO - [2024-07-24T03:56:40.089+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T03:56:40.114+0000] {logging_mixin.py:151} INFO - [2024-07-24T03:56:40.114+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T03:56:40.133+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.595 seconds
[2024-07-24T03:57:10.211+0000] {processor.py:157} INFO - Started process (PID=137) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T03:57:10.213+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T03:57:10.215+0000] {logging_mixin.py:151} INFO - [2024-07-24T03:57:10.215+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T03:57:10.249+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T03:57:10.250+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T03:57:10.721+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T03:57:10.743+0000] {logging_mixin.py:151} INFO - [2024-07-24T03:57:10.743+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T03:57:10.765+0000] {logging_mixin.py:151} INFO - [2024-07-24T03:57:10.765+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T03:57:10.783+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.575 seconds
[2024-07-24T03:57:40.967+0000] {processor.py:157} INFO - Started process (PID=139) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T03:57:40.969+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T03:57:40.974+0000] {logging_mixin.py:151} INFO - [2024-07-24T03:57:40.974+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T03:57:41.010+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T03:57:41.011+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T03:57:41.700+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T03:57:41.725+0000] {logging_mixin.py:151} INFO - [2024-07-24T03:57:41.724+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T03:57:41.749+0000] {logging_mixin.py:151} INFO - [2024-07-24T03:57:41.749+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T03:57:41.768+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.809 seconds
[2024-07-24T03:58:11.940+0000] {processor.py:157} INFO - Started process (PID=141) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T03:58:11.941+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T03:58:11.943+0000] {logging_mixin.py:151} INFO - [2024-07-24T03:58:11.943+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T03:58:11.981+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T03:58:11.982+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T03:58:12.458+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T03:58:12.482+0000] {logging_mixin.py:151} INFO - [2024-07-24T03:58:12.481+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T03:58:12.503+0000] {logging_mixin.py:151} INFO - [2024-07-24T03:58:12.502+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T03:58:12.519+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.583 seconds
[2024-07-24T03:58:42.698+0000] {processor.py:157} INFO - Started process (PID=143) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T03:58:42.699+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T03:58:42.701+0000] {logging_mixin.py:151} INFO - [2024-07-24T03:58:42.700+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T03:58:42.733+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T03:58:42.734+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T03:58:43.204+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T03:58:43.228+0000] {logging_mixin.py:151} INFO - [2024-07-24T03:58:43.228+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T03:58:43.249+0000] {logging_mixin.py:151} INFO - [2024-07-24T03:58:43.249+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T03:58:43.272+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.577 seconds
[2024-07-24T03:59:13.436+0000] {processor.py:157} INFO - Started process (PID=145) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T03:59:13.437+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T03:59:13.439+0000] {logging_mixin.py:151} INFO - [2024-07-24T03:59:13.439+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T03:59:13.468+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T03:59:13.469+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T03:59:13.955+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T03:59:13.987+0000] {logging_mixin.py:151} INFO - [2024-07-24T03:59:13.987+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T03:59:14.025+0000] {logging_mixin.py:151} INFO - [2024-07-24T03:59:14.024+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T03:59:14.052+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.619 seconds
[2024-07-24T03:59:44.239+0000] {processor.py:157} INFO - Started process (PID=147) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T03:59:44.240+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T03:59:44.243+0000] {logging_mixin.py:151} INFO - [2024-07-24T03:59:44.242+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T03:59:44.274+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T03:59:44.275+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T03:59:44.754+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T03:59:44.781+0000] {logging_mixin.py:151} INFO - [2024-07-24T03:59:44.780+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T03:59:44.815+0000] {logging_mixin.py:151} INFO - [2024-07-24T03:59:44.815+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T03:59:44.837+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.601 seconds
[2024-07-24T04:00:15.010+0000] {processor.py:157} INFO - Started process (PID=149) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:00:15.011+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T04:00:15.013+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:00:15.013+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:00:15.045+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T04:00:15.045+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T04:00:15.493+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:00:15.518+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:00:15.518+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T04:00:15.546+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:00:15.545+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T04:00:15.567+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.560 seconds
[2024-07-24T04:00:45.750+0000] {processor.py:157} INFO - Started process (PID=151) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:00:45.751+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T04:00:45.753+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:00:45.753+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:00:45.788+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T04:00:45.788+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T04:00:46.297+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:00:46.322+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:00:46.322+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T04:00:46.347+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:00:46.346+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T04:00:46.366+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.620 seconds
[2024-07-24T04:01:16.548+0000] {processor.py:157} INFO - Started process (PID=153) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:01:16.549+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T04:01:16.552+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:01:16.551+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:01:16.590+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T04:01:16.591+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T04:01:17.094+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:01:17.118+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:01:17.117+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T04:01:17.141+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:01:17.141+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T04:01:17.163+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.618 seconds
[2024-07-24T04:01:47.393+0000] {processor.py:157} INFO - Started process (PID=155) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:01:47.394+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T04:01:47.397+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:01:47.397+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:01:47.435+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T04:01:47.435+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T04:01:47.920+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:01:47.951+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:01:47.951+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T04:01:48.253+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:01:48.253+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T04:01:48.271+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.881 seconds
[2024-07-24T04:02:18.464+0000] {processor.py:157} INFO - Started process (PID=157) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:02:18.466+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T04:02:18.469+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:02:18.468+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:02:18.503+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T04:02:18.504+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T04:02:18.982+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:02:19.011+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:02:19.011+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T04:02:19.079+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:02:19.078+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T04:02:19.155+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.693 seconds
[2024-07-24T04:02:49.369+0000] {processor.py:157} INFO - Started process (PID=159) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:02:49.371+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T04:02:49.372+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:02:49.372+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:02:49.432+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T04:02:49.433+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T04:02:49.904+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:02:49.926+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:02:49.926+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T04:02:49.948+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:02:49.948+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T04:02:49.966+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.600 seconds
[2024-07-24T04:03:20.167+0000] {processor.py:157} INFO - Started process (PID=161) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:03:20.169+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T04:03:20.171+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:03:20.170+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:03:20.210+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T04:03:20.210+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T04:03:20.842+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:03:20.877+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:03:20.877+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T04:03:20.909+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:03:20.908+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T04:03:20.931+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.767 seconds
[2024-07-24T04:03:51.137+0000] {processor.py:157} INFO - Started process (PID=163) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:03:51.138+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T04:03:51.141+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:03:51.141+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:03:51.176+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T04:03:51.177+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T04:03:51.680+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:03:51.707+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:03:51.707+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T04:03:51.738+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:03:51.737+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T04:03:51.765+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.632 seconds
[2024-07-24T04:04:21.872+0000] {processor.py:157} INFO - Started process (PID=165) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:04:21.874+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T04:04:21.876+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:04:21.876+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:04:21.912+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T04:04:21.913+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T04:04:22.806+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:04:22.850+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:04:22.849+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T04:04:22.898+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:04:22.898+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T04:04:22.935+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 1.066 seconds
[2024-07-24T04:04:53.136+0000] {processor.py:157} INFO - Started process (PID=167) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:04:53.137+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T04:04:53.139+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:04:53.139+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:04:53.174+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T04:04:53.175+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T04:04:53.698+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:04:53.731+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:04:53.730+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T04:04:54.406+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:04:54.406+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T04:04:54.434+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 1.301 seconds
[2024-07-24T04:05:24.619+0000] {processor.py:157} INFO - Started process (PID=169) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:05:24.620+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T04:05:24.623+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:05:24.623+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:05:24.655+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T04:05:24.656+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T04:05:25.192+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:05:25.218+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:05:25.217+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T04:05:25.245+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:05:25.245+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T04:05:25.264+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.648 seconds
[2024-07-24T04:05:55.455+0000] {processor.py:157} INFO - Started process (PID=171) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:05:55.456+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T04:05:55.460+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:05:55.460+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:05:55.499+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T04:05:55.499+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T04:05:55.998+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:05:56.023+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:05:56.023+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T04:05:56.049+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:05:56.049+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T04:05:56.068+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.617 seconds
[2024-07-24T04:06:26.297+0000] {processor.py:157} INFO - Started process (PID=173) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:06:26.298+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T04:06:26.300+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:06:26.300+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:06:26.338+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T04:06:26.338+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T04:06:26.834+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:06:26.856+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:06:26.856+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T04:06:26.877+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:06:26.876+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T04:06:26.892+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.600 seconds
[2024-07-24T04:06:57.081+0000] {processor.py:157} INFO - Started process (PID=175) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:06:57.082+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T04:06:57.084+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:06:57.084+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:06:57.138+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T04:06:57.139+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T04:06:57.606+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:06:57.630+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:06:57.629+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T04:06:57.653+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:06:57.652+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T04:06:57.671+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.594 seconds
[2024-07-24T04:07:27.865+0000] {processor.py:157} INFO - Started process (PID=177) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:07:27.866+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T04:07:27.868+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:07:27.868+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:07:27.902+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T04:07:27.903+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T04:07:28.475+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:07:28.502+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:07:28.501+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T04:07:28.531+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:07:28.530+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T04:07:29.201+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 1.340 seconds
[2024-07-24T04:07:59.414+0000] {processor.py:157} INFO - Started process (PID=179) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:07:59.415+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T04:07:59.417+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:07:59.417+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:07:59.454+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T04:07:59.454+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T04:08:00.338+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:08:00.369+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:08:00.368+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T04:08:00.397+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:08:00.397+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T04:08:00.420+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 1.012 seconds
[2024-07-24T04:08:30.578+0000] {processor.py:157} INFO - Started process (PID=181) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:08:30.579+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T04:08:30.581+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:08:30.581+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:08:30.615+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T04:08:30.615+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T04:08:31.061+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:08:31.086+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:08:31.085+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T04:08:31.109+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:08:31.108+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T04:08:31.127+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.552 seconds
[2024-07-24T04:09:01.307+0000] {processor.py:157} INFO - Started process (PID=183) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:09:01.308+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T04:09:01.311+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:09:01.310+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:09:01.342+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T04:09:01.343+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T04:09:01.824+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:09:01.851+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:09:01.850+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T04:09:01.875+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:09:01.875+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T04:09:01.892+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.588 seconds
[2024-07-24T04:09:32.071+0000] {processor.py:157} INFO - Started process (PID=185) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:09:32.073+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T04:09:32.085+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:09:32.084+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:09:32.131+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T04:09:32.132+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T04:09:32.823+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:09:32.846+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:09:32.846+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T04:09:32.868+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:09:32.868+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T04:09:32.884+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.818 seconds
[2024-07-24T04:10:03.058+0000] {processor.py:157} INFO - Started process (PID=187) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:10:03.059+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T04:10:03.068+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:10:03.068+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:10:03.103+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T04:10:03.104+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T04:10:03.757+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:10:03.783+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:10:03.782+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T04:10:03.805+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:10:03.804+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T04:10:03.825+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.770 seconds
[2024-07-24T04:10:34.011+0000] {processor.py:157} INFO - Started process (PID=189) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:10:34.012+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T04:10:34.014+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:10:34.013+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:10:34.050+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T04:10:34.050+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T04:10:34.502+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:10:34.529+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:10:34.528+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T04:10:35.348+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:10:35.348+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T04:10:35.379+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 1.373 seconds
[2024-07-24T04:11:05.549+0000] {processor.py:157} INFO - Started process (PID=191) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:11:05.550+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T04:11:05.553+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:11:05.552+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:11:05.590+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T04:11:05.591+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T04:11:06.087+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:11:06.118+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:11:06.118+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T04:11:06.148+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:11:06.148+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T04:11:06.173+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.628 seconds
[2024-07-24T04:11:36.414+0000] {processor.py:157} INFO - Started process (PID=193) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:11:36.415+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T04:11:36.417+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:11:36.417+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:11:36.451+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T04:11:36.451+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T04:11:36.937+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:11:36.960+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:11:36.959+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T04:11:36.983+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:11:36.982+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T04:11:37.000+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.589 seconds
[2024-07-24T04:12:07.186+0000] {processor.py:157} INFO - Started process (PID=195) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:12:07.188+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T04:12:07.189+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:12:07.189+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:12:07.224+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T04:12:07.224+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T04:12:07.691+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:12:07.713+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:12:07.712+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T04:12:07.737+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:12:07.736+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T04:12:07.756+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.572 seconds
[2024-07-24T04:12:37.944+0000] {processor.py:157} INFO - Started process (PID=197) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:12:37.945+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T04:12:37.947+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:12:37.947+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:12:37.980+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T04:12:37.981+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T04:12:38.428+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:12:38.451+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:12:38.451+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T04:12:38.470+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:12:38.470+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T04:12:38.486+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.545 seconds
[2024-07-24T04:13:08.660+0000] {processor.py:157} INFO - Started process (PID=199) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:13:08.661+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T04:13:08.664+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:13:08.663+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:13:08.710+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T04:13:08.711+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T04:13:09.218+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:13:09.243+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:13:09.242+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T04:13:09.269+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:13:09.268+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T04:13:10.004+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 1.351 seconds
[2024-07-24T04:13:40.241+0000] {processor.py:157} INFO - Started process (PID=201) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:13:40.243+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T04:13:40.246+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:13:40.245+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:13:40.291+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T04:13:40.292+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T04:13:40.915+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:13:40.948+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:13:40.948+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T04:13:41.426+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:13:41.425+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T04:13:41.452+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 1.217 seconds
[2024-07-24T04:14:11.651+0000] {processor.py:157} INFO - Started process (PID=203) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:14:11.653+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T04:14:11.657+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:14:11.657+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:14:11.708+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T04:14:11.709+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T04:14:12.566+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:14:12.592+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:14:12.591+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T04:14:12.621+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:14:12.621+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T04:14:12.643+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.996 seconds
[2024-07-24T04:14:42.854+0000] {processor.py:157} INFO - Started process (PID=205) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:14:42.855+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T04:14:42.858+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:14:42.858+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:14:42.899+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T04:14:42.900+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T04:14:43.457+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:14:43.493+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:14:43.492+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T04:14:43.523+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:14:43.522+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T04:14:43.544+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.693 seconds
[2024-07-24T04:15:13.762+0000] {processor.py:157} INFO - Started process (PID=207) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:15:13.764+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T04:15:13.770+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:15:13.769+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:15:13.815+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T04:15:13.816+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T04:15:14.505+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:15:14.532+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:15:14.532+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T04:15:14.560+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:15:14.559+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T04:15:14.580+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.824 seconds
[2024-07-24T04:15:44.800+0000] {processor.py:157} INFO - Started process (PID=209) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:15:44.801+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T04:15:44.804+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:15:44.804+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:15:44.879+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T04:15:44.879+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T04:15:45.877+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:15:45.902+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:15:45.901+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T04:15:45.926+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:15:45.926+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T04:15:45.947+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 1.153 seconds
[2024-07-24T04:16:16.128+0000] {processor.py:157} INFO - Started process (PID=211) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:16:16.130+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T04:16:16.132+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:16:16.131+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:16:16.167+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T04:16:16.167+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T04:16:16.661+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:16:16.684+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:16:16.684+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T04:16:17.184+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:16:17.183+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T04:16:17.208+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 1.083 seconds
[2024-07-24T04:16:47.319+0000] {processor.py:157} INFO - Started process (PID=213) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:16:47.320+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T04:16:47.323+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:16:47.323+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:16:47.356+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T04:16:47.356+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T04:16:47.831+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:16:47.852+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:16:47.851+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T04:16:47.874+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:16:47.873+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T04:16:47.891+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.575 seconds
[2024-07-24T04:17:18.066+0000] {processor.py:157} INFO - Started process (PID=215) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:17:18.067+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T04:17:18.069+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:17:18.069+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:17:18.110+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T04:17:18.111+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T04:17:18.827+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:17:18.852+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:17:18.851+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T04:17:18.874+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:17:18.873+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T04:17:18.893+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.830 seconds
[2024-07-24T04:17:49.069+0000] {processor.py:157} INFO - Started process (PID=217) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:17:49.070+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T04:17:49.073+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:17:49.072+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:17:49.106+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T04:17:49.107+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T04:17:49.534+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:17:49.555+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:17:49.554+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T04:17:49.577+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:17:49.577+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T04:17:49.594+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.529 seconds
[2024-07-24T04:18:19.772+0000] {processor.py:157} INFO - Started process (PID=219) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:18:19.773+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T04:18:19.775+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:18:19.775+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:18:19.813+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T04:18:19.813+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T04:18:20.354+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:18:20.380+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:18:20.380+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T04:18:20.402+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:18:20.402+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T04:18:20.420+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.653 seconds
[2024-07-24T04:18:50.592+0000] {processor.py:157} INFO - Started process (PID=221) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:18:50.593+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T04:18:50.595+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:18:50.595+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:18:50.631+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T04:18:50.631+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T04:18:51.138+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:18:51.163+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:18:51.162+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T04:18:51.187+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:18:51.187+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T04:18:51.825+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 1.237 seconds
[2024-07-24T04:19:21.999+0000] {processor.py:157} INFO - Started process (PID=223) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:19:22.000+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T04:19:22.002+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:19:22.002+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:19:22.039+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T04:19:22.040+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T04:19:22.572+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:19:22.598+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:19:22.597+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T04:19:22.897+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:19:22.897+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T04:19:22.918+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.923 seconds
[2024-07-24T04:19:53.099+0000] {processor.py:157} INFO - Started process (PID=225) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:19:53.100+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T04:19:53.103+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:19:53.103+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:19:53.140+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T04:19:53.140+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T04:19:53.612+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:19:53.638+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:19:53.637+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T04:19:53.662+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:19:53.662+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T04:19:53.680+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.584 seconds
[2024-07-24T04:20:23.858+0000] {processor.py:157} INFO - Started process (PID=227) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:20:23.858+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T04:20:23.861+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:20:23.861+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:20:23.890+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T04:20:23.891+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T04:20:24.328+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:20:24.355+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:20:24.355+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T04:20:24.382+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:20:24.381+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T04:20:24.399+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.544 seconds
[2024-07-24T04:20:54.563+0000] {processor.py:157} INFO - Started process (PID=229) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:20:54.564+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T04:20:54.567+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:20:54.566+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:20:54.595+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T04:20:54.596+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T04:20:55.041+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:20:55.065+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:20:55.064+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T04:20:55.088+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:20:55.088+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T04:20:55.104+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.544 seconds
[2024-07-24T04:21:25.285+0000] {processor.py:157} INFO - Started process (PID=231) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:21:25.286+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T04:21:25.289+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:21:25.288+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:21:25.323+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T04:21:25.323+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T04:21:25.793+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:21:25.814+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:21:25.814+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T04:21:25.836+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:21:25.835+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T04:21:25.853+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.571 seconds
[2024-07-24T04:21:55.890+0000] {processor.py:157} INFO - Started process (PID=233) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:21:55.892+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T04:21:55.894+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:21:55.893+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:21:55.928+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T04:21:55.928+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T04:21:56.400+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:21:56.424+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:21:56.423+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T04:21:56.818+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:21:56.818+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T04:21:56.847+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.960 seconds
[2024-07-24T04:22:27.033+0000] {processor.py:157} INFO - Started process (PID=235) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:22:27.034+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T04:22:27.036+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:22:27.035+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:22:27.068+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T04:22:27.068+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T04:22:27.567+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:22:27.589+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:22:27.588+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T04:22:27.612+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:22:27.612+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T04:22:27.629+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.599 seconds
[2024-07-24T04:22:57.805+0000] {processor.py:157} INFO - Started process (PID=237) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:22:57.806+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T04:22:57.808+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:22:57.808+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:22:57.839+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T04:22:57.839+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T04:22:58.265+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:22:58.287+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:22:58.287+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T04:22:58.308+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:22:58.308+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T04:22:58.324+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.522 seconds
[2024-07-24T04:23:28.437+0000] {processor.py:157} INFO - Started process (PID=239) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:23:28.438+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T04:23:28.440+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:23:28.440+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:23:28.471+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T04:23:28.472+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T04:23:28.886+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:23:28.908+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:23:28.907+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T04:23:28.929+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:23:28.929+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T04:23:28.945+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.510 seconds
[2024-07-24T04:23:59.116+0000] {processor.py:157} INFO - Started process (PID=241) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:23:59.117+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T04:23:59.120+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:23:59.120+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:23:59.153+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T04:23:59.153+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T04:23:59.589+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:23:59.610+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:23:59.610+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T04:23:59.632+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:23:59.632+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T04:23:59.654+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.542 seconds
[2024-07-24T04:24:29.769+0000] {processor.py:157} INFO - Started process (PID=243) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:24:29.770+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T04:24:29.773+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:24:29.772+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:24:29.803+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T04:24:29.804+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T04:24:30.237+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:24:30.259+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:24:30.258+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T04:24:30.283+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:24:30.283+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T04:24:30.540+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.775 seconds
[2024-07-24T04:25:00.718+0000] {processor.py:157} INFO - Started process (PID=245) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:25:00.719+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T04:25:00.721+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:25:00.721+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:25:00.752+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T04:25:00.753+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T04:25:01.262+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:25:01.288+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:25:01.287+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T04:25:01.476+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:25:01.475+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T04:25:01.493+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.777 seconds
[2024-07-24T04:25:31.680+0000] {processor.py:157} INFO - Started process (PID=247) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:25:31.681+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T04:25:31.684+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:25:31.683+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:25:31.721+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T04:25:31.721+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T04:25:32.188+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:25:32.214+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:25:32.213+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T04:25:32.239+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:25:32.239+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T04:25:32.259+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.582 seconds
[2024-07-24T04:26:02.422+0000] {processor.py:157} INFO - Started process (PID=249) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:26:02.423+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T04:26:02.425+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:26:02.424+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:26:02.453+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T04:26:02.454+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T04:26:02.881+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:26:02.923+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:26:02.923+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T04:26:02.950+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:26:02.950+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T04:26:02.970+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.551 seconds
[2024-07-24T04:26:33.183+0000] {processor.py:157} INFO - Started process (PID=251) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:26:33.184+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T04:26:33.187+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:26:33.186+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:26:33.218+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T04:26:33.218+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T04:26:33.658+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:26:33.711+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:26:33.710+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T04:26:33.731+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:26:33.731+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T04:26:33.745+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.565 seconds
[2024-07-24T04:27:03.904+0000] {processor.py:157} INFO - Started process (PID=253) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:27:03.905+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T04:27:03.908+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:27:03.907+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:27:03.939+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T04:27:03.939+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T04:27:04.403+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:27:04.438+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:27:04.437+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T04:27:04.464+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:27:04.463+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T04:27:04.485+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.584 seconds
[2024-07-24T04:27:34.671+0000] {processor.py:157} INFO - Started process (PID=255) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:27:34.672+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T04:27:34.674+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:27:34.674+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:27:34.710+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T04:27:34.711+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T04:27:35.198+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:27:35.233+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:27:35.233+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T04:27:35.505+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:27:35.505+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T04:27:35.532+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.865 seconds
[2024-07-24T04:28:05.711+0000] {processor.py:157} INFO - Started process (PID=257) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:28:05.712+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T04:28:05.714+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:28:05.714+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:28:05.747+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T04:28:05.747+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T04:28:06.221+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:28:06.255+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:28:06.254+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T04:28:06.516+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:28:06.516+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T04:28:06.551+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.843 seconds
[2024-07-24T04:28:36.733+0000] {processor.py:157} INFO - Started process (PID=259) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:28:36.735+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T04:28:36.737+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:28:36.736+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:28:36.773+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T04:28:36.773+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T04:28:37.248+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:28:37.277+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:28:37.277+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T04:28:37.301+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:28:37.300+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T04:28:37.320+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.591 seconds
[2024-07-24T04:29:07.529+0000] {processor.py:157} INFO - Started process (PID=261) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:29:07.531+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T04:29:07.538+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:29:07.537+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:29:07.579+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T04:29:07.580+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T04:29:08.392+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:29:08.466+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:29:08.465+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T04:29:08.533+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:29:08.533+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T04:29:08.598+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 1.081 seconds
[2024-07-24T04:29:38.679+0000] {processor.py:157} INFO - Started process (PID=263) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:29:38.680+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T04:29:38.683+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:29:38.682+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:29:38.715+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T04:29:38.716+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T04:29:39.181+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:29:39.221+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:29:39.220+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T04:29:39.249+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:29:39.249+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T04:29:39.269+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.593 seconds
[2024-07-24T04:30:09.430+0000] {processor.py:157} INFO - Started process (PID=265) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:30:09.431+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T04:30:09.433+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:30:09.433+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:30:09.461+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T04:30:09.461+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T04:30:09.909+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:30:09.931+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:30:09.930+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T04:30:09.956+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:30:09.956+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T04:30:10.434+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 1.007 seconds
[2024-07-24T04:30:40.607+0000] {processor.py:157} INFO - Started process (PID=267) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:30:40.607+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T04:30:40.610+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:30:40.610+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:30:40.641+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T04:30:40.641+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T04:30:41.123+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:30:41.175+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:30:41.174+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T04:30:41.445+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:30:41.445+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T04:30:41.465+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.861 seconds
[2024-07-24T04:31:11.660+0000] {processor.py:157} INFO - Started process (PID=269) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:31:11.661+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T04:31:11.662+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:31:11.662+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:31:11.701+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T04:31:11.701+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T04:31:12.173+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T04:31:12.204+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:31:12.203+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T04:31:12.230+0000] {logging_mixin.py:151} INFO - [2024-07-24T04:31:12.230+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T04:31:12.250+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.594 seconds
[2024-07-24T16:16:24.196+0000] {processor.py:157} INFO - Started process (PID=271) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:16:24.240+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T16:16:24.286+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:16:24.278+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:16:25.039+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T16:16:25.040+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T16:16:30.902+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:16:31.376+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:16:31.372+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T16:16:31.666+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:16:31.665+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T16:16:31.934+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 8.164 seconds
[2024-07-24T16:17:02.367+0000] {processor.py:157} INFO - Started process (PID=273) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:17:02.370+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T16:17:02.403+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:17:02.398+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:17:02.486+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T16:17:02.487+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T16:17:03.760+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:17:03.820+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:17:03.820+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T16:17:03.858+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:17:03.858+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T16:17:03.883+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 1.547 seconds
[2024-07-24T16:17:34.068+0000] {processor.py:157} INFO - Started process (PID=275) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:17:34.069+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T16:17:34.075+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:17:34.075+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:17:34.106+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T16:17:34.107+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T16:17:34.531+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:17:34.555+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:17:34.554+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T16:17:34.574+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:17:34.574+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T16:17:34.590+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.525 seconds
[2024-07-24T16:18:04.791+0000] {processor.py:157} INFO - Started process (PID=277) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:18:04.793+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T16:18:04.802+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:18:04.801+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:18:04.850+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T16:18:04.850+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T16:18:06.376+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:18:06.528+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:18:06.526+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T16:18:07.539+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:18:07.539+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T16:18:07.555+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 2.777 seconds
[2024-07-24T16:18:37.744+0000] {processor.py:157} INFO - Started process (PID=279) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:18:37.745+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T16:18:37.748+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:18:37.747+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:18:37.781+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T16:18:37.781+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T16:18:38.273+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:18:38.302+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:18:38.301+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T16:18:38.511+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:18:38.511+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T16:18:38.529+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.790 seconds
[2024-07-24T16:19:08.709+0000] {processor.py:157} INFO - Started process (PID=281) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:19:08.709+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T16:19:08.711+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:19:08.711+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:19:08.745+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T16:19:08.746+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T16:19:09.208+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:19:09.234+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:19:09.234+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T16:19:09.257+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:19:09.257+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T16:19:09.276+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.571 seconds
[2024-07-24T16:19:39.456+0000] {processor.py:157} INFO - Started process (PID=283) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:19:39.457+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T16:19:39.459+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:19:39.458+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:19:39.492+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T16:19:39.492+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T16:19:39.909+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:19:39.931+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:19:39.931+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T16:19:39.950+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:19:39.950+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T16:19:39.965+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.512 seconds
[2024-07-24T16:20:10.147+0000] {processor.py:157} INFO - Started process (PID=285) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:20:10.148+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T16:20:10.150+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:20:10.150+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:20:10.184+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T16:20:10.184+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T16:20:10.641+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:20:10.668+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:20:10.667+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T16:20:10.693+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:20:10.693+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T16:20:10.710+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.566 seconds
[2024-07-24T16:20:40.761+0000] {processor.py:157} INFO - Started process (PID=287) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:20:40.762+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T16:20:40.765+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:20:40.764+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:20:40.803+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T16:20:40.804+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T16:20:41.276+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:20:41.301+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:20:41.301+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T16:20:41.326+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:20:41.325+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T16:20:41.346+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.588 seconds
[2024-07-24T16:21:11.508+0000] {processor.py:157} INFO - Started process (PID=289) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:21:11.509+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T16:21:11.512+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:21:11.512+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:21:11.549+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T16:21:11.550+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T16:21:11.997+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:21:12.021+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:21:12.020+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T16:21:12.224+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:21:12.223+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T16:21:12.239+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.734 seconds
[2024-07-24T16:21:42.404+0000] {processor.py:157} INFO - Started process (PID=291) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:21:42.406+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T16:21:42.408+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:21:42.408+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:21:42.447+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T16:21:42.448+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T16:21:42.878+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:21:42.914+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:21:42.913+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T16:21:42.935+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:21:42.934+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T16:21:42.950+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.550 seconds
[2024-07-24T16:22:13.128+0000] {processor.py:157} INFO - Started process (PID=293) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:22:13.129+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T16:22:13.131+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:22:13.131+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:22:13.165+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T16:22:13.165+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T16:22:13.585+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:22:13.607+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:22:13.606+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T16:22:13.625+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:22:13.625+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T16:22:13.640+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.514 seconds
[2024-07-24T16:22:43.706+0000] {processor.py:157} INFO - Started process (PID=295) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:22:43.707+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T16:22:43.708+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:22:43.708+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:22:43.742+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T16:22:43.743+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T16:22:44.204+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:22:44.226+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:22:44.225+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T16:22:44.247+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:22:44.247+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T16:22:44.264+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.562 seconds
[2024-07-24T16:23:14.439+0000] {processor.py:157} INFO - Started process (PID=297) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:23:14.440+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T16:23:14.442+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:23:14.442+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:23:14.473+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T16:23:14.474+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T16:23:14.915+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:23:14.938+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:23:14.937+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T16:23:14.959+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:23:14.959+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T16:23:14.976+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.540 seconds
[2024-07-24T16:23:45.103+0000] {processor.py:157} INFO - Started process (PID=299) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:23:45.105+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T16:23:45.107+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:23:45.107+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:23:45.140+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T16:23:45.141+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T16:23:45.572+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:23:45.595+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:23:45.595+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T16:23:45.624+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:23:45.624+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T16:23:45.796+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.696 seconds
[2024-07-24T16:24:16.050+0000] {processor.py:157} INFO - Started process (PID=301) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:24:16.057+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T16:24:16.077+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:24:16.075+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:24:16.118+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T16:24:16.118+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T16:24:16.678+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:24:16.701+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:24:16.700+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T16:24:16.898+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:24:16.898+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T16:24:16.921+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.941 seconds
[2024-07-24T16:24:47.114+0000] {processor.py:157} INFO - Started process (PID=303) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:24:47.116+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T16:24:47.118+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:24:47.118+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:24:47.156+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T16:24:47.157+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T16:24:47.706+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:24:47.734+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:24:47.733+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T16:24:47.758+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:24:47.758+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T16:24:47.783+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.672 seconds
[2024-07-24T16:25:18.026+0000] {processor.py:157} INFO - Started process (PID=305) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:25:18.028+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T16:25:18.040+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:25:18.040+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:25:18.134+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T16:25:18.135+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T16:25:18.836+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:25:18.865+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:25:18.865+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T16:25:18.899+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:25:18.898+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T16:25:18.922+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.901 seconds
[2024-07-24T16:25:49.123+0000] {processor.py:157} INFO - Started process (PID=307) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:25:49.124+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T16:25:49.126+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:25:49.126+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:25:49.164+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T16:25:49.165+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T16:25:49.698+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:25:49.740+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:25:49.739+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T16:25:49.774+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:25:49.774+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T16:25:49.798+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.679 seconds
[2024-07-24T16:26:19.992+0000] {processor.py:157} INFO - Started process (PID=309) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:26:19.993+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T16:26:19.996+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:26:19.995+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:26:20.028+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T16:26:20.029+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T16:26:20.464+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:26:20.494+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:26:20.493+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T16:26:20.528+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:26:20.527+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T16:26:20.557+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.572 seconds
[2024-07-24T16:26:50.728+0000] {processor.py:157} INFO - Started process (PID=311) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:26:50.729+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T16:26:50.731+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:26:50.731+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:26:50.763+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T16:26:50.763+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T16:26:51.210+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:26:51.234+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:26:51.233+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T16:26:51.701+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:26:51.701+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T16:26:51.737+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 1.014 seconds
[2024-07-24T16:27:21.909+0000] {processor.py:157} INFO - Started process (PID=313) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:27:21.914+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T16:27:21.918+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:27:21.918+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:27:21.977+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T16:27:21.978+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T16:27:22.465+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:27:22.504+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:27:22.503+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T16:27:22.685+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:27:22.685+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T16:27:22.703+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.804 seconds
[2024-07-24T16:27:52.882+0000] {processor.py:157} INFO - Started process (PID=315) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:27:52.883+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T16:27:52.895+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:27:52.895+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:27:52.927+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T16:27:52.927+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T16:27:53.377+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:27:53.401+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:27:53.401+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T16:27:53.425+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:27:53.424+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T16:27:53.440+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.561 seconds
[2024-07-24T16:28:23.597+0000] {processor.py:157} INFO - Started process (PID=317) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:28:23.598+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T16:28:23.600+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:28:23.599+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:28:23.629+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T16:28:23.630+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T16:28:24.030+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:28:24.053+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:28:24.053+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T16:28:24.074+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:28:24.074+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T16:28:24.089+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.494 seconds
[2024-07-24T16:28:54.251+0000] {processor.py:157} INFO - Started process (PID=319) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:28:54.251+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T16:28:54.253+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:28:54.253+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:28:54.281+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T16:28:54.281+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T16:28:54.690+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:28:54.712+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:28:54.712+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T16:28:54.734+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:28:54.734+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T16:28:54.750+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.502 seconds
[2024-07-24T16:29:24.880+0000] {processor.py:157} INFO - Started process (PID=321) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:29:24.881+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T16:29:24.883+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:29:24.883+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:29:24.918+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T16:29:24.918+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T16:29:25.356+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:29:25.382+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:29:25.381+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T16:29:25.401+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:29:25.401+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T16:29:25.550+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.673 seconds
[2024-07-24T16:29:55.735+0000] {processor.py:157} INFO - Started process (PID=323) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:29:55.736+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T16:29:55.739+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:29:55.739+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:29:55.774+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T16:29:55.775+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T16:29:56.256+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:29:56.288+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:29:56.287+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T16:29:56.469+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:29:56.469+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T16:29:56.483+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.753 seconds
[2024-07-24T16:30:26.645+0000] {processor.py:157} INFO - Started process (PID=325) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:30:26.646+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T16:30:26.649+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:30:26.649+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:30:26.681+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T16:30:26.681+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T16:30:27.115+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:30:27.139+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:30:27.139+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T16:30:27.331+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:30:27.331+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T16:30:27.369+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.728 seconds
[2024-07-24T16:30:57.531+0000] {processor.py:157} INFO - Started process (PID=327) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:30:57.532+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T16:30:57.535+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:30:57.535+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:30:57.581+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T16:30:57.582+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T16:30:58.103+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:30:58.126+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:30:58.125+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T16:30:58.147+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:30:58.147+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T16:30:58.161+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.633 seconds
[2024-07-24T16:31:28.352+0000] {processor.py:157} INFO - Started process (PID=329) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:31:28.353+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T16:31:28.356+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:31:28.355+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:31:28.386+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T16:31:28.386+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T16:31:28.875+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:31:28.896+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:31:28.895+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T16:31:28.915+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:31:28.915+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T16:31:28.930+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.582 seconds
[2024-07-24T16:31:59.106+0000] {processor.py:157} INFO - Started process (PID=331) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:31:59.107+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T16:31:59.109+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:31:59.109+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:31:59.172+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T16:31:59.172+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T16:31:59.622+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:31:59.652+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:31:59.651+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T16:31:59.676+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:31:59.675+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T16:31:59.692+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.589 seconds
[2024-07-24T16:32:29.873+0000] {processor.py:157} INFO - Started process (PID=333) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:32:29.874+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T16:32:29.876+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:32:29.876+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:32:29.904+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T16:32:29.904+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T16:32:30.369+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:32:30.396+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:32:30.396+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T16:32:30.620+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:32:30.619+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T16:32:30.637+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.766 seconds
[2024-07-24T16:33:00.822+0000] {processor.py:157} INFO - Started process (PID=335) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:33:00.823+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T16:33:00.825+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:33:00.825+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:33:00.859+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T16:33:00.859+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T16:33:01.309+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:33:01.333+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:33:01.333+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T16:33:01.488+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:33:01.488+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T16:33:01.503+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.685 seconds
[2024-07-24T16:33:31.551+0000] {processor.py:157} INFO - Started process (PID=337) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:33:31.552+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T16:33:31.554+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:33:31.554+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:33:31.584+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T16:33:31.584+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T16:33:32.046+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:33:32.067+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:33:32.067+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T16:33:32.088+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:33:32.088+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T16:33:32.104+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.556 seconds
[2024-07-24T16:34:02.264+0000] {processor.py:157} INFO - Started process (PID=339) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:34:02.265+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T16:34:02.268+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:34:02.267+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:34:02.298+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T16:34:02.298+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T16:34:02.722+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:34:02.747+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:34:02.747+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T16:34:02.767+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:34:02.766+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T16:34:02.781+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.521 seconds
[2024-07-24T16:34:32.936+0000] {processor.py:157} INFO - Started process (PID=341) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:34:32.937+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T16:34:32.940+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:34:32.939+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:34:32.969+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T16:34:32.970+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T16:34:33.387+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:34:33.409+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:34:33.408+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T16:34:33.429+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:34:33.429+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T16:34:33.443+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.509 seconds
[2024-07-24T16:35:03.606+0000] {processor.py:157} INFO - Started process (PID=343) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:35:03.607+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T16:35:03.608+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:35:03.608+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:35:03.639+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T16:35:03.639+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T16:35:04.046+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:35:04.071+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:35:04.070+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T16:35:04.092+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:35:04.092+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T16:35:04.257+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.654 seconds
[2024-07-24T16:35:34.416+0000] {processor.py:157} INFO - Started process (PID=345) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:35:34.416+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T16:35:34.418+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:35:34.418+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:35:34.451+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T16:35:34.451+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T16:35:34.878+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:35:34.898+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:35:34.898+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T16:35:35.039+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:35:35.038+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T16:35:35.053+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.640 seconds
[2024-07-24T16:36:05.223+0000] {processor.py:157} INFO - Started process (PID=347) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:36:05.224+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T16:36:05.226+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:36:05.225+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:36:05.256+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T16:36:05.256+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T16:36:05.678+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:36:05.699+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:36:05.698+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T16:36:05.849+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:36:05.848+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T16:36:05.865+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.644 seconds
[2024-07-24T16:36:36.024+0000] {processor.py:157} INFO - Started process (PID=349) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:36:36.025+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T16:36:36.028+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:36:36.028+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:36:36.058+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T16:36:36.058+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T16:36:36.522+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:36:36.547+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:36:36.547+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T16:36:36.570+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:36:36.569+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T16:36:36.587+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.566 seconds
[2024-07-24T16:37:06.762+0000] {processor.py:157} INFO - Started process (PID=351) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:37:06.763+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T16:37:06.765+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:37:06.765+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:37:06.799+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T16:37:06.799+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T16:37:07.411+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:37:07.434+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:37:07.433+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T16:37:07.455+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:37:07.455+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T16:37:07.471+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.712 seconds
[2024-07-24T16:37:37.642+0000] {processor.py:157} INFO - Started process (PID=353) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:37:37.643+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T16:37:37.645+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:37:37.644+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:37:37.678+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T16:37:37.678+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T16:37:38.127+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:37:38.146+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:37:38.146+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T16:37:38.169+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:37:38.169+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T16:37:38.185+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.546 seconds
[2024-07-24T16:38:08.354+0000] {processor.py:157} INFO - Started process (PID=355) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:38:08.355+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T16:38:08.358+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:38:08.358+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:38:08.389+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T16:38:08.389+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T16:38:08.848+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:38:08.872+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:38:08.871+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T16:38:09.039+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:38:09.039+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T16:38:09.055+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.704 seconds
[2024-07-24T16:38:39.222+0000] {processor.py:157} INFO - Started process (PID=357) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:38:39.223+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T16:38:39.225+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:38:39.225+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:38:39.261+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T16:38:39.262+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T16:38:39.683+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:38:39.703+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:38:39.702+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T16:38:39.852+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:38:39.851+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T16:38:39.866+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.647 seconds
[2024-07-24T16:39:10.039+0000] {processor.py:157} INFO - Started process (PID=359) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:39:10.040+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T16:39:10.042+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:39:10.041+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:39:10.070+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T16:39:10.070+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T16:39:10.475+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:39:10.502+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:39:10.502+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T16:39:10.667+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:39:10.667+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T16:39:10.683+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.646 seconds
[2024-07-24T16:39:40.944+0000] {processor.py:157} INFO - Started process (PID=361) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:39:40.945+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T16:39:40.967+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:39:40.963+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:39:41.033+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T16:39:41.033+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T16:39:41.564+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:39:41.597+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:39:41.597+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T16:39:41.619+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:39:41.619+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T16:39:41.634+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.743 seconds
[2024-07-24T16:40:11.802+0000] {processor.py:157} INFO - Started process (PID=363) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:40:11.803+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T16:40:11.807+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:40:11.807+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:40:11.839+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T16:40:11.839+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T16:40:12.282+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:40:12.306+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:40:12.305+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T16:40:12.328+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:40:12.327+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T16:40:12.345+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.546 seconds
[2024-07-24T16:40:42.520+0000] {processor.py:157} INFO - Started process (PID=365) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:40:42.522+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T16:40:42.525+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:40:42.525+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:40:42.556+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T16:40:42.556+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T16:40:43.011+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:40:43.032+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:40:43.031+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T16:40:43.052+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:40:43.051+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T16:40:43.195+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.680 seconds
[2024-07-24T16:41:13.355+0000] {processor.py:157} INFO - Started process (PID=367) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:41:13.356+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T16:41:13.358+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:41:13.358+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:41:13.387+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T16:41:13.387+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T16:41:13.801+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:41:13.824+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:41:13.824+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T16:41:13.995+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:41:13.995+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T16:41:14.014+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.663 seconds
[2024-07-24T16:41:44.210+0000] {processor.py:157} INFO - Started process (PID=369) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:41:44.211+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T16:41:44.214+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:41:44.213+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:41:44.245+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T16:41:44.245+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T16:41:44.689+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:41:44.710+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:41:44.710+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T16:41:44.861+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:41:44.861+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T16:41:44.876+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.670 seconds
[2024-07-24T16:42:15.041+0000] {processor.py:157} INFO - Started process (PID=371) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:42:15.042+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T16:42:15.044+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:42:15.043+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:42:15.072+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T16:42:15.072+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T16:42:15.475+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:42:15.496+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:42:15.496+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T16:42:15.515+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:42:15.514+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T16:42:15.528+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.490 seconds
[2024-07-24T16:42:45.702+0000] {processor.py:157} INFO - Started process (PID=373) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:42:45.703+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T16:42:45.705+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:42:45.705+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:42:45.737+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T16:42:45.737+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T16:42:46.166+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:42:46.201+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:42:46.200+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T16:42:46.226+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:42:46.226+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T16:42:46.243+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.545 seconds
[2024-07-24T16:43:16.322+0000] {processor.py:157} INFO - Started process (PID=375) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:43:16.323+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T16:43:16.325+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:43:16.324+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:43:16.355+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T16:43:16.355+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T16:43:16.778+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:43:16.803+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:43:16.803+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T16:43:16.824+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:43:16.823+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T16:43:16.840+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.521 seconds
[2024-07-24T16:43:47.001+0000] {processor.py:157} INFO - Started process (PID=377) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:43:47.002+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T16:43:47.004+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:43:47.004+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:43:47.034+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T16:43:47.034+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T16:43:47.478+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:43:47.501+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:43:47.501+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T16:43:47.680+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:43:47.679+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T16:43:47.712+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.714 seconds
[2024-07-24T16:44:17.905+0000] {processor.py:157} INFO - Started process (PID=379) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:44:17.916+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T16:44:17.919+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:44:17.918+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:44:17.948+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T16:44:17.948+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T16:44:18.388+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:44:18.413+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:44:18.413+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T16:44:18.557+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:44:18.557+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T16:44:18.572+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.708 seconds
[2024-07-24T16:44:48.747+0000] {processor.py:157} INFO - Started process (PID=381) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:44:48.748+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T16:44:48.749+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:44:48.749+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:44:48.779+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T16:44:48.780+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T16:44:49.213+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:44:49.235+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:44:49.234+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T16:44:49.386+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:44:49.386+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T16:44:49.406+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.662 seconds
[2024-07-24T16:45:19.572+0000] {processor.py:157} INFO - Started process (PID=383) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:45:19.573+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T16:45:19.575+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:45:19.575+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:45:19.612+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T16:45:19.613+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T16:45:20.049+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:45:20.208+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:45:20.207+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T16:45:20.226+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:45:20.226+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T16:45:20.240+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.671 seconds
[2024-07-24T16:45:50.278+0000] {processor.py:157} INFO - Started process (PID=385) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:45:50.279+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T16:45:50.281+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:45:50.281+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:45:50.311+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T16:45:50.311+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T16:45:50.758+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:45:50.782+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:45:50.781+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T16:45:50.803+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:45:50.803+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T16:45:50.820+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.546 seconds
[2024-07-24T16:46:20.975+0000] {processor.py:157} INFO - Started process (PID=387) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:46:20.976+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T16:46:20.978+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:46:20.978+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:46:21.006+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T16:46:21.007+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T16:46:21.456+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:46:21.476+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:46:21.475+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T16:46:21.494+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:46:21.494+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T16:46:21.645+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.673 seconds
[2024-07-24T16:46:51.850+0000] {processor.py:157} INFO - Started process (PID=389) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:46:51.851+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T16:46:51.853+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:46:51.853+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:46:51.926+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T16:46:51.927+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T16:46:52.402+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:46:52.462+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:46:52.462+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T16:46:52.617+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:46:52.617+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T16:46:52.632+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.786 seconds
[2024-07-24T16:47:22.806+0000] {processor.py:157} INFO - Started process (PID=391) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:47:22.807+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T16:47:22.809+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:47:22.808+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:47:22.839+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T16:47:22.840+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T16:47:23.256+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:47:23.275+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:47:23.275+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T16:47:23.420+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:47:23.420+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T16:47:23.434+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.631 seconds
[2024-07-24T16:47:53.593+0000] {processor.py:157} INFO - Started process (PID=393) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:47:53.594+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T16:47:53.595+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:47:53.595+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:47:53.624+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T16:47:53.624+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T16:47:54.030+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:47:54.167+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:47:54.167+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T16:47:54.185+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:47:54.185+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T16:47:54.198+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.608 seconds
[2024-07-24T16:48:24.348+0000] {processor.py:157} INFO - Started process (PID=395) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:48:24.349+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T16:48:24.351+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:48:24.351+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:48:24.381+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T16:48:24.382+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T16:48:24.794+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:48:24.816+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:48:24.815+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T16:48:24.841+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:48:24.841+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T16:48:24.862+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.516 seconds
[2024-07-24T16:48:55.031+0000] {processor.py:157} INFO - Started process (PID=397) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:48:55.032+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T16:48:55.034+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:48:55.034+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:48:55.065+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T16:48:55.066+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T16:48:55.501+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:48:55.521+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:48:55.521+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T16:48:55.542+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:48:55.542+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T16:48:55.557+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.532 seconds
[2024-07-24T16:49:25.626+0000] {processor.py:157} INFO - Started process (PID=399) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:49:25.627+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T16:49:25.629+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:49:25.629+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:49:25.663+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T16:49:25.664+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T16:49:26.083+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:49:26.105+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:49:26.104+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T16:49:26.127+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:49:26.127+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T16:49:26.277+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.654 seconds
[2024-07-24T16:49:56.444+0000] {processor.py:157} INFO - Started process (PID=401) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:49:56.445+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T16:49:56.447+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:49:56.447+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:49:56.476+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T16:49:56.476+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T16:49:56.908+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:49:56.930+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:49:56.929+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T16:49:57.081+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:49:57.081+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T16:49:57.095+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.654 seconds
[2024-07-24T16:50:27.258+0000] {processor.py:157} INFO - Started process (PID=403) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:50:27.259+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T16:50:27.261+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:50:27.261+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:50:27.292+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T16:50:27.293+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T16:50:27.773+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:50:27.799+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:50:27.798+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T16:50:27.958+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:50:27.958+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T16:50:27.974+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.721 seconds
[2024-07-24T16:50:58.161+0000] {processor.py:157} INFO - Started process (PID=405) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:50:58.161+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T16:50:58.166+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:50:58.165+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:50:58.205+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T16:50:58.205+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T16:50:58.636+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:50:58.781+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:50:58.780+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T16:50:58.799+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:50:58.799+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T16:50:58.814+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.656 seconds
[2024-07-24T16:51:28.999+0000] {processor.py:157} INFO - Started process (PID=407) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:51:29.000+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T16:51:29.002+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:51:29.002+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:51:29.032+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T16:51:29.033+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T16:51:29.452+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:51:29.474+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:51:29.474+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T16:51:29.494+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:51:29.494+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T16:51:29.510+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.514 seconds
[2024-07-24T16:51:59.679+0000] {processor.py:157} INFO - Started process (PID=409) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:51:59.680+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T16:51:59.682+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:51:59.682+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:51:59.711+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T16:51:59.711+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T16:52:00.169+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:52:00.192+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:52:00.191+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T16:52:00.216+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:52:00.216+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T16:52:00.379+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.718 seconds
[2024-07-24T16:52:30.558+0000] {processor.py:157} INFO - Started process (PID=411) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:52:30.559+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T16:52:30.561+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:52:30.561+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:52:30.594+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T16:52:30.595+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T16:52:31.096+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:52:31.124+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:52:31.123+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T16:52:31.328+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:52:31.327+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T16:52:31.345+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.790 seconds
[2024-07-24T16:53:01.539+0000] {processor.py:157} INFO - Started process (PID=413) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:53:01.540+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T16:53:01.544+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:53:01.544+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:53:01.583+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T16:53:01.584+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T16:53:02.128+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:53:02.153+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:53:02.153+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T16:53:02.328+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:53:02.328+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T16:53:02.345+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.810 seconds
[2024-07-24T16:53:32.510+0000] {processor.py:157} INFO - Started process (PID=415) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:53:32.510+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T16:53:32.512+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:53:32.512+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:53:32.540+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T16:53:32.541+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T16:53:32.953+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:53:33.103+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:53:33.102+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T16:53:33.121+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:53:33.121+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T16:53:33.141+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.634 seconds
[2024-07-24T16:54:03.229+0000] {processor.py:157} INFO - Started process (PID=417) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:54:03.230+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T16:54:03.231+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:54:03.231+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:54:03.260+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T16:54:03.260+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T16:54:03.673+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:54:03.692+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:54:03.691+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T16:54:03.710+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:54:03.710+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T16:54:03.724+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.498 seconds
[2024-07-24T16:54:33.883+0000] {processor.py:157} INFO - Started process (PID=419) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:54:33.884+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T16:54:33.886+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:54:33.886+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:54:33.917+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T16:54:33.917+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T16:54:34.320+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:54:34.343+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:54:34.343+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T16:54:34.368+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:54:34.368+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T16:54:34.386+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.505 seconds
[2024-07-24T16:55:04.550+0000] {processor.py:157} INFO - Started process (PID=421) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:55:04.551+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T16:55:04.553+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:55:04.553+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:55:04.585+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T16:55:04.585+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T16:55:05.001+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:55:05.029+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:55:05.028+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T16:55:05.050+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:55:05.050+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T16:55:05.199+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.652 seconds
[2024-07-24T16:55:35.347+0000] {processor.py:157} INFO - Started process (PID=423) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:55:35.347+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T16:55:35.349+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:55:35.349+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:55:35.377+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T16:55:35.377+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T16:55:35.775+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:55:35.794+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:55:35.793+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T16:55:35.939+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:55:35.939+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T16:55:35.954+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.610 seconds
[2024-07-24T16:56:06.119+0000] {processor.py:157} INFO - Started process (PID=425) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:56:06.120+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T16:56:06.122+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:56:06.122+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:56:06.153+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T16:56:06.154+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T16:56:06.647+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:56:06.671+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:56:06.670+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T16:56:06.853+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:56:06.852+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T16:56:06.870+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.755 seconds
[2024-07-24T16:56:37.078+0000] {processor.py:157} INFO - Started process (PID=427) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:56:37.080+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T16:56:37.082+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:56:37.082+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:56:37.112+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T16:56:37.112+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T16:56:37.606+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:56:37.799+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:56:37.798+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T16:56:37.818+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:56:37.818+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T16:56:37.835+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.780 seconds
[2024-07-24T16:57:07.997+0000] {processor.py:157} INFO - Started process (PID=429) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:57:07.998+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T16:57:08.000+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:57:08.000+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:57:08.031+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T16:57:08.031+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T16:57:08.425+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:57:08.457+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:57:08.457+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T16:57:08.475+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:57:08.475+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T16:57:08.489+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.494 seconds
[2024-07-24T16:57:38.649+0000] {processor.py:157} INFO - Started process (PID=431) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:57:38.650+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T16:57:38.652+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:57:38.652+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:57:38.682+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T16:57:38.683+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T16:57:39.102+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:57:39.124+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:57:39.123+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T16:57:39.145+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:57:39.145+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T16:57:39.160+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.514 seconds
[2024-07-24T16:58:09.281+0000] {processor.py:157} INFO - Started process (PID=433) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:58:09.282+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T16:58:09.288+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:58:09.288+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:58:09.317+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T16:58:09.317+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T16:58:09.746+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:58:09.767+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:58:09.766+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T16:58:09.947+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:58:09.947+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T16:58:09.964+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.687 seconds
[2024-07-24T16:58:40.138+0000] {processor.py:157} INFO - Started process (PID=435) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:58:40.139+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T16:58:40.141+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:58:40.141+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:58:40.171+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T16:58:40.172+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T16:58:40.590+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:58:40.610+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:58:40.610+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T16:58:40.803+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:58:40.802+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T16:58:40.819+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.685 seconds
[2024-07-24T16:59:10.985+0000] {processor.py:157} INFO - Started process (PID=437) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:59:10.986+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T16:59:10.988+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:59:10.988+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:59:11.019+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T16:59:11.020+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T16:59:11.467+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:59:11.625+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:59:11.624+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T16:59:11.649+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:59:11.649+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T16:59:11.665+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.683 seconds
[2024-07-24T16:59:41.726+0000] {processor.py:157} INFO - Started process (PID=439) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:59:41.727+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T16:59:41.729+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:59:41.729+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:59:41.759+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T16:59:41.759+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T16:59:42.226+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T16:59:42.383+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:59:42.382+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T16:59:42.403+0000] {logging_mixin.py:151} INFO - [2024-07-24T16:59:42.402+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T16:59:42.419+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.695 seconds
[2024-07-24T17:00:12.592+0000] {processor.py:157} INFO - Started process (PID=441) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:00:12.593+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T17:00:12.595+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:00:12.595+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:00:12.623+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T17:00:12.624+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T17:00:13.045+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:00:13.064+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:00:13.063+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T17:00:13.084+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:00:13.084+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T17:00:13.099+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.511 seconds
[2024-07-24T17:00:43.253+0000] {processor.py:157} INFO - Started process (PID=443) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:00:43.254+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T17:00:43.256+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:00:43.256+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:00:43.285+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T17:00:43.285+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T17:00:43.691+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:00:43.715+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:00:43.714+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T17:00:43.736+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:00:43.736+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T17:00:43.890+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.639 seconds
[2024-07-24T17:01:14.057+0000] {processor.py:157} INFO - Started process (PID=445) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:01:14.058+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T17:01:14.060+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:01:14.060+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:01:14.092+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T17:01:14.092+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T17:01:14.506+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:01:14.525+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:01:14.525+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T17:01:14.685+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:01:14.685+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T17:01:14.700+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.645 seconds
[2024-07-24T17:01:44.865+0000] {processor.py:157} INFO - Started process (PID=447) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:01:44.865+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T17:01:44.868+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:01:44.867+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:01:44.895+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T17:01:44.896+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T17:01:45.298+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:01:45.316+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:01:45.316+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T17:01:45.461+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:01:45.461+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T17:01:45.485+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.623 seconds
[2024-07-24T17:02:15.654+0000] {processor.py:157} INFO - Started process (PID=449) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:02:15.655+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T17:02:15.657+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:02:15.656+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:02:15.686+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T17:02:15.686+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T17:02:16.112+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:02:16.293+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:02:16.292+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T17:02:16.316+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:02:16.315+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T17:02:16.333+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.682 seconds
[2024-07-24T17:02:46.496+0000] {processor.py:157} INFO - Started process (PID=451) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:02:46.497+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T17:02:46.499+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:02:46.499+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:02:46.529+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T17:02:46.529+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T17:02:46.956+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:02:46.976+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:02:46.976+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T17:02:46.995+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:02:46.995+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T17:02:47.012+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.518 seconds
[2024-07-24T17:03:17.182+0000] {processor.py:157} INFO - Started process (PID=453) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:03:17.183+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T17:03:17.185+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:03:17.185+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:03:17.215+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T17:03:17.216+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T17:03:17.685+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:03:17.707+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:03:17.706+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T17:03:17.727+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:03:17.727+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T17:03:17.744+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.565 seconds
[2024-07-24T17:03:47.783+0000] {processor.py:157} INFO - Started process (PID=455) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:03:47.784+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T17:03:47.785+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:03:47.785+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:03:47.813+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T17:03:47.813+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T17:03:48.220+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:03:48.239+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:03:48.239+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T17:03:48.380+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:03:48.380+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T17:03:48.396+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.616 seconds
[2024-07-24T17:04:18.555+0000] {processor.py:157} INFO - Started process (PID=457) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:04:18.556+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T17:04:18.558+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:04:18.557+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:04:18.585+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T17:04:18.585+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T17:04:18.987+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:04:19.007+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:04:19.006+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T17:04:19.144+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:04:19.144+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T17:04:19.163+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.610 seconds
[2024-07-24T17:04:49.317+0000] {processor.py:157} INFO - Started process (PID=459) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:04:49.318+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T17:04:49.320+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:04:49.320+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:04:49.352+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T17:04:49.353+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T17:04:49.840+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:04:49.866+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:04:49.866+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T17:04:50.024+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:04:50.024+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T17:04:50.041+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.727 seconds
[2024-07-24T17:05:20.210+0000] {processor.py:157} INFO - Started process (PID=461) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:05:20.211+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T17:05:20.213+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:05:20.213+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:05:20.243+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T17:05:20.243+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T17:05:20.652+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:05:20.830+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:05:20.829+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T17:05:20.864+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:05:20.864+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T17:05:20.881+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.673 seconds
[2024-07-24T17:05:51.040+0000] {processor.py:157} INFO - Started process (PID=463) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:05:51.041+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T17:05:51.043+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:05:51.042+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:05:51.071+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T17:05:51.071+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T17:05:51.481+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:05:51.500+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:05:51.500+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T17:05:51.518+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:05:51.518+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T17:05:51.533+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.496 seconds
[2024-07-24T17:06:21.694+0000] {processor.py:157} INFO - Started process (PID=465) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:06:21.695+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T17:06:21.697+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:06:21.696+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:06:21.723+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T17:06:21.724+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T17:06:22.269+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:06:22.291+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:06:22.291+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T17:06:22.315+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:06:22.314+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T17:06:22.334+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.643 seconds
[2024-07-24T17:06:52.487+0000] {processor.py:157} INFO - Started process (PID=467) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:06:52.488+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T17:06:52.489+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:06:52.489+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:06:52.519+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T17:06:52.520+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T17:06:52.931+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:06:52.950+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:06:52.949+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T17:06:53.087+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:06:53.087+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T17:06:53.100+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.616 seconds
[2024-07-24T17:07:23.260+0000] {processor.py:157} INFO - Started process (PID=469) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:07:23.261+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T17:07:23.263+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:07:23.262+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:07:23.290+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T17:07:23.291+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T17:07:23.692+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:07:23.711+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:07:23.711+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T17:07:23.875+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:07:23.875+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T17:07:23.889+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.632 seconds
[2024-07-24T17:07:54.044+0000] {processor.py:157} INFO - Started process (PID=471) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:07:54.045+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T17:07:54.047+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:07:54.047+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:07:54.075+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T17:07:54.075+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T17:07:54.469+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:07:54.617+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:07:54.617+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T17:07:54.636+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:07:54.636+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T17:07:54.650+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.608 seconds
[2024-07-24T17:08:24.812+0000] {processor.py:157} INFO - Started process (PID=473) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:08:24.813+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T17:08:24.815+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:08:24.815+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:08:24.846+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T17:08:24.847+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T17:08:25.263+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:08:25.401+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:08:25.401+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T17:08:25.418+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:08:25.418+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T17:08:25.432+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.622 seconds
[2024-07-24T17:08:55.603+0000] {processor.py:157} INFO - Started process (PID=475) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:08:55.605+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T17:08:55.606+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:08:55.606+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:08:55.636+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T17:08:55.636+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T17:08:56.106+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:08:56.130+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:08:56.129+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T17:08:56.154+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:08:56.153+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T17:08:56.171+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.571 seconds
[2024-07-24T17:09:26.345+0000] {processor.py:157} INFO - Started process (PID=477) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:09:26.346+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T17:09:26.349+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:09:26.348+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:09:26.379+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T17:09:26.379+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T17:09:26.931+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:09:26.954+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:09:26.954+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T17:09:26.980+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:09:26.980+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T17:09:26.997+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.655 seconds
[2024-07-24T17:09:57.158+0000] {processor.py:157} INFO - Started process (PID=479) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:09:57.159+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T17:09:57.161+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:09:57.161+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:09:57.193+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T17:09:57.193+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T17:09:57.617+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:09:57.639+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:09:57.639+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T17:09:57.798+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:09:57.798+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T17:09:57.814+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.659 seconds
[2024-07-24T17:10:27.985+0000] {processor.py:157} INFO - Started process (PID=481) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:10:27.986+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T17:10:27.988+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:10:27.988+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:10:28.018+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T17:10:28.019+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T17:10:28.429+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:10:28.451+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:10:28.450+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T17:10:28.591+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:10:28.590+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T17:10:28.605+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.622 seconds
[2024-07-24T17:10:58.763+0000] {processor.py:157} INFO - Started process (PID=483) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:10:58.764+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T17:10:58.780+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:10:58.780+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:10:58.814+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T17:10:58.814+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T17:10:59.338+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:10:59.525+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:10:59.524+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T17:10:59.548+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:10:59.548+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T17:10:59.565+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.804 seconds
[2024-07-24T17:11:29.717+0000] {processor.py:157} INFO - Started process (PID=485) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:11:29.718+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T17:11:29.720+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:11:29.719+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:11:29.749+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T17:11:29.750+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T17:11:30.147+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:11:30.167+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:11:30.167+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T17:11:30.186+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:11:30.186+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T17:11:30.201+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.486 seconds
[2024-07-24T17:12:00.358+0000] {processor.py:157} INFO - Started process (PID=487) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:12:00.359+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T17:12:00.361+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:12:00.361+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:12:00.390+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T17:12:00.391+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T17:12:00.864+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:12:00.885+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:12:00.884+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T17:12:00.905+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:12:00.904+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T17:12:00.921+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.566 seconds
[2024-07-24T17:12:30.997+0000] {processor.py:157} INFO - Started process (PID=489) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:12:30.998+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T17:12:31.003+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:12:31.002+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:12:31.031+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T17:12:31.032+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T17:12:31.477+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:12:31.499+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:12:31.499+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T17:12:31.654+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:12:31.654+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T17:12:31.671+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.676 seconds
[2024-07-24T17:13:01.833+0000] {processor.py:157} INFO - Started process (PID=491) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:13:01.834+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T17:13:01.836+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:13:01.836+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:13:01.865+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T17:13:01.866+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T17:13:02.304+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:13:02.324+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:13:02.323+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T17:13:02.503+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:13:02.503+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T17:13:02.544+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.713 seconds
[2024-07-24T17:13:32.736+0000] {processor.py:157} INFO - Started process (PID=493) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:13:32.737+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T17:13:32.739+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:13:32.739+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:13:32.772+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T17:13:32.772+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T17:13:33.256+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:13:33.409+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:13:33.408+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T17:13:33.428+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:13:33.428+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T17:13:33.443+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.747 seconds
[2024-07-24T17:14:03.604+0000] {processor.py:157} INFO - Started process (PID=495) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:14:03.605+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T17:14:03.608+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:14:03.608+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:14:03.642+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T17:14:03.642+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T17:14:04.094+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:14:04.248+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:14:04.247+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T17:14:04.268+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:14:04.267+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T17:14:04.288+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.688 seconds
[2024-07-24T17:14:34.452+0000] {processor.py:157} INFO - Started process (PID=497) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:14:34.453+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T17:14:34.455+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:14:34.455+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:14:34.485+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T17:14:34.485+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T17:14:34.881+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:14:34.901+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:14:34.901+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T17:14:34.920+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:14:34.919+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T17:14:34.933+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.484 seconds
[2024-07-24T17:15:05.086+0000] {processor.py:157} INFO - Started process (PID=499) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:15:05.087+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T17:15:05.089+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:15:05.088+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:15:05.118+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T17:15:05.119+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T17:15:05.646+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:15:05.667+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:15:05.666+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T17:15:05.690+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:15:05.690+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T17:15:05.712+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.629 seconds
[2024-07-24T17:15:35.883+0000] {processor.py:157} INFO - Started process (PID=501) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:15:35.884+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T17:15:35.886+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:15:35.886+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:15:35.917+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T17:15:35.917+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T17:15:36.349+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:15:36.371+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:15:36.370+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T17:15:36.524+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:15:36.524+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T17:15:36.540+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.660 seconds
[2024-07-24T17:16:06.707+0000] {processor.py:157} INFO - Started process (PID=503) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:16:06.708+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T17:16:06.711+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:16:06.711+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:16:06.742+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T17:16:06.743+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T17:16:07.251+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:16:07.276+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:16:07.275+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T17:16:07.473+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:16:07.473+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T17:16:07.493+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.789 seconds
[2024-07-24T17:16:37.538+0000] {processor.py:157} INFO - Started process (PID=505) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:16:37.539+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T17:16:37.541+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:16:37.541+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:16:37.572+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T17:16:37.573+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T17:16:38.014+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:16:38.200+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:16:38.200+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T17:16:38.228+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:16:38.228+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T17:16:38.281+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.746 seconds
[2024-07-24T17:17:08.499+0000] {processor.py:157} INFO - Started process (PID=507) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:17:08.500+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T17:17:08.502+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:17:08.502+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:17:08.535+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T17:17:08.535+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T17:17:09.010+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:17:09.041+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:17:09.040+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T17:17:09.065+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:17:09.065+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T17:17:09.082+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.617 seconds
[2024-07-24T17:17:39.252+0000] {processor.py:157} INFO - Started process (PID=509) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:17:39.253+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T17:17:39.255+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:17:39.255+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:17:39.288+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T17:17:39.288+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T17:17:39.728+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:17:39.748+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:17:39.747+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T17:17:39.770+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:17:39.769+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T17:17:39.790+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.541 seconds
[2024-07-24T17:18:09.961+0000] {processor.py:157} INFO - Started process (PID=511) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:18:09.962+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T17:18:09.964+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:18:09.964+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:18:09.998+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T17:18:09.998+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T17:18:10.467+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:18:10.495+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:18:10.494+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T17:18:10.691+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:18:10.691+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T17:18:10.713+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.756 seconds
[2024-07-24T17:18:40.886+0000] {processor.py:157} INFO - Started process (PID=513) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:18:40.889+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T17:18:40.895+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:18:40.895+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:18:40.936+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T17:18:40.937+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T17:18:41.566+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:18:41.598+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:18:41.598+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T17:18:41.777+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:18:41.777+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T17:18:41.795+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.912 seconds
[2024-07-24T17:19:11.979+0000] {processor.py:157} INFO - Started process (PID=515) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:19:11.980+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T17:19:11.982+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:19:11.982+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:19:12.020+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T17:19:12.021+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T17:19:12.706+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:19:12.736+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:19:12.736+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T17:19:12.954+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:19:12.954+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T17:19:12.980+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 1.005 seconds
[2024-07-24T17:19:43.156+0000] {processor.py:157} INFO - Started process (PID=517) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:19:43.157+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T17:19:43.160+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:19:43.159+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:19:43.193+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T17:19:43.194+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T17:19:43.712+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:19:43.889+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:19:43.888+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T17:19:43.908+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:19:43.907+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T17:19:43.923+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.770 seconds
[2024-07-24T17:20:14.110+0000] {processor.py:157} INFO - Started process (PID=519) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:20:14.111+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T17:20:14.113+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:20:14.113+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:20:14.144+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T17:20:14.144+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T17:20:14.711+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:20:14.740+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:20:14.739+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T17:20:14.771+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:20:14.770+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T17:20:14.791+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.683 seconds
[2024-07-24T17:20:44.963+0000] {processor.py:157} INFO - Started process (PID=521) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:20:44.964+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T17:20:44.967+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:20:44.966+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:20:44.999+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T17:20:45.000+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T17:20:45.594+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:20:45.617+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:20:45.617+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T17:20:45.638+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:20:45.638+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T17:20:45.658+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.699 seconds
[2024-07-24T17:21:15.825+0000] {processor.py:157} INFO - Started process (PID=523) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:21:15.827+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T17:21:15.831+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:21:15.830+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:21:15.868+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T17:21:15.868+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T17:21:16.356+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:21:16.386+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:21:16.386+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T17:21:16.590+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:21:16.590+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T17:21:16.614+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.793 seconds
[2024-07-24T17:21:46.781+0000] {processor.py:157} INFO - Started process (PID=525) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:21:46.782+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T17:21:46.785+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:21:46.784+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:21:46.819+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T17:21:46.820+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T17:21:47.315+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:21:47.340+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:21:47.339+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T17:21:47.511+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:21:47.511+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T17:21:47.530+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.751 seconds
[2024-07-24T17:22:17.744+0000] {processor.py:157} INFO - Started process (PID=527) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:22:17.752+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T17:22:17.755+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:22:17.755+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:22:17.787+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T17:22:17.787+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T17:22:18.260+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:22:18.452+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:22:18.451+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T17:22:18.474+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:22:18.474+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T17:22:18.492+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.781 seconds
[2024-07-24T17:22:48.663+0000] {processor.py:157} INFO - Started process (PID=529) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:22:48.664+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T17:22:48.667+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:22:48.666+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:22:48.696+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T17:22:48.697+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T17:22:49.223+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:22:49.247+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:22:49.246+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T17:22:49.265+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:22:49.265+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T17:22:49.279+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.618 seconds
[2024-07-24T17:23:19.446+0000] {processor.py:157} INFO - Started process (PID=531) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:23:19.448+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T17:23:19.449+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:23:19.449+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:23:19.480+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T17:23:19.481+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T17:23:19.919+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:23:19.940+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:23:19.940+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T17:23:19.960+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:23:19.960+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T17:23:19.977+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.533 seconds
[2024-07-24T17:23:50.158+0000] {processor.py:157} INFO - Started process (PID=533) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:23:50.160+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T17:23:50.162+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:23:50.162+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:23:50.196+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T17:23:50.197+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T17:23:50.788+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:23:50.813+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:23:50.812+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T17:23:50.843+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:23:50.843+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T17:23:50.867+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.714 seconds
[2024-07-24T17:24:21.045+0000] {processor.py:157} INFO - Started process (PID=535) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:24:21.046+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T17:24:21.048+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:24:21.048+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:24:21.083+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T17:24:21.083+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T17:24:21.560+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:24:21.584+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:24:21.583+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T17:24:21.764+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:24:21.763+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T17:24:21.781+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.740 seconds
[2024-07-24T17:24:51.907+0000] {processor.py:157} INFO - Started process (PID=537) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:24:51.908+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T17:24:51.910+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:24:51.909+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:24:51.945+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T17:24:51.946+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T17:24:52.444+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:24:52.468+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:24:52.468+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T17:24:52.633+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:24:52.633+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T17:24:52.650+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.746 seconds
[2024-07-24T17:25:22.840+0000] {processor.py:157} INFO - Started process (PID=539) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:25:22.841+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T17:25:22.843+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:25:22.842+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:25:22.881+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T17:25:22.882+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T17:25:23.356+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:25:23.518+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:25:23.518+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T17:25:23.539+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:25:23.539+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T17:25:23.558+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.721 seconds
[2024-07-24T17:25:53.756+0000] {processor.py:157} INFO - Started process (PID=541) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:25:53.758+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T17:25:53.762+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:25:53.761+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:25:53.803+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T17:25:53.803+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T17:25:54.541+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:25:54.564+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:25:54.563+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T17:25:54.590+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:25:54.589+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T17:25:54.608+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.865 seconds
[2024-07-24T17:26:24.784+0000] {processor.py:157} INFO - Started process (PID=543) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:26:24.785+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T17:26:24.788+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:26:24.787+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:26:24.822+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T17:26:24.822+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T17:26:25.441+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:26:25.461+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:26:25.461+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T17:26:25.480+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:26:25.480+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T17:26:25.499+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.717 seconds
[2024-07-24T17:26:55.554+0000] {processor.py:157} INFO - Started process (PID=545) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:26:55.555+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T17:26:55.557+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:26:55.557+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:26:55.595+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T17:26:55.596+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T17:26:56.102+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:26:56.127+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:26:56.126+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T17:26:56.435+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:26:56.435+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T17:26:56.454+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.903 seconds
[2024-07-24T17:27:26.712+0000] {processor.py:157} INFO - Started process (PID=547) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:27:26.716+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T17:27:26.718+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:27:26.718+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:27:26.751+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T17:27:26.751+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T17:27:27.323+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:27:27.345+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:27:27.345+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T17:27:27.523+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:27:27.523+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T17:27:27.540+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.903 seconds
[2024-07-24T17:27:57.729+0000] {processor.py:157} INFO - Started process (PID=549) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:27:57.730+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T17:27:57.732+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:27:57.731+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:27:57.766+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T17:27:57.766+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T17:27:58.213+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:27:58.376+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:27:58.376+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T17:27:58.398+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:27:58.397+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T17:27:58.413+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.687 seconds
[2024-07-24T17:28:28.584+0000] {processor.py:157} INFO - Started process (PID=551) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:28:28.584+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T17:28:28.586+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:28:28.586+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:28:28.615+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T17:28:28.616+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T17:28:29.022+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:28:29.179+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:28:29.178+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T17:28:29.201+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:28:29.201+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T17:28:29.219+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.638 seconds
[2024-07-24T17:28:59.381+0000] {processor.py:157} INFO - Started process (PID=553) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:28:59.382+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T17:28:59.384+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:28:59.384+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:28:59.417+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T17:28:59.417+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T17:29:00.065+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:29:00.085+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:29:00.085+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T17:29:00.106+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:29:00.105+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T17:29:00.121+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.744 seconds
[2024-07-24T17:29:30.304+0000] {processor.py:157} INFO - Started process (PID=555) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:29:30.305+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T17:29:30.308+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:29:30.308+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:29:30.340+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T17:29:30.341+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T17:29:30.944+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:29:30.975+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:29:30.974+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T17:29:30.998+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:29:30.997+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T17:29:31.013+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.713 seconds
[2024-07-24T17:30:01.199+0000] {processor.py:157} INFO - Started process (PID=557) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:30:01.200+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T17:30:01.202+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:30:01.202+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:30:01.232+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T17:30:01.233+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T17:30:01.666+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:30:01.688+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:30:01.688+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T17:30:01.843+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:30:01.843+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T17:30:01.861+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.665 seconds
[2024-07-24T17:30:32.022+0000] {processor.py:157} INFO - Started process (PID=559) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:30:32.023+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T17:30:32.024+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:30:32.024+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:30:32.080+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T17:30:32.080+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T17:30:32.523+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:30:32.546+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:30:32.546+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T17:30:32.695+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:30:32.695+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T17:30:32.711+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.693 seconds
[2024-07-24T17:31:02.888+0000] {processor.py:157} INFO - Started process (PID=561) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:31:02.889+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T17:31:02.891+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:31:02.891+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:31:02.921+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T17:31:02.921+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T17:31:03.364+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:31:03.531+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:31:03.531+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T17:31:03.552+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:31:03.551+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T17:31:03.567+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.681 seconds
[2024-07-24T17:31:33.757+0000] {processor.py:157} INFO - Started process (PID=563) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:31:33.758+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T17:31:33.761+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:31:33.760+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:31:33.791+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T17:31:33.791+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T17:31:34.318+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:31:34.337+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:31:34.337+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T17:31:34.355+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:31:34.355+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T17:31:34.370+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.616 seconds
[2024-07-24T17:32:04.447+0000] {processor.py:157} INFO - Started process (PID=565) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:32:04.448+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T17:32:04.450+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:32:04.450+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:32:04.479+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T17:32:04.479+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T17:32:04.999+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:32:05.028+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:32:05.027+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T17:32:05.045+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:32:05.045+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T17:32:05.061+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.616 seconds
[2024-07-24T17:32:35.223+0000] {processor.py:157} INFO - Started process (PID=567) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:32:35.224+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T17:32:35.226+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:32:35.226+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:32:35.255+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T17:32:35.256+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T17:32:35.795+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:32:35.813+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:32:35.813+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T17:32:35.831+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:32:35.831+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T17:32:35.849+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.628 seconds
[2024-07-24T17:33:06.018+0000] {processor.py:157} INFO - Started process (PID=569) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:33:06.019+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T17:33:06.021+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:33:06.021+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:33:06.065+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T17:33:06.065+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T17:33:06.678+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:33:06.712+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:33:06.711+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T17:33:06.930+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:33:06.930+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T17:33:06.953+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.939 seconds
[2024-07-24T17:33:37.139+0000] {processor.py:157} INFO - Started process (PID=571) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:33:37.140+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T17:33:37.143+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:33:37.142+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:33:37.180+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T17:33:37.181+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T17:33:37.624+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:33:37.804+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:33:37.804+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T17:33:37.840+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:33:37.840+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T17:33:37.867+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.735 seconds
[2024-07-24T17:34:08.072+0000] {processor.py:157} INFO - Started process (PID=573) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:34:08.072+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T17:34:08.074+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:34:08.074+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:34:08.106+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T17:34:08.106+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T17:34:08.598+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:34:08.759+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:34:08.758+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T17:34:08.777+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:34:08.776+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T17:34:08.791+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.723 seconds
[2024-07-24T17:34:38.960+0000] {processor.py:157} INFO - Started process (PID=575) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:34:38.961+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T17:34:38.963+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:34:38.963+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:34:38.994+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T17:34:38.994+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T17:34:39.564+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:34:39.586+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:34:39.586+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T17:34:39.608+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:34:39.607+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T17:34:39.623+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.667 seconds
[2024-07-24T17:35:09.784+0000] {processor.py:157} INFO - Started process (PID=577) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:35:09.785+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T17:35:09.788+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:35:09.788+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:35:09.819+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T17:35:09.819+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T17:35:10.379+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:35:10.401+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:35:10.400+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T17:35:10.420+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:35:10.420+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T17:35:10.439+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.658 seconds
[2024-07-24T17:35:40.612+0000] {processor.py:157} INFO - Started process (PID=579) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:35:40.613+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T17:35:40.616+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:35:40.615+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:35:40.643+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T17:35:40.644+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T17:35:41.215+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:35:41.235+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:35:41.234+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T17:35:41.255+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:35:41.255+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T17:35:41.270+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.662 seconds
[2024-07-24T17:36:11.439+0000] {processor.py:157} INFO - Started process (PID=581) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:36:11.440+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T17:36:11.442+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:36:11.442+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:36:11.475+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T17:36:11.475+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T17:36:11.910+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:36:11.932+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:36:11.931+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T17:36:12.099+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:36:12.099+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T17:36:12.115+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.680 seconds
[2024-07-24T17:36:42.230+0000] {processor.py:157} INFO - Started process (PID=583) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:36:42.231+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T17:36:42.233+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:36:42.232+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:36:42.264+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T17:36:42.264+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T17:36:42.734+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:36:42.885+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:36:42.885+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T17:36:42.904+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:36:42.904+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T17:36:42.918+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.691 seconds
[2024-07-24T17:37:13.082+0000] {processor.py:157} INFO - Started process (PID=585) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:37:13.083+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T17:37:13.085+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:37:13.085+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:37:13.115+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T17:37:13.116+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T17:37:13.640+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:37:13.665+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:37:13.665+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T17:37:13.725+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:37:13.725+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T17:37:13.745+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.666 seconds
[2024-07-24T17:37:43.916+0000] {processor.py:157} INFO - Started process (PID=587) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:37:43.917+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T17:37:43.920+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:37:43.920+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:37:43.962+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T17:37:43.963+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T17:37:44.955+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:37:44.997+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:37:44.996+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T17:37:45.044+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:37:45.043+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T17:37:45.142+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 1.230 seconds
[2024-07-24T17:38:15.325+0000] {processor.py:157} INFO - Started process (PID=589) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:38:15.326+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T17:38:15.332+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:38:15.331+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:38:15.364+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T17:38:15.365+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T17:38:16.236+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:38:16.255+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:38:16.254+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T17:38:16.274+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:38:16.274+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T17:38:16.288+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.966 seconds
[2024-07-24T17:38:46.452+0000] {processor.py:157} INFO - Started process (PID=591) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:38:46.453+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T17:38:46.455+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:38:46.454+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:38:46.486+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T17:38:46.487+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T17:38:46.914+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:38:46.939+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:38:46.938+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T17:38:47.100+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:38:47.100+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T17:38:47.115+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.671 seconds
[2024-07-24T17:39:17.286+0000] {processor.py:157} INFO - Started process (PID=593) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:39:17.287+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T17:39:17.288+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:39:17.288+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:39:17.319+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T17:39:17.319+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T17:39:17.742+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:39:17.892+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:39:17.892+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T17:39:17.914+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:39:17.914+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T17:39:17.931+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.648 seconds
[2024-07-24T17:39:48.117+0000] {processor.py:157} INFO - Started process (PID=595) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:39:48.118+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T17:39:48.120+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:39:48.120+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:39:48.152+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T17:39:48.152+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T17:39:48.662+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:39:48.825+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:39:48.824+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T17:39:48.842+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:39:48.842+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T17:39:48.856+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.744 seconds
[2024-07-24T17:40:19.011+0000] {processor.py:157} INFO - Started process (PID=597) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:40:19.012+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T17:40:19.014+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:40:19.014+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:40:19.043+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T17:40:19.044+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T17:40:19.588+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:40:19.605+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:40:19.605+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T17:40:19.622+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:40:19.622+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T17:40:19.635+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.627 seconds
[2024-07-24T17:40:49.801+0000] {processor.py:157} INFO - Started process (PID=599) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:40:49.802+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T17:40:49.804+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:40:49.803+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:40:49.833+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T17:40:49.833+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T17:40:50.348+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:40:50.369+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:40:50.368+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T17:40:50.385+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:40:50.385+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T17:40:50.400+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.601 seconds
[2024-07-24T17:41:20.556+0000] {processor.py:157} INFO - Started process (PID=601) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:41:20.557+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T17:41:20.559+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:41:20.559+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:41:20.588+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T17:41:20.589+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T17:41:21.148+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:41:21.168+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:41:21.168+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T17:41:21.197+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:41:21.197+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T17:41:21.262+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.708 seconds
[2024-07-24T17:41:51.442+0000] {processor.py:157} INFO - Started process (PID=603) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:41:51.444+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T17:41:51.463+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:41:51.463+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:41:51.499+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T17:41:51.499+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T17:41:52.029+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:41:52.078+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:41:52.077+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T17:41:52.351+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:41:52.350+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T17:41:52.366+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.929 seconds
[2024-07-24T17:42:22.546+0000] {processor.py:157} INFO - Started process (PID=605) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:42:22.547+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T17:42:22.549+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:42:22.549+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:42:22.578+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T17:42:22.579+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T17:42:22.986+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:42:23.131+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:42:23.131+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T17:42:23.149+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:42:23.149+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T17:42:23.166+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.622 seconds
[2024-07-24T17:42:53.335+0000] {processor.py:157} INFO - Started process (PID=607) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:42:53.336+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T17:42:53.337+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:42:53.337+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:42:53.368+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T17:42:53.369+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T17:42:53.928+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:42:53.949+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:42:53.949+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T17:42:53.970+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:42:53.970+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T17:42:53.988+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.656 seconds
[2024-07-24T17:43:24.151+0000] {processor.py:157} INFO - Started process (PID=609) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:43:24.152+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T17:43:24.154+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:43:24.154+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:43:24.185+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T17:43:24.186+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T17:43:24.724+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:43:24.742+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:43:24.742+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T17:43:24.759+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:43:24.759+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T17:43:24.773+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.625 seconds
[2024-07-24T17:43:54.944+0000] {processor.py:157} INFO - Started process (PID=611) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:43:54.945+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T17:43:54.947+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:43:54.947+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:43:54.977+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T17:43:54.978+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T17:43:55.517+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:43:55.535+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:43:55.535+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T17:43:55.556+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:43:55.556+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T17:43:55.570+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.629 seconds
[2024-07-24T17:44:25.732+0000] {processor.py:157} INFO - Started process (PID=613) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:44:25.732+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T17:44:25.735+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:44:25.735+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:44:25.765+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T17:44:25.766+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T17:44:26.349+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:44:26.372+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:44:26.371+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T17:44:26.392+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:44:26.392+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T17:44:26.407+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.679 seconds
[2024-07-24T17:44:56.604+0000] {processor.py:157} INFO - Started process (PID=615) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:44:56.605+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T17:44:56.607+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:44:56.607+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:44:56.638+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T17:44:56.638+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T17:44:57.092+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:44:57.250+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:44:57.250+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T17:44:57.271+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:44:57.271+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T17:44:57.286+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.686 seconds
[2024-07-24T17:45:27.455+0000] {processor.py:157} INFO - Started process (PID=617) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:45:27.456+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T17:45:27.459+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:45:27.458+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:45:27.491+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T17:45:27.492+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T17:45:27.923+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:45:28.070+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:45:28.070+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T17:45:28.090+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:45:28.090+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T17:45:28.104+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.652 seconds
[2024-07-24T17:45:58.280+0000] {processor.py:157} INFO - Started process (PID=619) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:45:58.281+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T17:45:58.282+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:45:58.282+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:45:58.316+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T17:45:58.317+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T17:45:58.920+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:45:58.943+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:45:58.942+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T17:45:58.967+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:45:58.967+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T17:45:58.990+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.714 seconds
[2024-07-24T17:46:29.037+0000] {processor.py:157} INFO - Started process (PID=621) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:46:29.038+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T17:46:29.040+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:46:29.040+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:46:29.070+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T17:46:29.070+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T17:46:29.634+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:46:29.662+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:46:29.661+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T17:46:29.686+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:46:29.686+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T17:46:29.705+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.670 seconds
[2024-07-24T17:46:59.875+0000] {processor.py:157} INFO - Started process (PID=623) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:46:59.876+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T17:46:59.878+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:46:59.878+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:46:59.908+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T17:46:59.908+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T17:47:00.534+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:47:00.555+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:47:00.555+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T17:47:00.651+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:47:00.651+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T17:47:00.705+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.834 seconds
[2024-07-24T17:47:30.868+0000] {processor.py:157} INFO - Started process (PID=625) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:47:30.869+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T17:47:30.897+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:47:30.897+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:47:30.934+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T17:47:30.934+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T17:47:31.429+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:47:31.529+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:47:31.529+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T17:47:31.707+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:47:31.706+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T17:47:31.727+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.863 seconds
[2024-07-24T17:48:01.882+0000] {processor.py:157} INFO - Started process (PID=627) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:48:01.883+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T17:48:01.891+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:48:01.890+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:48:01.921+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T17:48:01.922+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T17:48:02.351+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:48:02.505+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:48:02.504+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T17:48:02.525+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:48:02.525+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T17:48:02.540+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.661 seconds
[2024-07-24T17:48:32.714+0000] {processor.py:157} INFO - Started process (PID=629) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:48:32.715+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T17:48:32.717+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:48:32.716+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:48:32.744+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T17:48:32.744+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T17:48:33.282+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:48:33.301+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:48:33.301+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T17:48:33.322+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:48:33.322+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T17:48:33.336+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.624 seconds
[2024-07-24T17:49:03.495+0000] {processor.py:157} INFO - Started process (PID=631) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:49:03.496+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T17:49:03.497+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:49:03.497+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:49:03.525+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T17:49:03.525+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T17:49:04.056+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:49:04.073+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:49:04.073+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T17:49:04.093+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:49:04.093+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T17:49:04.108+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.616 seconds
[2024-07-24T17:49:34.270+0000] {processor.py:157} INFO - Started process (PID=633) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:49:34.271+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T17:49:34.272+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:49:34.272+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:49:34.300+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T17:49:34.301+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T17:49:34.819+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:49:34.838+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:49:34.838+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T17:49:34.855+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:49:34.855+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T17:49:34.867+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.600 seconds
[2024-07-24T17:50:05.017+0000] {processor.py:157} INFO - Started process (PID=635) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:50:05.018+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T17:50:05.019+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:50:05.019+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:50:05.046+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T17:50:05.047+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T17:50:05.576+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:50:05.594+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:50:05.594+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T17:50:05.613+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:50:05.613+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T17:50:05.628+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.613 seconds
[2024-07-24T17:50:35.808+0000] {processor.py:157} INFO - Started process (PID=637) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:50:35.809+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T17:50:35.812+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:50:35.811+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:50:35.841+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T17:50:35.842+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T17:50:36.428+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:50:36.451+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:50:36.450+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T17:50:36.470+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:50:36.470+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T17:50:36.483+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.679 seconds
[2024-07-24T17:51:06.642+0000] {processor.py:157} INFO - Started process (PID=639) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:51:06.643+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T17:51:06.645+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:51:06.645+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:51:06.674+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T17:51:06.675+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T17:51:07.170+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:51:07.343+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:51:07.342+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T17:51:07.360+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:51:07.360+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T17:51:07.374+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.735 seconds
[2024-07-24T17:51:37.447+0000] {processor.py:157} INFO - Started process (PID=641) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:51:37.448+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T17:51:37.449+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:51:37.449+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:51:37.479+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T17:51:37.479+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T17:51:38.048+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:51:38.072+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:51:38.071+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T17:51:38.093+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:51:38.093+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T17:51:38.109+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.665 seconds
[2024-07-24T17:52:08.284+0000] {processor.py:157} INFO - Started process (PID=643) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:52:08.285+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T17:52:08.287+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:52:08.286+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:52:08.318+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T17:52:08.319+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T17:52:08.886+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:52:08.911+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:52:08.910+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T17:52:09.025+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:52:09.025+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T17:52:09.064+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.782 seconds
[2024-07-24T17:52:39.229+0000] {processor.py:157} INFO - Started process (PID=645) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:52:39.229+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T17:52:39.231+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:52:39.231+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:52:39.260+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T17:52:39.261+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T17:52:39.945+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:52:39.963+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:52:39.962+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T17:52:39.981+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:52:39.981+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T17:52:39.995+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.770 seconds
[2024-07-24T17:53:10.144+0000] {processor.py:157} INFO - Started process (PID=647) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:53:10.145+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T17:53:10.148+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:53:10.148+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:53:10.177+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T17:53:10.178+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T17:53:10.706+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:53:10.725+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:53:10.724+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T17:53:10.743+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:53:10.743+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T17:53:10.763+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.622 seconds
[2024-07-24T17:53:40.934+0000] {processor.py:157} INFO - Started process (PID=649) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:53:40.935+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T17:53:40.937+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:53:40.937+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:53:40.972+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T17:53:40.973+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T17:53:41.442+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:53:41.613+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:53:41.613+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T17:53:41.638+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:53:41.638+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T17:53:41.654+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.723 seconds
[2024-07-24T17:54:11.830+0000] {processor.py:157} INFO - Started process (PID=651) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:54:11.832+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T17:54:11.835+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:54:11.835+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:54:11.879+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T17:54:11.879+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T17:54:12.565+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:54:12.587+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:54:12.586+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T17:54:12.611+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:54:12.610+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T17:54:12.627+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.802 seconds
[2024-07-24T17:54:42.796+0000] {processor.py:157} INFO - Started process (PID=653) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:54:42.797+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T17:54:42.799+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:54:42.799+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:54:42.828+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T17:54:42.828+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T17:54:43.354+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:54:43.373+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:54:43.372+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T17:54:43.392+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:54:43.392+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T17:54:43.407+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.614 seconds
[2024-07-24T17:55:13.599+0000] {processor.py:157} INFO - Started process (PID=655) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:55:13.600+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T17:55:13.602+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:55:13.602+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:55:13.632+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T17:55:13.633+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T17:55:14.210+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:55:14.232+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:55:14.232+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T17:55:14.257+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:55:14.257+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T17:55:14.277+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.681 seconds
[2024-07-24T17:55:44.455+0000] {processor.py:157} INFO - Started process (PID=657) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:55:44.458+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T17:55:44.460+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:55:44.460+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:55:44.491+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T17:55:44.492+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T17:55:45.177+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:55:45.200+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:55:45.199+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T17:55:45.226+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:55:45.225+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T17:55:45.244+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.792 seconds
[2024-07-24T17:56:15.440+0000] {processor.py:157} INFO - Started process (PID=659) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:56:15.441+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T17:56:15.443+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:56:15.443+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:56:15.478+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T17:56:15.478+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T17:56:16.143+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:56:16.164+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:56:16.163+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T17:56:16.184+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:56:16.184+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T17:56:16.199+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.762 seconds
[2024-07-24T17:56:46.387+0000] {processor.py:157} INFO - Started process (PID=661) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:56:46.388+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T17:56:46.390+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:56:46.390+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:56:46.422+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T17:56:46.423+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T17:56:47.120+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:56:47.146+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:56:47.145+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T17:56:47.167+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:56:47.167+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T17:56:47.184+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.800 seconds
[2024-07-24T17:57:17.375+0000] {processor.py:157} INFO - Started process (PID=663) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:57:17.376+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T17:57:17.378+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:57:17.378+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:57:17.418+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T17:57:17.418+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T17:57:18.070+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:57:18.091+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:57:18.091+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T17:57:18.111+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:57:18.111+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T17:57:18.128+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.755 seconds
[2024-07-24T17:57:48.307+0000] {processor.py:157} INFO - Started process (PID=665) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:57:48.308+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T17:57:48.310+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:57:48.310+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:57:48.342+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T17:57:48.342+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T17:57:49.008+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:57:49.041+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:57:49.040+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T17:57:49.060+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:57:49.059+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T17:57:49.078+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.773 seconds
[2024-07-24T17:58:19.247+0000] {processor.py:157} INFO - Started process (PID=667) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:58:19.248+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T17:58:19.249+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:58:19.249+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:58:19.283+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T17:58:19.284+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T17:58:19.879+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:58:19.905+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:58:19.904+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T17:58:19.925+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:58:19.925+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T17:58:19.940+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.696 seconds
[2024-07-24T17:58:50.128+0000] {processor.py:157} INFO - Started process (PID=669) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:58:50.129+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T17:58:50.130+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:58:50.130+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:58:50.161+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T17:58:50.162+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T17:58:50.728+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:58:50.750+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:58:50.749+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T17:58:50.797+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:58:50.797+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T17:58:50.822+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.697 seconds
[2024-07-24T17:59:21.006+0000] {processor.py:157} INFO - Started process (PID=671) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:59:21.007+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T17:59:21.009+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:59:21.008+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:59:21.041+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T17:59:21.042+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T17:59:21.727+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:59:21.754+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:59:21.754+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T17:59:21.776+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:59:21.775+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T17:59:21.791+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.790 seconds
[2024-07-24T17:59:51.969+0000] {processor.py:157} INFO - Started process (PID=673) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:59:51.970+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T17:59:51.974+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:59:51.974+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:59:52.012+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T17:59:52.013+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T17:59:52.640+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T17:59:52.671+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:59:52.671+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T17:59:52.695+0000] {logging_mixin.py:151} INFO - [2024-07-24T17:59:52.695+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T17:59:52.714+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.748 seconds
[2024-07-24T18:00:22.853+0000] {processor.py:157} INFO - Started process (PID=675) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:00:22.854+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T18:00:22.857+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:00:22.856+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:00:22.888+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T18:00:22.889+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T18:00:23.498+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:00:23.520+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:00:23.519+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T18:00:23.607+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:00:23.607+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T18:00:23.663+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.814 seconds
[2024-07-24T18:00:53.859+0000] {processor.py:157} INFO - Started process (PID=677) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:00:53.860+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T18:00:53.863+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:00:53.862+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:00:53.894+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T18:00:53.895+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T18:00:54.560+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:00:54.583+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:00:54.583+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T18:00:54.604+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:00:54.603+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T18:00:54.619+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.764 seconds
[2024-07-24T18:01:24.804+0000] {processor.py:157} INFO - Started process (PID=679) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:01:24.805+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T18:01:24.818+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:01:24.817+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:01:24.851+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T18:01:24.851+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T18:01:25.428+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:01:25.449+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:01:25.448+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T18:01:25.472+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:01:25.472+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T18:01:25.488+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.688 seconds
[2024-07-24T18:01:55.678+0000] {processor.py:157} INFO - Started process (PID=681) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:01:55.679+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T18:01:55.681+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:01:55.681+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:01:55.711+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T18:01:55.712+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T18:01:56.406+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:01:56.436+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:01:56.436+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T18:01:56.470+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:01:56.469+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T18:01:56.488+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.813 seconds
[2024-07-24T18:02:26.663+0000] {processor.py:157} INFO - Started process (PID=683) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:02:26.664+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T18:02:26.667+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:02:26.666+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:02:26.697+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T18:02:26.698+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T18:02:27.313+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:02:27.334+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:02:27.334+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T18:02:27.354+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:02:27.354+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T18:02:27.371+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.711 seconds
[2024-07-24T18:02:57.570+0000] {processor.py:157} INFO - Started process (PID=685) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:02:57.571+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T18:02:57.574+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:02:57.573+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:02:57.608+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T18:02:57.608+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T18:02:58.257+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:02:58.289+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:02:58.288+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T18:02:58.310+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:02:58.310+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T18:02:58.326+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.760 seconds
[2024-07-24T18:03:28.511+0000] {processor.py:157} INFO - Started process (PID=687) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:03:28.511+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T18:03:28.514+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:03:28.513+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:03:28.544+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T18:03:28.545+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T18:03:29.104+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:03:29.128+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:03:29.128+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T18:03:29.154+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:03:29.154+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T18:03:29.185+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.678 seconds
[2024-07-24T18:03:59.355+0000] {processor.py:157} INFO - Started process (PID=689) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:03:59.355+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T18:03:59.358+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:03:59.358+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:03:59.391+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T18:03:59.392+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T18:04:00.013+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:04:00.035+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:04:00.034+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T18:04:00.057+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:04:00.057+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T18:04:00.072+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.721 seconds
[2024-07-24T18:04:30.269+0000] {processor.py:157} INFO - Started process (PID=691) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:04:30.270+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T18:04:30.272+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:04:30.271+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:04:30.308+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T18:04:30.309+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T18:04:30.895+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:04:30.914+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:04:30.913+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T18:04:30.938+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:04:30.938+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T18:04:30.956+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.691 seconds
[2024-07-24T18:05:01.137+0000] {processor.py:157} INFO - Started process (PID=693) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:05:01.138+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T18:05:01.141+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:05:01.140+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:05:01.175+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T18:05:01.176+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T18:05:01.786+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:05:01.809+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:05:01.808+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T18:05:01.829+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:05:01.828+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T18:05:01.845+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.712 seconds
[2024-07-24T18:05:32.012+0000] {processor.py:157} INFO - Started process (PID=695) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:05:32.013+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T18:05:32.015+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:05:32.015+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:05:32.051+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T18:05:32.052+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T18:05:32.657+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:05:32.682+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:05:32.682+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T18:05:32.706+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:05:32.705+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T18:05:32.727+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.718 seconds
[2024-07-24T18:06:02.927+0000] {processor.py:157} INFO - Started process (PID=697) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:06:02.928+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T18:06:02.931+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:06:02.930+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:06:02.963+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T18:06:02.964+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T18:06:03.577+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:06:03.597+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:06:03.596+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T18:06:03.622+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:06:03.622+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T18:06:03.646+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.722 seconds
[2024-07-24T18:06:33.850+0000] {processor.py:157} INFO - Started process (PID=699) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:06:33.851+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T18:06:33.854+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:06:33.853+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:06:33.886+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T18:06:33.887+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T18:06:34.469+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:06:34.486+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:06:34.486+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T18:06:34.508+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:06:34.507+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T18:06:34.524+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.677 seconds
[2024-07-24T18:07:04.689+0000] {processor.py:157} INFO - Started process (PID=701) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:07:04.690+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T18:07:04.691+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:07:04.691+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:07:04.724+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T18:07:04.724+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T18:07:05.274+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:07:05.294+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:07:05.293+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T18:07:05.317+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:07:05.317+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T18:07:05.334+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.648 seconds
[2024-07-24T18:07:35.518+0000] {processor.py:157} INFO - Started process (PID=703) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:07:35.519+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T18:07:35.521+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:07:35.521+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:07:35.552+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T18:07:35.552+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T18:07:36.214+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:07:36.236+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:07:36.236+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T18:07:36.256+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:07:36.256+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T18:07:36.274+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.759 seconds
[2024-07-24T18:08:06.455+0000] {processor.py:157} INFO - Started process (PID=705) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:08:06.456+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T18:08:06.458+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:08:06.458+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:08:06.492+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T18:08:06.493+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T18:08:07.115+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:08:07.137+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:08:07.137+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T18:08:07.158+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:08:07.157+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T18:08:07.174+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.723 seconds
[2024-07-24T18:08:37.340+0000] {processor.py:157} INFO - Started process (PID=707) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:08:37.342+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T18:08:37.344+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:08:37.344+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:08:37.375+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T18:08:37.376+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T18:08:37.981+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:08:38.002+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:08:38.001+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T18:08:38.026+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:08:38.025+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T18:08:38.053+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.716 seconds
[2024-07-24T18:09:08.240+0000] {processor.py:157} INFO - Started process (PID=709) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:09:08.241+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T18:09:08.243+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:09:08.243+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:09:08.275+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T18:09:08.276+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T18:09:08.828+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:09:08.851+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:09:08.851+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T18:09:08.871+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:09:08.871+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T18:09:08.886+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.649 seconds
[2024-07-24T18:09:39.061+0000] {processor.py:157} INFO - Started process (PID=711) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:09:39.062+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T18:09:39.065+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:09:39.065+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:09:39.096+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T18:09:39.096+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T18:09:39.728+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:09:39.752+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:09:39.751+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T18:09:39.777+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:09:39.777+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T18:09:39.802+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.744 seconds
[2024-07-24T18:10:09.969+0000] {processor.py:157} INFO - Started process (PID=713) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:10:09.970+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T18:10:09.973+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:10:09.972+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:10:10.002+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T18:10:10.002+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T18:10:10.591+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:10:10.612+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:10:10.612+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T18:10:10.634+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:10:10.634+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T18:10:10.651+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.684 seconds
[2024-07-24T18:10:40.837+0000] {processor.py:157} INFO - Started process (PID=715) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:10:40.837+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T18:10:40.840+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:10:40.840+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:10:40.874+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T18:10:40.875+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T18:10:41.490+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:10:41.518+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:10:41.517+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T18:10:41.543+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:10:41.542+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T18:10:41.562+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.728 seconds
[2024-07-24T18:11:11.735+0000] {processor.py:157} INFO - Started process (PID=717) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:11:11.736+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T18:11:11.737+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:11:11.737+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:11:11.770+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T18:11:11.770+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T18:11:12.399+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:11:12.425+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:11:12.425+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T18:11:12.448+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:11:12.448+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T18:11:12.466+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.734 seconds
[2024-07-24T18:11:42.639+0000] {processor.py:157} INFO - Started process (PID=719) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:11:42.639+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T18:11:42.641+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:11:42.641+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:11:42.675+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T18:11:42.675+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T18:11:43.316+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:11:43.341+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:11:43.341+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T18:11:43.363+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:11:43.363+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T18:11:43.383+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.748 seconds
[2024-07-24T18:12:13.584+0000] {processor.py:157} INFO - Started process (PID=721) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:12:13.585+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T18:12:13.587+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:12:13.587+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:12:13.623+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T18:12:13.624+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T18:12:14.203+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:12:14.225+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:12:14.224+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T18:12:14.244+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:12:14.244+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T18:12:14.261+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.681 seconds
[2024-07-24T18:12:44.444+0000] {processor.py:157} INFO - Started process (PID=723) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:12:44.445+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T18:12:44.447+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:12:44.447+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:12:44.479+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T18:12:44.479+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T18:12:45.090+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:12:45.112+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:12:45.111+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T18:12:45.137+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:12:45.137+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T18:12:45.155+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.714 seconds
[2024-07-24T18:13:15.332+0000] {processor.py:157} INFO - Started process (PID=725) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:13:15.333+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T18:13:15.336+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:13:15.335+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:13:15.372+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T18:13:15.373+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T18:13:15.976+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:13:16.001+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:13:16.000+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T18:13:16.032+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:13:16.031+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T18:13:16.054+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.725 seconds
[2024-07-24T18:13:46.245+0000] {processor.py:157} INFO - Started process (PID=727) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:13:46.246+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T18:13:46.248+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:13:46.248+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:13:46.287+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T18:13:46.287+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T18:13:47.095+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:13:47.127+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:13:47.126+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T18:13:47.156+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:13:47.156+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T18:13:47.180+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.938 seconds
[2024-07-24T18:14:17.402+0000] {processor.py:157} INFO - Started process (PID=729) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:14:17.403+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T18:14:17.406+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:14:17.405+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:14:17.447+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T18:14:17.448+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T18:14:18.090+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:14:18.115+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:14:18.115+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T18:14:18.139+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:14:18.139+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T18:14:18.161+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.762 seconds
[2024-07-24T18:14:48.368+0000] {processor.py:157} INFO - Started process (PID=731) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:14:48.369+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T18:14:48.372+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:14:48.371+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:14:48.411+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T18:14:48.412+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T18:14:49.042+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:14:49.061+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:14:49.060+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T18:14:49.081+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:14:49.081+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T18:14:49.096+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.731 seconds
[2024-07-24T18:15:19.276+0000] {processor.py:157} INFO - Started process (PID=733) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:15:19.278+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T18:15:19.280+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:15:19.279+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:15:19.325+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T18:15:19.325+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T18:15:19.943+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:15:19.964+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:15:19.963+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T18:15:19.986+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:15:19.985+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T18:15:20.002+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.729 seconds
[2024-07-24T18:15:50.177+0000] {processor.py:157} INFO - Started process (PID=735) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:15:50.178+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T18:15:50.181+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:15:50.180+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:15:50.213+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T18:15:50.213+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T18:15:50.876+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:15:50.895+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:15:50.895+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T18:15:50.919+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:15:50.919+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T18:15:50.938+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.764 seconds
[2024-07-24T18:16:21.008+0000] {processor.py:157} INFO - Started process (PID=737) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:16:21.009+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T18:16:21.013+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:16:21.012+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:16:21.049+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T18:16:21.049+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T18:16:21.659+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:16:21.683+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:16:21.682+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T18:16:21.705+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:16:21.705+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T18:16:21.722+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.718 seconds
[2024-07-24T18:16:51.926+0000] {processor.py:157} INFO - Started process (PID=739) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:16:51.927+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T18:16:51.929+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:16:51.928+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:16:51.961+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T18:16:51.962+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T18:16:52.576+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:16:52.597+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:16:52.596+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T18:16:52.616+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:16:52.616+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T18:16:52.633+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.711 seconds
[2024-07-24T18:17:22.803+0000] {processor.py:157} INFO - Started process (PID=741) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:17:22.803+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T18:17:22.806+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:17:22.805+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:17:22.840+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T18:17:22.840+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T18:17:23.422+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:17:23.440+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:17:23.440+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T18:17:23.460+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:17:23.460+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T18:17:23.480+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.680 seconds
[2024-07-24T18:17:53.551+0000] {processor.py:157} INFO - Started process (PID=743) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:17:53.552+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T18:17:53.553+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:17:53.553+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:17:53.583+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T18:17:53.583+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T18:17:54.157+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:17:54.179+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:17:54.178+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T18:17:54.202+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:17:54.201+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T18:17:54.223+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.676 seconds
[2024-07-24T18:18:24.400+0000] {processor.py:157} INFO - Started process (PID=745) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:18:24.400+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T18:18:24.402+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:18:24.402+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:18:24.434+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T18:18:24.434+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T18:18:25.016+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:18:25.035+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:18:25.035+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T18:18:25.057+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:18:25.057+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T18:18:25.074+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.677 seconds
[2024-07-24T18:18:55.253+0000] {processor.py:157} INFO - Started process (PID=747) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:18:55.254+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T18:18:55.257+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:18:55.257+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:18:55.295+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T18:18:55.296+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T18:18:55.872+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:18:55.890+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:18:55.890+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T18:18:55.909+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:18:55.909+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T18:18:55.924+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.673 seconds
[2024-07-24T18:19:26.065+0000] {processor.py:157} INFO - Started process (PID=749) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:19:26.066+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T18:19:26.068+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:19:26.068+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:19:26.099+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T18:19:26.100+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T18:19:26.676+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:19:26.697+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:19:26.696+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T18:19:26.717+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:19:26.717+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T18:19:26.741+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.679 seconds
[2024-07-24T18:19:56.921+0000] {processor.py:157} INFO - Started process (PID=751) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:19:56.922+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T18:19:56.924+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:19:56.924+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:19:56.956+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T18:19:56.956+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T18:19:57.602+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:19:57.622+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:19:57.621+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T18:19:57.642+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:19:57.642+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T18:19:57.660+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.741 seconds
[2024-07-24T18:20:27.842+0000] {processor.py:157} INFO - Started process (PID=753) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:20:27.843+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T18:20:27.845+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:20:27.845+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:20:27.874+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T18:20:27.875+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T18:20:28.502+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:20:28.522+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:20:28.522+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T18:20:28.542+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:20:28.542+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T18:20:28.558+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.720 seconds
[2024-07-24T18:20:58.725+0000] {processor.py:157} INFO - Started process (PID=755) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:20:58.725+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T18:20:58.727+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:20:58.727+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:20:58.762+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T18:20:58.762+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T18:20:59.354+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:20:59.374+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:20:59.373+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T18:20:59.397+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:20:59.396+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T18:20:59.422+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.701 seconds
[2024-07-24T18:21:29.615+0000] {processor.py:157} INFO - Started process (PID=757) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:21:29.616+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T18:21:29.618+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:21:29.618+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:21:29.650+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T18:21:29.650+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T18:21:30.234+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:21:30.255+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:21:30.254+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T18:21:30.277+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:21:30.277+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T18:21:30.293+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.681 seconds
[2024-07-24T18:22:00.480+0000] {processor.py:157} INFO - Started process (PID=759) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:22:00.481+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T18:22:00.483+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:22:00.483+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:22:00.516+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T18:22:00.516+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T18:22:01.108+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:22:01.135+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:22:01.135+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T18:22:01.158+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:22:01.158+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T18:22:01.177+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.700 seconds
[2024-07-24T18:22:31.343+0000] {processor.py:157} INFO - Started process (PID=761) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:22:31.344+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T18:22:31.347+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:22:31.347+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:22:31.379+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T18:22:31.379+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T18:22:31.991+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:22:32.015+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:22:32.014+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T18:22:32.038+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:22:32.038+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T18:22:32.054+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.714 seconds
[2024-07-24T18:23:02.252+0000] {processor.py:157} INFO - Started process (PID=763) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:23:02.253+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T18:23:02.255+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:23:02.255+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:23:02.286+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T18:23:02.287+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T18:23:02.913+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:23:02.932+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:23:02.931+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T18:23:02.951+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:23:02.951+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T18:23:02.974+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.726 seconds
[2024-07-24T18:23:33.158+0000] {processor.py:157} INFO - Started process (PID=765) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:23:33.159+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T18:23:33.160+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:23:33.160+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:23:33.193+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T18:23:33.193+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T18:23:33.799+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:23:33.820+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:23:33.820+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T18:23:33.843+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:23:33.842+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T18:23:33.860+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.706 seconds
[2024-07-24T18:24:04.027+0000] {processor.py:157} INFO - Started process (PID=767) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:24:04.028+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T18:24:04.030+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:24:04.030+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:24:04.063+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T18:24:04.063+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T18:24:04.650+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:24:04.669+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:24:04.669+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T18:24:04.692+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:24:04.692+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T18:24:04.708+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.685 seconds
[2024-07-24T18:24:34.901+0000] {processor.py:157} INFO - Started process (PID=769) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:24:34.902+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T18:24:34.905+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:24:34.904+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:24:34.940+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T18:24:34.940+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T18:24:35.489+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:24:35.507+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:24:35.506+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T18:24:35.526+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:24:35.526+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T18:24:35.541+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.644 seconds
[2024-07-24T18:25:05.717+0000] {processor.py:157} INFO - Started process (PID=771) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:25:05.718+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T18:25:05.721+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:25:05.720+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:25:05.754+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T18:25:05.754+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T18:25:06.384+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:25:06.409+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:25:06.409+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T18:25:06.433+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:25:06.433+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T18:25:06.451+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.736 seconds
[2024-07-24T18:25:36.616+0000] {processor.py:157} INFO - Started process (PID=773) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:25:36.617+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T18:25:36.619+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:25:36.619+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:25:36.651+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T18:25:36.651+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T18:25:37.242+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:25:37.263+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:25:37.263+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T18:25:37.283+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:25:37.282+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T18:25:37.297+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.684 seconds
[2024-07-24T18:26:07.468+0000] {processor.py:157} INFO - Started process (PID=775) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:26:07.469+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T18:26:07.471+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:26:07.471+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:26:07.501+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T18:26:07.502+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T18:26:08.083+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:26:08.105+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:26:08.105+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T18:26:08.128+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:26:08.128+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T18:26:08.147+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.682 seconds
[2024-07-24T18:26:38.331+0000] {processor.py:157} INFO - Started process (PID=777) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:26:38.331+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T18:26:38.333+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:26:38.333+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:26:38.362+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T18:26:38.362+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T18:26:38.871+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:26:38.891+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:26:38.890+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T18:26:38.911+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:26:38.911+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T18:26:38.926+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.599 seconds
[2024-07-24T18:27:09.040+0000] {processor.py:157} INFO - Started process (PID=779) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:27:09.041+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T18:27:09.042+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:27:09.042+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:27:09.070+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T18:27:09.070+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T18:27:09.585+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:27:09.605+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:27:09.604+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T18:27:09.624+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:27:09.624+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T18:27:09.644+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.606 seconds
[2024-07-24T18:27:39.806+0000] {processor.py:157} INFO - Started process (PID=781) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:27:39.807+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T18:27:39.809+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:27:39.809+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:27:39.839+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T18:27:39.840+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T18:27:40.434+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:27:40.453+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:27:40.452+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T18:27:40.471+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:27:40.471+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T18:27:40.485+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.682 seconds
[2024-07-24T18:28:10.643+0000] {processor.py:157} INFO - Started process (PID=783) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:28:10.645+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T18:28:10.647+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:28:10.647+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:28:10.677+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T18:28:10.677+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T18:28:11.268+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:28:11.289+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:28:11.289+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T18:28:11.308+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:28:11.307+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T18:28:11.322+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.683 seconds
[2024-07-24T18:28:41.484+0000] {processor.py:157} INFO - Started process (PID=785) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:28:41.485+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T18:28:41.487+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:28:41.487+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:28:41.515+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T18:28:41.516+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T18:28:42.019+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:28:42.035+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:28:42.035+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T18:28:42.053+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:28:42.053+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T18:28:42.068+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.588 seconds
[2024-07-24T18:29:12.235+0000] {processor.py:157} INFO - Started process (PID=787) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:29:12.236+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T18:29:12.238+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:29:12.238+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:29:12.269+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T18:29:12.269+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T18:29:12.818+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:29:12.835+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:29:12.835+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T18:29:12.852+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:29:12.852+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T18:29:12.866+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.634 seconds
[2024-07-24T18:29:43.015+0000] {processor.py:157} INFO - Started process (PID=789) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:29:43.016+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T18:29:43.018+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:29:43.017+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:29:43.045+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T18:29:43.045+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T18:29:43.536+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:29:43.554+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:29:43.554+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T18:29:43.571+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:29:43.571+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T18:29:43.584+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.572 seconds
[2024-07-24T18:30:13.738+0000] {processor.py:157} INFO - Started process (PID=791) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:30:13.738+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T18:30:13.740+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:30:13.740+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:30:13.768+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T18:30:13.768+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T18:30:14.250+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:30:14.265+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:30:14.265+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T18:30:14.283+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:30:14.283+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T18:30:14.298+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.563 seconds
[2024-07-24T18:30:44.447+0000] {processor.py:157} INFO - Started process (PID=793) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:30:44.448+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T18:30:44.450+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:30:44.450+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:30:44.478+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T18:30:44.478+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T18:30:45.019+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:30:45.040+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:30:45.039+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T18:30:45.060+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:30:45.060+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T18:30:45.075+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.630 seconds
[2024-07-24T18:31:15.241+0000] {processor.py:157} INFO - Started process (PID=795) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:31:15.242+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T18:31:15.243+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:31:15.243+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:31:15.275+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T18:31:15.275+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T18:31:15.799+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:31:15.817+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:31:15.817+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T18:31:15.834+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:31:15.834+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T18:31:15.847+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.609 seconds
[2024-07-24T18:31:45.996+0000] {processor.py:157} INFO - Started process (PID=797) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:31:45.997+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T18:31:45.999+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:31:45.999+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:31:46.033+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T18:31:46.033+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T18:31:46.573+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:31:46.606+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:31:46.605+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T18:31:46.624+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:31:46.624+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T18:31:46.639+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.646 seconds
[2024-07-24T18:32:16.799+0000] {processor.py:157} INFO - Started process (PID=799) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:32:16.800+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T18:32:16.802+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:32:16.802+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:32:16.829+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T18:32:16.829+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T18:32:17.388+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:32:17.407+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:32:17.407+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T18:32:17.431+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:32:17.431+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T18:32:17.450+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.653 seconds
[2024-07-24T18:32:47.609+0000] {processor.py:157} INFO - Started process (PID=801) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:32:47.609+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T18:32:47.611+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:32:47.611+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:32:47.639+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T18:32:47.640+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T18:32:48.178+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:32:48.196+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:32:48.195+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T18:32:48.214+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:32:48.214+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T18:32:48.228+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.621 seconds
[2024-07-24T18:33:18.389+0000] {processor.py:157} INFO - Started process (PID=803) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:33:18.390+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T18:33:18.391+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:33:18.391+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:33:18.418+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T18:33:18.418+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T18:33:18.931+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:33:18.947+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:33:18.947+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T18:33:18.964+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:33:18.964+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T18:33:18.977+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.591 seconds
[2024-07-24T18:33:49.125+0000] {processor.py:157} INFO - Started process (PID=805) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:33:49.125+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T18:33:49.127+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:33:49.127+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:33:49.154+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T18:33:49.154+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T18:33:49.649+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:33:49.667+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:33:49.667+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T18:33:49.688+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:33:49.687+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T18:33:49.703+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.581 seconds
[2024-07-24T18:34:19.860+0000] {processor.py:157} INFO - Started process (PID=807) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:34:19.860+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T18:34:19.862+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:34:19.862+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:34:19.890+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T18:34:19.890+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T18:34:20.441+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:34:20.461+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:34:20.460+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T18:34:20.479+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:34:20.479+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T18:34:20.493+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.636 seconds
[2024-07-24T18:34:50.642+0000] {processor.py:157} INFO - Started process (PID=809) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:34:50.643+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T18:34:50.644+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:34:50.644+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:34:50.672+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T18:34:50.672+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T18:34:51.215+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:34:51.234+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:34:51.234+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T18:34:51.253+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:34:51.253+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T18:34:51.267+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.628 seconds
[2024-07-24T18:35:21.430+0000] {processor.py:157} INFO - Started process (PID=811) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:35:21.431+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T18:35:21.433+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:35:21.433+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:35:21.462+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T18:35:21.462+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T18:35:21.980+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:35:21.997+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:35:21.996+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T18:35:22.014+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:35:22.014+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T18:35:22.027+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.600 seconds
[2024-07-24T18:35:52.179+0000] {processor.py:157} INFO - Started process (PID=813) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:35:52.180+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T18:35:52.182+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:35:52.182+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:35:52.211+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T18:35:52.211+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T18:35:52.772+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:35:52.791+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:35:52.790+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T18:35:52.810+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:35:52.810+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T18:35:52.824+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.648 seconds
[2024-07-24T18:36:23.012+0000] {processor.py:157} INFO - Started process (PID=815) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:36:23.015+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T18:36:23.024+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:36:23.024+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:36:23.087+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T18:36:23.087+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T18:36:24.057+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:36:24.077+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:36:24.076+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T18:36:24.093+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:36:24.093+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T18:36:24.105+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 1.115 seconds
[2024-07-24T18:36:54.167+0000] {processor.py:157} INFO - Started process (PID=817) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:36:54.168+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T18:36:54.170+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:36:54.170+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:36:54.197+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T18:36:54.197+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T18:36:54.713+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:36:54.731+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:36:54.730+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T18:36:54.748+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:36:54.748+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T18:36:54.762+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.599 seconds
[2024-07-24T18:37:24.926+0000] {processor.py:157} INFO - Started process (PID=819) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:37:24.927+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T18:37:24.930+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:37:24.929+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:37:24.958+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T18:37:24.959+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T18:37:25.523+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:37:25.541+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:37:25.541+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T18:37:25.560+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:37:25.559+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T18:37:25.574+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.651 seconds
[2024-07-24T18:37:55.728+0000] {processor.py:157} INFO - Started process (PID=821) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:37:55.729+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T18:37:55.730+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:37:55.730+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:37:55.759+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T18:37:55.760+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T18:37:56.335+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:37:56.354+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:37:56.354+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T18:37:56.373+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:37:56.373+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T18:37:56.391+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.666 seconds
[2024-07-24T18:38:26.557+0000] {processor.py:157} INFO - Started process (PID=823) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:38:26.558+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T18:38:26.560+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:38:26.560+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:38:26.588+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T18:38:26.589+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T18:38:27.113+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:38:27.131+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:38:27.130+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T18:38:27.149+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:38:27.149+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T18:38:27.163+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.609 seconds
[2024-07-24T18:38:57.317+0000] {processor.py:157} INFO - Started process (PID=825) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:38:57.318+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T18:38:57.320+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:38:57.320+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:38:57.349+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T18:38:57.349+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T18:38:57.932+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:38:57.950+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:38:57.950+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T18:38:57.968+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:38:57.968+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T18:38:57.981+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.667 seconds
[2024-07-24T18:39:28.154+0000] {processor.py:157} INFO - Started process (PID=827) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:39:28.155+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T18:39:28.157+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:39:28.157+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:39:28.187+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T18:39:28.187+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T18:39:28.739+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:39:28.757+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:39:28.757+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T18:39:28.774+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:39:28.774+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T18:39:28.789+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.637 seconds
[2024-07-24T18:39:59.029+0000] {processor.py:157} INFO - Started process (PID=829) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:39:59.030+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T18:39:59.032+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:39:59.032+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:39:59.069+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T18:39:59.069+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T18:39:59.722+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:39:59.746+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:39:59.745+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T18:39:59.769+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:39:59.769+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T18:39:59.787+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.762 seconds
[2024-07-24T18:40:29.964+0000] {processor.py:157} INFO - Started process (PID=831) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:40:29.965+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T18:40:29.967+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:40:29.967+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:40:30.000+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T18:40:30.001+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T18:40:30.640+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:40:30.661+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:40:30.661+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T18:40:30.683+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:40:30.683+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T18:40:30.701+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.740 seconds
[2024-07-24T18:41:00.875+0000] {processor.py:157} INFO - Started process (PID=833) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:41:00.877+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T18:41:00.878+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:41:00.878+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:41:00.913+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T18:41:00.913+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T18:41:01.532+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:41:01.559+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:41:01.558+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T18:41:01.581+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:41:01.581+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T18:41:01.598+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.726 seconds
[2024-07-24T18:41:31.811+0000] {processor.py:157} INFO - Started process (PID=835) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:41:31.813+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T18:41:31.815+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:41:31.815+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:41:31.856+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T18:41:31.857+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T18:41:32.609+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:41:32.633+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:41:32.632+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T18:41:32.658+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:41:32.658+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T18:41:32.679+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.870 seconds
[2024-07-24T18:42:02.910+0000] {processor.py:157} INFO - Started process (PID=837) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:42:02.912+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T18:42:02.915+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:42:02.915+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:42:02.950+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T18:42:02.951+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T18:42:03.603+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:42:03.649+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:42:03.648+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T18:42:03.695+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:42:03.695+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T18:42:03.719+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.813 seconds
[2024-07-24T18:42:33.915+0000] {processor.py:157} INFO - Started process (PID=839) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:42:33.916+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T18:42:33.918+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:42:33.918+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:42:33.952+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T18:42:33.953+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T18:42:34.574+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:42:34.595+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:42:34.594+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T18:42:34.615+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:42:34.615+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T18:42:34.631+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.719 seconds
[2024-07-24T18:43:04.800+0000] {processor.py:157} INFO - Started process (PID=841) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:43:04.801+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T18:43:04.803+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:43:04.803+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:43:04.836+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T18:43:04.836+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T18:43:05.429+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:43:05.461+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:43:05.461+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T18:43:05.483+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:43:05.482+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T18:43:05.503+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.706 seconds
[2024-07-24T18:43:35.618+0000] {processor.py:157} INFO - Started process (PID=843) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:43:35.619+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T18:43:35.621+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:43:35.621+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:43:35.652+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T18:43:35.653+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T18:43:36.289+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:43:36.312+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:43:36.311+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T18:43:36.334+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:43:36.334+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T18:43:36.355+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.740 seconds
[2024-07-24T18:44:06.546+0000] {processor.py:157} INFO - Started process (PID=845) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:44:06.547+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T18:44:06.549+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:44:06.549+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:44:06.586+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T18:44:06.586+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T18:44:07.244+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:44:07.265+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:44:07.265+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T18:44:07.286+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:44:07.286+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T18:44:07.302+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.759 seconds
[2024-07-24T18:44:37.476+0000] {processor.py:157} INFO - Started process (PID=847) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:44:37.477+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T18:44:37.480+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:44:37.480+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:44:37.518+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T18:44:37.519+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T18:44:38.110+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:44:38.138+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:44:38.137+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T18:44:38.160+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:44:38.159+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T18:44:38.176+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.703 seconds
[2024-07-24T18:45:08.350+0000] {processor.py:157} INFO - Started process (PID=849) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:45:08.351+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T18:45:08.354+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:45:08.354+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:45:08.397+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T18:45:08.398+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T18:45:09.303+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:45:09.335+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:45:09.335+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T18:45:09.373+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:45:09.373+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T18:45:09.411+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 1.065 seconds
[2024-07-24T18:45:39.529+0000] {processor.py:157} INFO - Started process (PID=851) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:45:39.530+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T18:45:39.533+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:45:39.533+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:45:39.567+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T18:45:39.567+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T18:45:40.153+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:45:40.174+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:45:40.174+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T18:45:40.193+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:45:40.193+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T18:45:40.214+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.688 seconds
[2024-07-24T18:46:10.407+0000] {processor.py:157} INFO - Started process (PID=853) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:46:10.408+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T18:46:10.410+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:46:10.410+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:46:10.441+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T18:46:10.442+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T18:46:11.036+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:46:11.060+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:46:11.059+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T18:46:11.087+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:46:11.087+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T18:46:11.106+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.702 seconds
[2024-07-24T18:46:41.268+0000] {processor.py:157} INFO - Started process (PID=855) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:46:41.269+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T18:46:41.272+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:46:41.271+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:46:41.305+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T18:46:41.305+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T18:46:41.871+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:46:41.900+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:46:41.899+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T18:46:41.921+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:46:41.921+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T18:46:41.937+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.671 seconds
[2024-07-24T18:47:12.059+0000] {processor.py:157} INFO - Started process (PID=857) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:47:12.060+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T18:47:12.062+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:47:12.062+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:47:12.096+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T18:47:12.096+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T18:47:12.658+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:47:12.680+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:47:12.679+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T18:47:12.701+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:47:12.701+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T18:47:12.718+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.662 seconds
[2024-07-24T18:47:42.885+0000] {processor.py:157} INFO - Started process (PID=859) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:47:42.886+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T18:47:42.888+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:47:42.888+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:47:42.919+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T18:47:42.919+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T18:47:43.476+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:47:43.503+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:47:43.503+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T18:47:43.521+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:47:43.521+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T18:47:43.537+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.655 seconds
[2024-07-24T18:48:13.700+0000] {processor.py:157} INFO - Started process (PID=861) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:48:13.701+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T18:48:13.703+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:48:13.703+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:48:13.732+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T18:48:13.733+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T18:48:14.297+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:48:14.322+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:48:14.321+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T18:48:14.344+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:48:14.344+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T18:48:14.360+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.662 seconds
[2024-07-24T18:48:44.518+0000] {processor.py:157} INFO - Started process (PID=863) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:48:44.518+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T18:48:44.520+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:48:44.520+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:48:44.550+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T18:48:44.551+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T18:48:45.104+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:48:45.125+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:48:45.124+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T18:48:45.153+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:48:45.152+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T18:48:45.173+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.657 seconds
[2024-07-24T18:49:15.348+0000] {processor.py:157} INFO - Started process (PID=865) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:49:15.349+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T18:49:15.351+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:49:15.351+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:49:15.382+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T18:49:15.382+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T18:49:15.965+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:49:15.987+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:49:15.986+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T18:49:16.009+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:49:16.009+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T18:49:16.032+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.688 seconds
[2024-07-24T18:49:46.202+0000] {processor.py:157} INFO - Started process (PID=867) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:49:46.203+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T18:49:46.205+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:49:46.205+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:49:46.237+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T18:49:46.237+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T18:49:46.812+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:49:46.833+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:49:46.833+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T18:49:46.854+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:49:46.854+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T18:49:46.870+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.671 seconds
[2024-07-24T18:50:16.942+0000] {processor.py:157} INFO - Started process (PID=869) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:50:16.943+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T18:50:16.945+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:50:16.945+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:50:16.978+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T18:50:16.978+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T18:50:17.620+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:50:17.643+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:50:17.643+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T18:50:17.665+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:50:17.665+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T18:50:17.687+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.748 seconds
[2024-07-24T18:50:47.864+0000] {processor.py:157} INFO - Started process (PID=871) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:50:47.865+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T18:50:47.867+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:50:47.867+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:50:47.897+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T18:50:47.898+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T18:50:48.459+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:50:48.479+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:50:48.479+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T18:50:48.498+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:50:48.498+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T18:50:48.514+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.654 seconds
[2024-07-24T18:51:18.677+0000] {processor.py:157} INFO - Started process (PID=873) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:51:18.678+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T18:51:18.680+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:51:18.680+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:51:18.708+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T18:51:18.708+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T18:51:19.230+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:51:19.250+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:51:19.249+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T18:51:19.269+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:51:19.269+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T18:51:19.283+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.609 seconds
[2024-07-24T18:51:49.459+0000] {processor.py:157} INFO - Started process (PID=875) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:51:49.460+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T18:51:49.461+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:51:49.461+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:51:49.492+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T18:51:49.493+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T18:51:50.056+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:51:50.086+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:51:50.086+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T18:51:50.105+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:51:50.105+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T18:51:50.121+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.664 seconds
[2024-07-24T18:52:20.294+0000] {processor.py:157} INFO - Started process (PID=877) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:52:20.294+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T18:52:20.296+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:52:20.296+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:52:20.327+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T18:52:20.327+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T18:52:20.875+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:52:20.894+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:52:20.893+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T18:52:20.912+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:52:20.912+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T18:52:20.925+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.635 seconds
[2024-07-24T18:52:51.084+0000] {processor.py:157} INFO - Started process (PID=879) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:52:51.085+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T18:52:51.087+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:52:51.087+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:52:51.117+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T18:52:51.118+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T18:52:51.672+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:52:51.694+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:52:51.693+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T18:52:51.715+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:52:51.714+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T18:52:51.730+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.649 seconds
[2024-07-24T18:53:21.893+0000] {processor.py:157} INFO - Started process (PID=881) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:53:21.894+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T18:53:21.895+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:53:21.895+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:53:21.924+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T18:53:21.925+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T18:53:22.495+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:53:22.515+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:53:22.515+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T18:53:22.537+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:53:22.537+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T18:53:22.554+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.663 seconds
[2024-07-24T18:53:52.727+0000] {processor.py:157} INFO - Started process (PID=883) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:53:52.728+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T18:53:52.730+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:53:52.730+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:53:52.758+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T18:53:52.759+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T18:53:53.321+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:53:53.340+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:53:53.339+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T18:53:53.360+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:53:53.360+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T18:53:53.376+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.651 seconds
[2024-07-24T18:54:23.547+0000] {processor.py:157} INFO - Started process (PID=885) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:54:23.548+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T18:54:23.550+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:54:23.550+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:54:23.580+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T18:54:23.580+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T18:54:24.121+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:54:24.140+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:54:24.139+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T18:54:24.157+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:54:24.156+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T18:54:24.170+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.628 seconds
[2024-07-24T18:54:54.332+0000] {processor.py:157} INFO - Started process (PID=887) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:54:54.333+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T18:54:54.335+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:54:54.335+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:54:54.366+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T18:54:54.366+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T18:54:54.908+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:54:54.927+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:54:54.926+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T18:54:54.945+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:54:54.944+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T18:54:54.960+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.630 seconds
[2024-07-24T18:55:25.125+0000] {processor.py:157} INFO - Started process (PID=889) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:55:25.126+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T18:55:25.128+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:55:25.127+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:55:25.155+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T18:55:25.155+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T18:55:25.718+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:55:25.738+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:55:25.738+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T18:55:25.759+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:55:25.759+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T18:55:25.773+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.650 seconds
[2024-07-24T18:55:55.931+0000] {processor.py:157} INFO - Started process (PID=891) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:55:55.932+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T18:55:55.933+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:55:55.933+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:55:55.964+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T18:55:55.964+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T18:55:56.529+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:55:56.557+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:55:56.557+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T18:55:56.577+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:55:56.577+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T18:55:56.593+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.665 seconds
[2024-07-24T18:56:26.764+0000] {processor.py:157} INFO - Started process (PID=893) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:56:26.765+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T18:56:26.767+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:56:26.766+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:56:26.796+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T18:56:26.797+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T18:56:27.347+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:56:27.367+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:56:27.367+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T18:56:27.400+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:56:27.400+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T18:56:27.420+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.659 seconds
[2024-07-24T18:56:57.616+0000] {processor.py:157} INFO - Started process (PID=895) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:56:57.616+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T18:56:57.619+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:56:57.618+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:56:57.649+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T18:56:57.649+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T18:56:58.189+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:56:58.206+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:56:58.206+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T18:56:58.223+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:56:58.223+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T18:56:58.237+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.624 seconds
[2024-07-24T18:57:28.398+0000] {processor.py:157} INFO - Started process (PID=897) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:57:28.399+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T18:57:28.402+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:57:28.401+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:57:28.431+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T18:57:28.432+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T18:57:29.015+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:57:29.035+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:57:29.035+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T18:57:29.054+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:57:29.054+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T18:57:29.070+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.675 seconds
[2024-07-24T18:57:59.194+0000] {processor.py:157} INFO - Started process (PID=899) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:57:59.195+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T18:57:59.196+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:57:59.196+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:57:59.227+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T18:57:59.228+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T18:57:59.792+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:57:59.809+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:57:59.808+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T18:57:59.827+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:57:59.827+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T18:57:59.841+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.650 seconds
[2024-07-24T18:58:30.008+0000] {processor.py:157} INFO - Started process (PID=901) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:58:30.009+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T18:58:30.011+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:58:30.011+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:58:30.039+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T18:58:30.039+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T18:58:30.601+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:58:30.619+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:58:30.618+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T18:58:30.637+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:58:30.637+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T18:58:30.650+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.644 seconds
[2024-07-24T18:59:00.819+0000] {processor.py:157} INFO - Started process (PID=903) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:59:00.820+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T18:59:00.822+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:59:00.822+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:59:00.858+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T18:59:00.858+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T18:59:01.552+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:59:01.578+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:59:01.578+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T18:59:01.610+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:59:01.610+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T18:59:01.636+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.821 seconds
[2024-07-24T18:59:31.813+0000] {processor.py:157} INFO - Started process (PID=905) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:59:31.814+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T18:59:31.816+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:59:31.815+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:59:31.849+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T18:59:31.850+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T18:59:32.573+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T18:59:32.598+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:59:32.598+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T18:59:32.624+0000] {logging_mixin.py:151} INFO - [2024-07-24T18:59:32.624+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T18:59:32.646+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.836 seconds
[2024-07-24T19:00:02.821+0000] {processor.py:157} INFO - Started process (PID=907) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:00:02.822+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T19:00:02.825+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:00:02.824+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:00:02.858+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T19:00:02.858+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T19:00:03.485+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:00:03.506+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:00:03.506+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T19:00:03.527+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:00:03.527+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T19:00:03.545+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.727 seconds
[2024-07-24T19:00:33.728+0000] {processor.py:157} INFO - Started process (PID=909) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:00:33.729+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T19:00:33.731+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:00:33.731+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:00:33.761+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T19:00:33.762+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T19:00:34.334+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:00:34.360+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:00:34.359+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T19:00:34.391+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:00:34.390+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T19:00:34.407+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.681 seconds
[2024-07-24T19:01:04.575+0000] {processor.py:157} INFO - Started process (PID=911) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:01:04.576+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T19:01:04.578+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:01:04.578+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:01:04.608+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T19:01:04.608+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T19:01:05.174+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:01:05.193+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:01:05.192+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T19:01:05.212+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:01:05.212+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T19:01:05.227+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.655 seconds
[2024-07-24T19:01:35.271+0000] {processor.py:157} INFO - Started process (PID=913) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:01:35.272+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T19:01:35.274+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:01:35.274+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:01:35.304+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T19:01:35.304+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T19:01:35.865+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:01:35.884+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:01:35.883+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T19:01:35.903+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:01:35.903+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T19:01:35.924+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.655 seconds
[2024-07-24T19:02:06.144+0000] {processor.py:157} INFO - Started process (PID=915) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:02:06.146+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T19:02:06.149+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:02:06.149+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:02:06.211+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T19:02:06.211+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T19:02:06.898+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:02:06.919+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:02:06.919+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T19:02:06.941+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:02:06.941+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T19:02:06.959+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.819 seconds
[2024-07-24T19:02:37.129+0000] {processor.py:157} INFO - Started process (PID=917) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:02:37.130+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T19:02:37.132+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:02:37.132+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:02:37.166+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T19:02:37.167+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T19:02:37.785+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:02:37.808+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:02:37.808+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T19:02:37.827+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:02:37.827+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T19:02:37.842+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.716 seconds
[2024-07-24T19:03:08.010+0000] {processor.py:157} INFO - Started process (PID=919) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:03:08.011+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T19:03:08.014+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:03:08.014+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:03:08.053+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T19:03:08.054+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T19:03:08.667+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:03:08.686+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:03:08.685+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T19:03:08.704+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:03:08.704+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T19:03:08.718+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.712 seconds
[2024-07-24T19:03:38.888+0000] {processor.py:157} INFO - Started process (PID=921) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:03:38.889+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T19:03:38.891+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:03:38.891+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:03:38.924+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T19:03:38.924+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T19:03:39.461+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:03:39.486+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:03:39.486+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T19:03:39.513+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:03:39.513+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T19:03:39.535+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.650 seconds
[2024-07-24T19:04:09.705+0000] {processor.py:157} INFO - Started process (PID=923) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:04:09.706+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T19:04:09.708+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:04:09.708+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:04:09.739+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T19:04:09.739+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T19:04:10.365+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:04:10.387+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:04:10.386+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T19:04:10.409+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:04:10.409+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T19:04:10.429+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.728 seconds
[2024-07-24T19:04:40.590+0000] {processor.py:157} INFO - Started process (PID=925) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:04:40.591+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T19:04:40.592+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:04:40.592+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:04:40.622+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T19:04:40.622+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T19:04:41.261+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:04:41.282+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:04:41.282+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T19:04:41.302+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:04:41.302+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T19:04:41.318+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.731 seconds
[2024-07-24T19:05:11.490+0000] {processor.py:157} INFO - Started process (PID=927) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:05:11.491+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T19:05:11.492+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:05:11.492+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:05:11.526+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T19:05:11.526+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T19:05:12.105+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:05:12.126+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:05:12.126+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T19:05:12.147+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:05:12.147+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T19:05:12.165+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.678 seconds
[2024-07-24T19:05:42.338+0000] {processor.py:157} INFO - Started process (PID=929) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:05:42.339+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T19:05:42.341+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:05:42.341+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:05:42.372+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T19:05:42.372+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T19:05:42.960+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:05:42.978+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:05:42.978+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T19:05:42.997+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:05:42.996+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T19:05:43.012+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.677 seconds
[2024-07-24T19:06:13.185+0000] {processor.py:157} INFO - Started process (PID=931) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:06:13.186+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T19:06:13.188+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:06:13.188+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:06:13.223+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T19:06:13.224+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T19:06:13.834+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:06:13.858+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:06:13.857+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T19:06:13.879+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:06:13.879+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T19:06:13.897+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.718 seconds
[2024-07-24T19:06:44.081+0000] {processor.py:157} INFO - Started process (PID=933) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:06:44.082+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T19:06:44.084+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:06:44.084+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:06:44.115+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T19:06:44.116+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T19:06:44.677+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:06:44.695+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:06:44.695+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T19:06:44.715+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:06:44.714+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T19:06:44.729+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.651 seconds
[2024-07-24T19:07:14.898+0000] {processor.py:157} INFO - Started process (PID=935) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:07:14.899+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T19:07:14.901+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:07:14.901+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:07:14.931+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T19:07:14.931+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T19:07:15.453+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:07:15.472+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:07:15.472+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T19:07:15.491+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:07:15.490+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T19:07:15.508+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.612 seconds
[2024-07-24T19:07:45.694+0000] {processor.py:157} INFO - Started process (PID=937) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:07:45.696+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T19:07:45.698+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:07:45.698+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:07:45.760+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T19:07:45.760+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T19:07:46.631+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:07:46.669+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:07:46.668+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T19:07:46.694+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:07:46.694+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T19:07:46.713+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 1.022 seconds
[2024-07-24T19:08:16.860+0000] {processor.py:157} INFO - Started process (PID=939) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:08:16.862+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T19:08:16.865+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:08:16.865+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:08:16.908+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T19:08:16.909+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T19:08:19.149+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:08:19.232+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:08:19.229+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T19:08:19.301+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:08:19.301+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T19:08:19.326+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 2.472 seconds
[2024-07-24T19:08:49.520+0000] {processor.py:157} INFO - Started process (PID=941) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:08:49.521+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T19:08:49.523+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:08:49.523+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:08:49.561+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T19:08:49.562+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T19:08:50.382+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:08:50.433+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:08:50.432+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T19:08:50.478+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:08:50.478+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T19:08:50.520+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 1.003 seconds
[2024-07-24T19:09:20.695+0000] {processor.py:157} INFO - Started process (PID=943) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:09:20.696+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T19:09:20.699+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:09:20.699+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:09:20.742+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T19:09:20.743+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T19:09:21.658+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:09:21.689+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:09:21.688+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T19:09:21.717+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:09:21.717+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T19:09:21.742+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 1.052 seconds
[2024-07-24T19:09:51.892+0000] {processor.py:157} INFO - Started process (PID=945) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:09:51.893+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T19:09:51.896+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:09:51.896+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:09:51.930+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T19:09:51.930+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T19:09:52.512+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:09:52.532+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:09:52.531+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T19:09:52.552+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:09:52.551+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T19:09:52.568+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.679 seconds
[2024-07-24T19:10:22.734+0000] {processor.py:157} INFO - Started process (PID=947) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:10:22.735+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T19:10:22.737+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:10:22.736+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:10:22.768+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T19:10:22.769+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T19:10:23.316+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:10:23.356+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:10:23.356+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T19:10:23.377+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:10:23.377+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T19:10:23.393+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.662 seconds
[2024-07-24T19:10:53.552+0000] {processor.py:157} INFO - Started process (PID=949) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:10:53.553+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T19:10:53.554+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:10:53.554+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:10:53.583+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T19:10:53.583+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T19:10:54.112+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:10:54.133+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:10:54.132+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T19:10:54.152+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:10:54.152+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T19:10:54.168+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.619 seconds
[2024-07-24T19:11:24.339+0000] {processor.py:157} INFO - Started process (PID=951) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:11:24.339+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T19:11:24.341+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:11:24.341+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:11:24.372+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T19:11:24.372+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T19:11:24.874+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:11:24.898+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:11:24.897+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T19:11:24.920+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:11:24.919+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T19:11:24.939+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.603 seconds
[2024-07-24T19:11:55.106+0000] {processor.py:157} INFO - Started process (PID=953) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:11:55.106+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T19:11:55.109+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:11:55.108+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:11:55.137+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T19:11:55.137+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T19:11:55.670+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:11:55.690+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:11:55.689+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T19:11:55.707+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:11:55.707+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T19:11:55.723+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.620 seconds
[2024-07-24T19:12:25.873+0000] {processor.py:157} INFO - Started process (PID=955) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:12:25.874+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T19:12:25.876+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:12:25.876+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:12:25.905+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T19:12:25.906+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T19:12:26.455+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:12:26.476+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:12:26.476+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T19:12:26.499+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:12:26.498+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T19:12:26.514+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.644 seconds
[2024-07-24T19:12:56.678+0000] {processor.py:157} INFO - Started process (PID=957) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:12:56.679+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T19:12:56.681+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:12:56.681+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:12:56.710+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T19:12:56.711+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T19:12:57.228+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:12:57.247+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:12:57.247+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T19:12:57.267+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:12:57.267+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T19:12:57.281+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.606 seconds
[2024-07-24T19:13:27.436+0000] {processor.py:157} INFO - Started process (PID=959) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:13:27.436+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T19:13:27.438+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:13:27.438+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:13:27.468+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T19:13:27.468+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T19:13:28.075+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:13:28.097+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:13:28.097+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T19:13:28.118+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:13:28.117+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T19:13:28.133+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.700 seconds
[2024-07-24T19:13:58.280+0000] {processor.py:157} INFO - Started process (PID=961) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:13:58.280+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T19:13:58.282+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:13:58.282+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:13:58.312+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T19:13:58.312+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T19:13:58.876+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:13:58.895+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:13:58.894+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T19:13:58.911+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:13:58.911+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T19:13:58.924+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.647 seconds
[2024-07-24T19:14:29.092+0000] {processor.py:157} INFO - Started process (PID=963) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:14:29.092+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T19:14:29.094+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:14:29.094+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:14:29.123+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T19:14:29.123+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T19:14:29.668+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:14:29.688+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:14:29.687+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T19:14:29.705+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:14:29.705+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T19:14:29.719+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.630 seconds
[2024-07-24T19:14:59.885+0000] {processor.py:157} INFO - Started process (PID=965) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:14:59.886+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T19:14:59.888+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:14:59.887+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:14:59.919+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T19:14:59.919+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T19:15:00.462+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:15:00.481+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:15:00.480+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T19:15:00.497+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:15:00.497+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T19:15:00.516+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.634 seconds
[2024-07-24T19:15:30.682+0000] {processor.py:157} INFO - Started process (PID=967) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:15:30.683+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T19:15:30.685+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:15:30.685+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:15:30.714+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T19:15:30.715+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T19:15:31.303+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:15:31.324+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:15:31.323+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T19:15:31.344+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:15:31.343+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T19:15:31.359+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.679 seconds
[2024-07-24T19:16:01.521+0000] {processor.py:157} INFO - Started process (PID=969) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:16:01.522+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T19:16:01.525+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:16:01.524+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:16:01.556+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T19:16:01.556+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T19:16:02.152+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:16:02.171+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:16:02.171+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T19:16:02.193+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:16:02.192+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T19:16:02.208+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.690 seconds
[2024-07-24T19:16:32.311+0000] {processor.py:157} INFO - Started process (PID=971) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:16:32.312+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T19:16:32.314+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:16:32.314+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:16:32.344+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T19:16:32.344+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T19:16:32.939+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:16:32.960+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:16:32.959+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T19:16:32.978+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:16:32.977+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T19:16:32.992+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.684 seconds
[2024-07-24T19:17:03.181+0000] {processor.py:157} INFO - Started process (PID=973) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:17:03.182+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T19:17:03.183+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:17:03.183+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:17:03.212+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T19:17:03.213+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T19:17:03.802+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:17:03.821+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:17:03.821+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T19:17:03.839+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:17:03.839+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T19:17:03.854+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.676 seconds
[2024-07-24T19:17:34.013+0000] {processor.py:157} INFO - Started process (PID=975) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:17:34.014+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T19:17:34.015+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:17:34.015+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:17:34.049+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T19:17:34.049+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T19:17:34.622+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:17:34.640+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:17:34.639+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T19:17:34.657+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:17:34.657+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T19:17:34.670+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.660 seconds
[2024-07-24T19:18:04.783+0000] {processor.py:157} INFO - Started process (PID=977) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:18:04.784+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T19:18:04.790+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:18:04.790+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:18:04.821+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T19:18:04.821+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T19:18:05.355+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:18:05.373+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:18:05.373+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T19:18:05.392+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:18:05.391+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T19:18:05.414+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.634 seconds
[2024-07-24T19:18:35.582+0000] {processor.py:157} INFO - Started process (PID=979) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:18:35.583+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T19:18:35.584+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:18:35.584+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:18:35.614+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T19:18:35.615+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T19:18:36.182+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:18:36.202+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:18:36.201+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T19:18:36.221+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:18:36.221+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T19:18:36.236+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.656 seconds
[2024-07-24T19:19:06.395+0000] {processor.py:157} INFO - Started process (PID=981) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:19:06.396+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T19:19:06.398+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:19:06.398+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:19:06.428+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T19:19:06.428+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T19:19:06.997+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:19:07.016+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:19:07.016+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T19:19:07.037+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:19:07.036+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T19:19:07.055+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.663 seconds
[2024-07-24T19:19:37.255+0000] {processor.py:157} INFO - Started process (PID=983) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:19:37.256+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T19:19:37.260+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:19:37.260+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:19:37.299+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T19:19:37.300+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T19:19:37.901+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:19:37.923+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:19:37.923+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T19:19:37.942+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:19:37.941+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T19:19:37.957+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.707 seconds
[2024-07-24T19:20:08.126+0000] {processor.py:157} INFO - Started process (PID=985) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:20:08.126+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T19:20:08.129+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:20:08.128+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:20:08.158+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T19:20:08.158+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T19:20:08.688+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:20:08.707+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:20:08.707+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T19:20:08.725+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:20:08.724+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T19:20:08.739+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.616 seconds
[2024-07-24T19:20:38.826+0000] {processor.py:157} INFO - Started process (PID=987) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:20:38.826+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T19:20:38.828+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:20:38.828+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:20:38.857+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T19:20:38.857+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T19:20:39.361+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:20:39.382+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:20:39.382+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T19:20:39.400+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:20:39.400+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T19:20:39.414+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.592 seconds
[2024-07-24T19:21:09.574+0000] {processor.py:157} INFO - Started process (PID=989) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:21:09.574+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T19:21:09.576+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:21:09.576+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:21:09.603+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T19:21:09.604+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T19:21:10.168+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:21:10.189+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:21:10.189+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T19:21:10.214+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:21:10.214+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T19:21:10.228+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.657 seconds
[2024-07-24T19:21:40.387+0000] {processor.py:157} INFO - Started process (PID=991) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:21:40.388+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T19:21:40.390+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:21:40.390+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:21:40.420+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T19:21:40.420+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T19:21:40.954+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:21:40.976+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:21:40.975+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T19:21:41.000+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:21:41.000+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T19:21:41.017+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.633 seconds
[2024-07-24T19:22:11.185+0000] {processor.py:157} INFO - Started process (PID=993) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:22:11.185+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T19:22:11.188+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:22:11.187+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:22:11.220+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T19:22:11.220+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T19:22:11.827+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:22:11.847+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:22:11.846+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T19:22:11.866+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:22:11.866+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T19:22:11.883+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.701 seconds
[2024-07-24T19:22:42.058+0000] {processor.py:157} INFO - Started process (PID=995) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:22:42.059+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T19:22:42.061+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:22:42.061+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:22:42.091+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T19:22:42.092+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T19:22:42.660+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:22:42.680+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:22:42.680+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T19:22:42.699+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:22:42.699+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T19:22:42.713+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.658 seconds
[2024-07-24T19:23:12.802+0000] {processor.py:157} INFO - Started process (PID=997) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:23:12.803+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T19:23:12.806+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:23:12.805+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:23:12.837+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T19:23:12.837+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T19:23:13.385+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:23:13.403+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:23:13.402+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T19:23:13.420+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:23:13.420+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T19:23:13.434+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.635 seconds
[2024-07-24T19:23:43.600+0000] {processor.py:157} INFO - Started process (PID=999) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:23:43.601+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T19:23:43.603+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:23:43.603+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:23:43.634+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T19:23:43.634+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T19:23:44.178+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:23:44.197+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:23:44.197+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T19:23:44.217+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:23:44.217+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T19:23:44.232+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.635 seconds
[2024-07-24T19:24:14.392+0000] {processor.py:157} INFO - Started process (PID=1001) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:24:14.393+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T19:24:14.395+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:24:14.395+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:24:14.423+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T19:24:14.424+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T19:24:14.945+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:24:14.963+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:24:14.962+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T19:24:14.981+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:24:14.980+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T19:24:14.994+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.604 seconds
[2024-07-24T19:24:45.162+0000] {processor.py:157} INFO - Started process (PID=1003) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:24:45.162+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T19:24:45.164+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:24:45.164+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:24:45.193+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T19:24:45.193+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T19:24:45.751+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:24:45.770+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:24:45.769+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T19:24:45.790+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:24:45.790+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T19:24:45.805+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.646 seconds
[2024-07-24T19:25:15.964+0000] {processor.py:157} INFO - Started process (PID=1005) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:25:15.965+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T19:25:15.967+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:25:15.967+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:25:16.000+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T19:25:16.000+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T19:25:16.559+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:25:16.578+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:25:16.578+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T19:25:16.599+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:25:16.599+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T19:25:16.614+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.653 seconds
[2024-07-24T19:25:46.787+0000] {processor.py:157} INFO - Started process (PID=1007) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:25:46.788+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T19:25:46.790+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:25:46.790+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:25:46.819+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T19:25:46.820+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T19:25:47.387+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:25:47.408+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:25:47.407+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T19:25:47.426+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:25:47.426+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T19:25:47.440+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.656 seconds
[2024-07-24T19:26:17.604+0000] {processor.py:157} INFO - Started process (PID=1009) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:26:17.605+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T19:26:17.607+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:26:17.607+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:26:17.635+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T19:26:17.636+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T19:26:18.177+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:26:18.195+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:26:18.195+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T19:26:18.214+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:26:18.214+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T19:26:18.230+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.629 seconds
[2024-07-24T19:26:48.403+0000] {processor.py:157} INFO - Started process (PID=1011) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:26:48.403+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T19:26:48.406+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:26:48.405+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:26:48.436+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T19:26:48.436+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T19:26:48.972+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:26:48.991+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:26:48.991+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T19:26:49.008+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:26:49.008+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T19:26:49.021+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.621 seconds
[2024-07-24T19:27:19.187+0000] {processor.py:157} INFO - Started process (PID=1013) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:27:19.187+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T19:27:19.189+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:27:19.189+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:27:19.218+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T19:27:19.219+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T19:27:19.729+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:27:19.746+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:27:19.745+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T19:27:19.763+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:27:19.762+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T19:27:19.776+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.592 seconds
[2024-07-24T19:27:49.931+0000] {processor.py:157} INFO - Started process (PID=1015) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:27:49.932+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T19:27:49.933+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:27:49.933+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:27:49.963+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T19:27:49.963+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T19:27:50.488+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:27:50.509+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:27:50.508+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T19:27:50.530+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:27:50.530+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T19:27:50.548+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.619 seconds
[2024-07-24T19:28:20.721+0000] {processor.py:157} INFO - Started process (PID=1017) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:28:20.722+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T19:28:20.725+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:28:20.725+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:28:20.759+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T19:28:20.760+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T19:28:21.322+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:28:21.341+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:28:21.340+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T19:28:21.359+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:28:21.359+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T19:28:21.382+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.666 seconds
[2024-07-24T19:28:51.546+0000] {processor.py:157} INFO - Started process (PID=1019) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:28:51.547+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T19:28:51.550+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:28:51.549+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:28:51.580+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T19:28:51.580+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T19:28:52.186+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:28:52.207+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:28:52.206+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T19:28:52.227+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:28:52.227+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T19:28:52.243+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.707 seconds
[2024-07-24T19:29:22.343+0000] {processor.py:157} INFO - Started process (PID=1021) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:29:22.344+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T19:29:22.346+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:29:22.346+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:29:22.375+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T19:29:22.376+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T19:29:22.939+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:29:22.960+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:29:22.959+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T19:29:22.979+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:29:22.979+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T19:29:22.994+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.654 seconds
[2024-07-24T19:29:53.157+0000] {processor.py:157} INFO - Started process (PID=1023) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:29:53.158+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T19:29:53.160+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:29:53.160+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:29:53.189+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T19:29:53.189+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T19:29:53.772+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:29:53.795+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:29:53.794+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T19:29:53.815+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:29:53.815+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T19:29:53.830+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.676 seconds
[2024-07-24T19:30:23.993+0000] {processor.py:157} INFO - Started process (PID=1025) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:30:23.994+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T19:30:23.995+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:30:23.995+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:30:24.025+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T19:30:24.025+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T19:30:24.538+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:30:24.556+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:30:24.556+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T19:30:24.576+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:30:24.576+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T19:30:24.591+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.602 seconds
[2024-07-24T19:30:54.759+0000] {processor.py:157} INFO - Started process (PID=1027) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:30:54.759+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T19:30:54.762+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:30:54.761+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:30:54.790+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T19:30:54.791+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T19:30:55.315+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:30:55.333+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:30:55.333+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T19:30:55.353+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:30:55.353+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T19:30:55.371+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.615 seconds
[2024-07-24T19:31:25.528+0000] {processor.py:157} INFO - Started process (PID=1029) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:31:25.529+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T19:31:25.531+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:31:25.531+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:31:25.560+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T19:31:25.560+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T19:31:26.138+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:31:26.158+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:31:26.158+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T19:31:26.180+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:31:26.180+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T19:31:26.198+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.673 seconds
[2024-07-24T19:31:56.366+0000] {processor.py:157} INFO - Started process (PID=1031) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:31:56.367+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T19:31:56.369+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:31:56.369+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:31:56.398+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T19:31:56.399+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T19:31:56.977+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:31:56.995+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:31:56.994+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T19:31:57.013+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:31:57.013+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T19:31:57.030+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.668 seconds
[2024-07-24T19:32:27.194+0000] {processor.py:157} INFO - Started process (PID=1033) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:32:27.195+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T19:32:27.198+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:32:27.197+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:32:27.228+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T19:32:27.229+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T19:32:27.821+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:32:27.841+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:32:27.841+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T19:32:27.860+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:32:27.860+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T19:32:27.875+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.684 seconds
[2024-07-24T19:32:57.969+0000] {processor.py:157} INFO - Started process (PID=1035) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:32:57.970+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T19:32:57.972+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:32:57.972+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:32:58.001+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T19:32:58.001+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T19:32:58.562+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:32:58.580+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:32:58.579+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T19:32:58.601+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:32:58.601+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T19:32:58.616+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.650 seconds
[2024-07-24T19:33:28.785+0000] {processor.py:157} INFO - Started process (PID=1037) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:33:28.787+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T19:33:28.790+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:33:28.789+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:33:28.821+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T19:33:28.821+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T19:33:29.398+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:33:29.420+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:33:29.419+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T19:33:29.438+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:33:29.438+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T19:33:29.455+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.672 seconds
[2024-07-24T19:33:59.629+0000] {processor.py:157} INFO - Started process (PID=1039) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:33:59.630+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T19:33:59.631+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:33:59.631+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:33:59.662+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T19:33:59.663+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T19:34:00.244+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:34:00.264+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:34:00.263+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T19:34:00.282+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:34:00.282+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T19:34:00.296+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.672 seconds
[2024-07-24T19:34:30.442+0000] {processor.py:157} INFO - Started process (PID=1041) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:34:30.443+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T19:34:30.445+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:34:30.445+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:34:30.475+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T19:34:30.475+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T19:34:31.031+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:34:31.055+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:34:31.054+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T19:34:31.077+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:34:31.077+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T19:34:31.093+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.653 seconds
[2024-07-24T19:35:01.290+0000] {processor.py:157} INFO - Started process (PID=1043) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:35:01.292+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T19:35:01.303+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:35:01.302+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:35:01.371+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T19:35:01.374+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T19:35:02.347+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:35:02.383+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:35:02.382+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T19:35:02.418+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:35:02.418+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T19:35:02.451+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 1.166 seconds
[2024-07-24T19:35:32.630+0000] {processor.py:157} INFO - Started process (PID=1045) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:35:32.631+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T19:35:32.634+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:35:32.633+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:35:32.670+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T19:35:32.671+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T19:35:33.370+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:35:33.399+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:35:33.398+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T19:35:33.429+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:35:33.428+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T19:35:33.448+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.821 seconds
[2024-07-24T19:36:03.601+0000] {processor.py:157} INFO - Started process (PID=1047) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:36:03.602+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T19:36:03.605+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:36:03.605+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:36:03.643+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T19:36:03.643+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T19:36:04.333+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:36:04.361+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:36:04.360+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T19:36:04.387+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:36:04.387+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T19:36:04.409+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.812 seconds
[2024-07-24T19:36:34.593+0000] {processor.py:157} INFO - Started process (PID=1049) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:36:34.594+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T19:36:34.596+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:36:34.596+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:36:34.627+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T19:36:34.627+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T19:36:35.205+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:36:35.233+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:36:35.233+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T19:36:35.263+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:36:35.263+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T19:36:35.284+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.694 seconds
[2024-07-24T19:37:05.457+0000] {processor.py:157} INFO - Started process (PID=1051) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:37:05.458+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T19:37:05.460+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:37:05.460+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:37:05.497+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T19:37:05.498+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T19:37:06.046+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:37:06.067+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:37:06.066+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T19:37:06.088+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:37:06.087+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T19:37:06.101+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.649 seconds
[2024-07-24T19:37:36.260+0000] {processor.py:157} INFO - Started process (PID=1053) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:37:36.261+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T19:37:36.263+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:37:36.263+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:37:36.293+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T19:37:36.294+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T19:37:36.927+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:37:36.952+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:37:36.952+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T19:37:36.979+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:37:36.979+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T19:37:36.999+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.742 seconds
[2024-07-24T19:38:07.176+0000] {processor.py:157} INFO - Started process (PID=1055) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:38:07.177+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T19:38:07.181+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:38:07.181+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:38:07.215+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T19:38:07.216+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T19:38:07.814+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:38:07.839+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:38:07.839+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T19:38:07.861+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:38:07.861+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T19:38:07.880+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.707 seconds
[2024-07-24T19:38:38.042+0000] {processor.py:157} INFO - Started process (PID=1057) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:38:38.043+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T19:38:38.045+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:38:38.045+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:38:38.073+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T19:38:38.073+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T19:38:38.610+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:38:38.632+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:38:38.632+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T19:38:38.652+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:38:38.651+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T19:38:38.671+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.632 seconds
[2024-07-24T19:39:08.816+0000] {processor.py:157} INFO - Started process (PID=1059) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:39:08.817+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T19:39:08.819+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:39:08.818+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:39:08.845+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T19:39:08.846+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T19:39:09.364+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:39:09.388+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:39:09.388+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T19:39:09.408+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:39:09.408+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T19:39:09.422+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.608 seconds
[2024-07-24T19:39:39.577+0000] {processor.py:157} INFO - Started process (PID=1061) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:39:39.578+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T19:39:39.580+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:39:39.580+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:39:39.607+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T19:39:39.608+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T19:39:40.235+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:39:40.255+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:39:40.255+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T19:39:40.274+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:39:40.274+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T19:39:40.288+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.714 seconds
[2024-07-24T19:40:10.445+0000] {processor.py:157} INFO - Started process (PID=1063) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:40:10.446+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T19:40:10.448+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:40:10.448+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:40:10.478+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T19:40:10.478+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T19:40:11.015+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:40:11.042+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:40:11.041+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T19:40:11.065+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:40:11.065+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T19:40:11.082+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.639 seconds
[2024-07-24T19:40:41.226+0000] {processor.py:157} INFO - Started process (PID=1065) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:40:41.227+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T19:40:41.229+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:40:41.228+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:40:41.258+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T19:40:41.259+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T19:40:41.865+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:40:41.889+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:40:41.888+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T19:40:41.922+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:40:41.921+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T19:40:41.944+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.721 seconds
[2024-07-24T19:41:12.117+0000] {processor.py:157} INFO - Started process (PID=1067) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:41:12.118+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T19:41:12.120+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:41:12.120+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:41:12.153+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T19:41:12.153+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T19:41:12.743+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:41:12.764+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:41:12.763+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T19:41:12.786+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:41:12.786+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T19:41:12.802+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.688 seconds
[2024-07-24T19:41:42.984+0000] {processor.py:157} INFO - Started process (PID=1069) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:41:42.985+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T19:41:42.988+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:41:42.988+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:41:43.017+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T19:41:43.018+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T19:41:43.545+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:41:43.575+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:41:43.574+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T19:41:43.596+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:41:43.596+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T19:41:43.611+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.629 seconds
[2024-07-24T19:42:13.645+0000] {processor.py:157} INFO - Started process (PID=1071) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:42:13.645+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T19:42:13.648+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:42:13.648+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:42:13.676+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T19:42:13.676+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T19:42:14.214+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:42:14.233+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:42:14.233+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T19:42:14.251+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:42:14.251+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T19:42:14.264+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.622 seconds
[2024-07-24T19:42:44.429+0000] {processor.py:157} INFO - Started process (PID=1073) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:42:44.430+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T19:42:44.432+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:42:44.432+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:42:44.461+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T19:42:44.462+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T19:42:44.992+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:42:45.011+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:42:45.011+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T19:42:45.032+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:42:45.032+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T19:42:45.049+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.622 seconds
[2024-07-24T19:43:15.204+0000] {processor.py:157} INFO - Started process (PID=1075) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:43:15.205+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T19:43:15.207+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:43:15.207+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:43:15.235+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T19:43:15.235+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T19:43:15.787+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:43:15.805+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:43:15.805+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T19:43:15.824+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:43:15.824+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T19:43:15.839+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.638 seconds
[2024-07-24T19:43:46.015+0000] {processor.py:157} INFO - Started process (PID=1077) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:43:46.017+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T19:43:46.019+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:43:46.018+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:43:46.055+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T19:43:46.055+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T19:43:46.608+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:43:46.629+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:43:46.629+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T19:43:46.651+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:43:46.651+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T19:43:46.668+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.657 seconds
[2024-07-24T19:44:16.824+0000] {processor.py:157} INFO - Started process (PID=1079) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:44:16.825+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T19:44:16.827+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:44:16.826+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:44:16.857+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T19:44:16.858+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T19:44:17.407+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:44:17.427+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:44:17.427+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T19:44:17.447+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:44:17.447+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T19:44:17.463+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.642 seconds
[2024-07-24T19:44:47.630+0000] {processor.py:157} INFO - Started process (PID=1081) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:44:47.631+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T19:44:47.632+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:44:47.632+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:44:47.660+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T19:44:47.660+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T19:44:48.231+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:44:48.258+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:44:48.258+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T19:44:48.285+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:44:48.285+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T19:44:48.305+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.677 seconds
[2024-07-24T19:45:18.474+0000] {processor.py:157} INFO - Started process (PID=1083) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:45:18.474+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T19:45:18.476+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:45:18.476+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:45:18.507+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T19:45:18.507+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T19:45:19.027+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:45:19.045+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:45:19.044+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T19:45:19.062+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:45:19.061+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T19:45:19.075+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.606 seconds
[2024-07-24T19:45:49.233+0000] {processor.py:157} INFO - Started process (PID=1085) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:45:49.234+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T19:45:49.236+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:45:49.235+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:45:49.263+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T19:45:49.264+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T19:45:49.767+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:45:49.785+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:45:49.784+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T19:45:49.801+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:45:49.801+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T19:45:49.814+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.584 seconds
[2024-07-24T19:46:19.982+0000] {processor.py:157} INFO - Started process (PID=1087) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:46:19.983+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T19:46:19.985+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:46:19.985+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:46:20.014+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T19:46:20.014+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T19:46:20.544+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:46:20.561+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:46:20.561+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T19:46:20.578+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:46:20.578+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T19:46:20.594+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.615 seconds
[2024-07-24T19:46:50.748+0000] {processor.py:157} INFO - Started process (PID=1089) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:46:50.749+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T19:46:50.751+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:46:50.751+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:46:50.780+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T19:46:50.780+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T19:46:51.350+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:46:51.369+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:46:51.369+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T19:46:51.393+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:46:51.393+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T19:46:51.408+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.663 seconds
[2024-07-24T19:47:21.580+0000] {processor.py:157} INFO - Started process (PID=1091) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:47:21.581+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T19:47:21.583+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:47:21.583+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:47:21.615+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T19:47:21.615+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T19:47:22.179+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:47:22.199+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:47:22.198+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T19:47:22.221+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:47:22.220+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T19:47:22.236+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.660 seconds
[2024-07-24T19:47:52.404+0000] {processor.py:157} INFO - Started process (PID=1093) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:47:52.405+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T19:47:52.407+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:47:52.407+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:47:52.438+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T19:47:52.438+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T19:47:53.005+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:47:53.027+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:47:53.026+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T19:47:53.047+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:47:53.047+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T19:47:53.063+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.662 seconds
[2024-07-24T19:48:23.157+0000] {processor.py:157} INFO - Started process (PID=1095) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:48:23.158+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T19:48:23.159+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:48:23.159+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:48:23.188+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T19:48:23.189+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T19:48:23.744+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:48:23.767+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:48:23.767+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T19:48:23.787+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:48:23.787+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T19:48:23.803+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.649 seconds
[2024-07-24T19:48:53.966+0000] {processor.py:157} INFO - Started process (PID=1097) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:48:53.967+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T19:48:53.969+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:48:53.969+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:48:53.998+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T19:48:53.998+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T19:48:54.537+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:48:54.555+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:48:54.555+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T19:48:54.572+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:48:54.572+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T19:48:54.587+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.624 seconds
[2024-07-24T19:49:24.744+0000] {processor.py:157} INFO - Started process (PID=1099) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:49:24.744+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T19:49:24.746+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:49:24.746+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:49:24.774+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T19:49:24.774+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T19:49:25.313+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:49:25.330+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:49:25.329+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T19:49:25.350+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:49:25.350+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T19:49:25.373+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.632 seconds
[2024-07-24T19:49:55.548+0000] {processor.py:157} INFO - Started process (PID=1101) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:49:55.549+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T19:49:55.553+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:49:55.552+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:49:55.582+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T19:49:55.582+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T19:49:56.133+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:49:56.153+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:49:56.153+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T19:49:56.178+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:49:56.178+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T19:49:56.195+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.650 seconds
[2024-07-24T19:50:26.354+0000] {processor.py:157} INFO - Started process (PID=1103) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:50:26.355+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T19:50:26.356+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:50:26.356+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:50:26.386+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T19:50:26.387+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T19:50:26.952+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:50:26.973+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:50:26.972+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T19:50:26.999+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:50:26.998+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T19:50:27.014+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.663 seconds
[2024-07-24T19:50:57.170+0000] {processor.py:157} INFO - Started process (PID=1105) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:50:57.170+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T19:50:57.172+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:50:57.172+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:50:57.202+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T19:50:57.202+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T19:50:57.800+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:50:57.821+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:50:57.820+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T19:50:57.840+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:50:57.840+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T19:50:57.854+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.687 seconds
[2024-07-24T19:51:28.019+0000] {processor.py:157} INFO - Started process (PID=1107) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:51:28.020+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T19:51:28.022+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:51:28.021+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:51:28.049+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T19:51:28.050+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T19:51:28.568+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:51:28.587+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:51:28.587+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T19:51:28.606+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:51:28.606+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T19:51:28.623+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.606 seconds
[2024-07-24T19:51:58.806+0000] {processor.py:157} INFO - Started process (PID=1109) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:51:58.807+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T19:51:58.809+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:51:58.808+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:51:58.836+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T19:51:58.836+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T19:51:59.375+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:51:59.395+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:51:59.395+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T19:51:59.417+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:51:59.417+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T19:51:59.436+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.634 seconds
[2024-07-24T19:52:29.592+0000] {processor.py:157} INFO - Started process (PID=1111) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:52:29.593+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T19:52:29.596+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:52:29.596+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:52:29.626+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T19:52:29.626+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T19:52:30.177+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:52:30.194+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:52:30.193+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T19:52:30.213+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:52:30.213+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T19:52:30.226+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.638 seconds
[2024-07-24T19:53:00.388+0000] {processor.py:157} INFO - Started process (PID=1113) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:53:00.388+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T19:53:00.390+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:53:00.390+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:53:00.418+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T19:53:00.419+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T19:53:00.939+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:53:00.968+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:53:00.967+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T19:53:00.989+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:53:00.988+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T19:53:01.007+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.622 seconds
[2024-07-24T19:53:31.168+0000] {processor.py:157} INFO - Started process (PID=1115) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:53:31.169+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T19:53:31.171+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:53:31.171+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:53:31.200+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T19:53:31.201+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T19:53:31.736+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:53:31.755+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:53:31.754+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T19:53:31.773+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:53:31.773+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T19:53:31.793+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.628 seconds
[2024-07-24T19:54:01.964+0000] {processor.py:157} INFO - Started process (PID=1117) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:54:01.966+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T19:54:01.967+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:54:01.967+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:54:01.997+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T19:54:01.998+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T19:54:02.547+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:54:02.566+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:54:02.565+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T19:54:02.585+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:54:02.585+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T19:54:02.599+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.638 seconds
[2024-07-24T19:54:32.752+0000] {processor.py:157} INFO - Started process (PID=1119) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:54:32.753+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T19:54:32.755+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:54:32.755+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:54:32.787+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T19:54:32.788+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T19:54:33.325+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:54:33.346+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:54:33.346+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T19:54:33.364+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:54:33.364+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T19:54:33.378+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.628 seconds
[2024-07-24T19:55:03.547+0000] {processor.py:157} INFO - Started process (PID=1121) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:55:03.548+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T19:55:03.549+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:55:03.549+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:55:03.578+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T19:55:03.579+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T19:55:04.098+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:55:04.114+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:55:04.113+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T19:55:04.131+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:55:04.130+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T19:55:04.145+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.600 seconds
[2024-07-24T19:55:34.297+0000] {processor.py:157} INFO - Started process (PID=1123) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:55:34.298+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T19:55:34.300+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:55:34.300+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:55:34.327+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T19:55:34.327+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T19:55:34.907+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:55:34.928+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:55:34.928+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T19:55:34.952+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:55:34.952+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T19:55:34.970+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.675 seconds
[2024-07-24T19:56:05.148+0000] {processor.py:157} INFO - Started process (PID=1125) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:56:05.149+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T19:56:05.151+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:56:05.151+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:56:05.185+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T19:56:05.186+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T19:56:05.757+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:56:05.774+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:56:05.773+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T19:56:05.792+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:56:05.792+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T19:56:05.806+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.660 seconds
[2024-07-24T19:56:35.964+0000] {processor.py:157} INFO - Started process (PID=1127) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:56:35.965+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T19:56:35.967+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:56:35.966+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:56:35.996+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T19:56:35.996+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T19:56:36.555+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:56:36.574+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:56:36.573+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T19:56:36.593+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:56:36.593+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T19:56:36.608+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.647 seconds
[2024-07-24T19:57:06.733+0000] {processor.py:157} INFO - Started process (PID=1129) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:57:06.734+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T19:57:06.736+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:57:06.736+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:57:06.765+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T19:57:06.765+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T19:57:07.299+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:57:07.325+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:57:07.324+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T19:57:07.349+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:57:07.349+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T19:57:07.368+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.637 seconds
[2024-07-24T19:57:37.529+0000] {processor.py:157} INFO - Started process (PID=1131) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:57:37.530+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T19:57:37.532+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:57:37.531+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:57:37.560+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T19:57:37.560+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T19:57:38.102+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:57:38.122+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:57:38.122+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T19:57:38.143+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:57:38.143+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T19:57:38.159+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.632 seconds
[2024-07-24T19:58:08.311+0000] {processor.py:157} INFO - Started process (PID=1133) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:58:08.312+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T19:58:08.314+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:58:08.314+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:58:08.343+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T19:58:08.344+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T19:58:08.883+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:58:08.902+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:58:08.901+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T19:58:08.919+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:58:08.919+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T19:58:08.932+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.623 seconds
[2024-07-24T19:58:39.100+0000] {processor.py:157} INFO - Started process (PID=1135) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:58:39.101+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T19:58:39.103+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:58:39.102+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:58:39.131+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T19:58:39.132+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T19:58:39.656+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:58:39.674+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:58:39.673+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T19:58:39.693+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:58:39.692+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T19:58:39.706+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.609 seconds
[2024-07-24T19:59:09.861+0000] {processor.py:157} INFO - Started process (PID=1137) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:59:09.862+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T19:59:09.864+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:59:09.864+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:59:09.893+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T19:59:09.894+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T19:59:10.442+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:59:10.462+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:59:10.461+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T19:59:10.485+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:59:10.485+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T19:59:10.500+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.642 seconds
[2024-07-24T19:59:40.672+0000] {processor.py:157} INFO - Started process (PID=1139) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:59:40.673+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T19:59:40.675+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:59:40.675+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:59:40.706+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T19:59:40.706+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T19:59:41.318+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T19:59:41.338+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:59:41.338+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T19:59:41.357+0000] {logging_mixin.py:151} INFO - [2024-07-24T19:59:41.357+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T19:59:41.370+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.701 seconds
[2024-07-24T20:00:11.526+0000] {processor.py:157} INFO - Started process (PID=1141) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:00:11.527+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T20:00:11.530+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:00:11.529+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:00:11.559+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T20:00:11.560+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T20:00:12.145+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:00:12.167+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:00:12.166+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T20:00:12.192+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:00:12.191+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T20:00:12.210+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.686 seconds
[2024-07-24T20:00:42.283+0000] {processor.py:157} INFO - Started process (PID=1143) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:00:42.284+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T20:00:42.286+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:00:42.286+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:00:42.320+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T20:00:42.321+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T20:00:42.898+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:00:42.919+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:00:42.918+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T20:00:42.940+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:00:42.940+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T20:00:42.956+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.677 seconds
[2024-07-24T20:01:13.119+0000] {processor.py:157} INFO - Started process (PID=1145) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:01:13.120+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T20:01:13.122+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:01:13.122+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:01:13.152+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T20:01:13.153+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T20:01:13.732+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:01:13.750+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:01:13.750+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T20:01:13.768+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:01:13.768+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T20:01:13.788+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.672 seconds
[2024-07-24T20:01:43.947+0000] {processor.py:157} INFO - Started process (PID=1147) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:01:43.948+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T20:01:43.950+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:01:43.950+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:01:43.981+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T20:01:43.982+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T20:01:44.577+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:01:44.597+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:01:44.596+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T20:01:44.617+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:01:44.617+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T20:01:44.632+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.688 seconds
[2024-07-24T20:02:14.824+0000] {processor.py:157} INFO - Started process (PID=1149) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:02:14.825+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T20:02:14.828+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:02:14.828+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:02:14.864+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T20:02:14.865+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T20:02:15.478+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:02:15.501+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:02:15.500+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T20:02:15.522+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:02:15.521+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T20:02:15.539+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.719 seconds
[2024-07-24T20:02:45.717+0000] {processor.py:157} INFO - Started process (PID=1151) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:02:45.718+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T20:02:45.720+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:02:45.720+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:02:45.748+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T20:02:45.749+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T20:02:46.362+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:02:46.382+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:02:46.382+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T20:02:46.400+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:02:46.400+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T20:02:46.415+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.701 seconds
[2024-07-24T20:03:16.576+0000] {processor.py:157} INFO - Started process (PID=1153) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:03:16.577+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T20:03:16.578+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:03:16.578+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:03:16.608+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T20:03:16.609+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T20:03:17.217+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:03:17.234+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:03:17.234+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T20:03:17.251+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:03:17.251+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T20:03:17.265+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.692 seconds
[2024-07-24T20:03:47.435+0000] {processor.py:157} INFO - Started process (PID=1155) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:03:47.436+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T20:03:47.438+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:03:47.437+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:03:47.468+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T20:03:47.469+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T20:03:48.019+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:03:48.038+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:03:48.037+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T20:03:48.057+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:03:48.057+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T20:03:48.071+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.639 seconds
[2024-07-24T20:04:18.227+0000] {processor.py:157} INFO - Started process (PID=1157) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:04:18.228+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T20:04:18.229+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:04:18.229+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:04:18.259+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T20:04:18.259+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T20:04:18.832+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:04:18.851+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:04:18.851+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T20:04:18.874+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:04:18.874+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T20:04:18.890+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.666 seconds
[2024-07-24T20:04:48.978+0000] {processor.py:157} INFO - Started process (PID=1159) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:04:48.979+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T20:04:48.980+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:04:48.980+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:04:49.009+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T20:04:49.009+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T20:04:49.532+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:04:49.549+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:04:49.549+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T20:04:49.566+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:04:49.566+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T20:04:49.580+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.605 seconds
[2024-07-24T20:05:19.736+0000] {processor.py:157} INFO - Started process (PID=1161) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:05:19.737+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T20:05:19.739+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:05:19.739+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:05:19.769+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T20:05:19.769+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T20:05:20.307+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:05:20.328+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:05:20.327+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T20:05:20.346+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:05:20.346+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T20:05:20.369+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.636 seconds
[2024-07-24T20:05:50.519+0000] {processor.py:157} INFO - Started process (PID=1163) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:05:50.520+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T20:05:50.522+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:05:50.522+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:05:50.550+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T20:05:50.551+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T20:05:51.132+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:05:51.151+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:05:51.151+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T20:05:51.169+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:05:51.169+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T20:05:51.185+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.669 seconds
[2024-07-24T20:06:21.359+0000] {processor.py:157} INFO - Started process (PID=1165) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:06:21.360+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T20:06:21.363+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:06:21.362+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:06:21.394+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T20:06:21.395+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T20:06:21.946+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:06:21.963+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:06:21.962+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T20:06:21.980+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:06:21.980+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T20:06:21.994+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.637 seconds
[2024-07-24T20:06:52.150+0000] {processor.py:157} INFO - Started process (PID=1167) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:06:52.151+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T20:06:52.152+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:06:52.152+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:06:52.183+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T20:06:52.184+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T20:06:52.782+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:06:52.802+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:06:52.801+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T20:06:52.824+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:06:52.824+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T20:06:52.839+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.693 seconds
[2024-07-24T20:07:22.958+0000] {processor.py:157} INFO - Started process (PID=1169) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:07:22.959+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T20:07:22.960+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:07:22.960+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:07:22.990+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T20:07:22.991+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T20:07:23.529+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:07:23.547+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:07:23.547+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T20:07:23.565+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:07:23.565+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T20:07:23.578+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.623 seconds
[2024-07-24T20:07:53.746+0000] {processor.py:157} INFO - Started process (PID=1171) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:07:53.746+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T20:07:53.748+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:07:53.748+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:07:53.776+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T20:07:53.776+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T20:07:54.306+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:07:54.325+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:07:54.325+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T20:07:54.343+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:07:54.343+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T20:07:54.357+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.615 seconds
[2024-07-24T20:08:24.515+0000] {processor.py:157} INFO - Started process (PID=1173) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:08:24.516+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T20:08:24.518+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:08:24.518+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:08:24.548+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T20:08:24.549+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T20:08:25.070+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:08:25.091+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:08:25.091+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T20:08:25.111+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:08:25.110+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T20:08:25.126+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.614 seconds
[2024-07-24T20:08:55.300+0000] {processor.py:157} INFO - Started process (PID=1175) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:08:55.301+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T20:08:55.303+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:08:55.303+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:08:55.336+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T20:08:55.336+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T20:08:55.907+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:08:55.924+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:08:55.924+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T20:08:55.941+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:08:55.941+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T20:08:55.955+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.658 seconds
[2024-07-24T20:09:26.116+0000] {processor.py:157} INFO - Started process (PID=1177) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:09:26.117+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T20:09:26.119+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:09:26.119+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:09:26.152+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T20:09:26.152+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T20:09:26.711+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:09:26.728+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:09:26.728+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T20:09:26.748+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:09:26.747+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T20:09:26.761+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.648 seconds
[2024-07-24T20:09:56.926+0000] {processor.py:157} INFO - Started process (PID=1179) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:09:56.926+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T20:09:56.930+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:09:56.930+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:09:56.959+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T20:09:56.959+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T20:09:57.512+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:09:57.530+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:09:57.530+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T20:09:57.548+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:09:57.548+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T20:09:57.562+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.639 seconds
[2024-07-24T20:10:27.742+0000] {processor.py:157} INFO - Started process (PID=1181) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:10:27.743+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T20:10:27.745+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:10:27.745+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:10:27.787+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T20:10:27.787+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T20:10:28.422+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:10:28.447+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:10:28.447+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T20:10:28.476+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:10:28.476+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T20:10:28.493+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.754 seconds
[2024-07-24T20:10:58.590+0000] {processor.py:157} INFO - Started process (PID=1183) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:10:58.591+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T20:10:58.593+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:10:58.592+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:10:58.623+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T20:10:58.624+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T20:10:59.281+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:10:59.309+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:10:59.308+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T20:10:59.332+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:10:59.332+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T20:10:59.348+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.761 seconds
[2024-07-24T20:11:29.524+0000] {processor.py:157} INFO - Started process (PID=1185) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:11:29.525+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T20:11:29.527+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:11:29.527+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:11:29.557+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T20:11:29.558+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T20:11:30.152+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:11:30.172+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:11:30.172+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T20:11:30.190+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:11:30.190+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T20:11:30.205+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.684 seconds
[2024-07-24T20:12:00.371+0000] {processor.py:157} INFO - Started process (PID=1187) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:12:00.372+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T20:12:00.374+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:12:00.374+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:12:00.403+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T20:12:00.404+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T20:12:00.922+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:12:00.942+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:12:00.941+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T20:12:00.959+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:12:00.959+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T20:12:00.973+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.606 seconds
[2024-07-24T20:12:31.097+0000] {processor.py:157} INFO - Started process (PID=1189) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:12:31.098+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T20:12:31.099+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:12:31.099+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:12:31.129+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T20:12:31.129+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T20:12:31.690+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:12:31.711+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:12:31.710+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T20:12:31.732+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:12:31.731+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T20:12:31.748+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.653 seconds
[2024-07-24T20:13:01.926+0000] {processor.py:157} INFO - Started process (PID=1191) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:13:01.927+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T20:13:01.929+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:13:01.929+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:13:01.960+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T20:13:01.961+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T20:13:02.560+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:13:02.583+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:13:02.582+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T20:13:02.606+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:13:02.606+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T20:13:02.624+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.700 seconds
[2024-07-24T20:13:32.794+0000] {processor.py:157} INFO - Started process (PID=1193) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:13:32.795+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T20:13:32.798+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:13:32.798+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:13:32.829+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T20:13:32.830+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T20:13:33.379+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:13:33.403+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:13:33.402+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T20:13:33.420+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:13:33.420+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T20:13:33.434+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.643 seconds
[2024-07-24T20:14:03.561+0000] {processor.py:157} INFO - Started process (PID=1195) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:14:03.561+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T20:14:03.563+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:14:03.563+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:14:03.591+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T20:14:03.592+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T20:14:04.118+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:14:04.138+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:14:04.138+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T20:14:04.155+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:14:04.154+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T20:14:04.167+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.610 seconds
[2024-07-24T20:14:34.337+0000] {processor.py:157} INFO - Started process (PID=1197) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:14:34.337+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T20:14:34.339+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:14:34.339+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:14:34.369+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T20:14:34.369+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T20:14:34.898+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:14:34.916+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:14:34.915+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T20:14:34.933+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:14:34.932+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T20:14:34.946+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.612 seconds
[2024-07-24T20:15:05.102+0000] {processor.py:157} INFO - Started process (PID=1199) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:15:05.102+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T20:15:05.104+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:15:05.104+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:15:05.133+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T20:15:05.133+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T20:15:05.696+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:15:05.714+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:15:05.713+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T20:15:05.731+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:15:05.731+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T20:15:05.747+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.648 seconds
[2024-07-24T20:15:35.915+0000] {processor.py:157} INFO - Started process (PID=1201) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:15:35.916+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T20:15:35.919+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:15:35.918+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:15:35.951+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T20:15:35.951+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T20:15:36.527+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:15:36.545+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:15:36.545+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T20:15:36.563+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:15:36.563+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T20:15:36.583+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.670 seconds
[2024-07-24T20:16:06.746+0000] {processor.py:157} INFO - Started process (PID=1203) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:16:06.747+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T20:16:06.749+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:16:06.749+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:16:06.781+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T20:16:06.782+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T20:16:07.361+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:16:07.380+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:16:07.380+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T20:16:07.400+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:16:07.400+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T20:16:07.414+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.670 seconds
[2024-07-24T20:16:37.568+0000] {processor.py:157} INFO - Started process (PID=1205) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:16:37.569+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T20:16:37.576+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:16:37.576+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:16:37.609+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T20:16:37.610+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T20:16:38.229+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:16:38.250+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:16:38.250+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T20:16:38.272+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:16:38.272+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T20:16:38.292+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.728 seconds
[2024-07-24T20:17:08.497+0000] {processor.py:157} INFO - Started process (PID=1207) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:17:08.498+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T20:17:08.500+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:17:08.500+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:17:08.530+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T20:17:08.531+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T20:17:09.115+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:17:09.146+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:17:09.145+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T20:17:09.168+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:17:09.168+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T20:17:09.185+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.691 seconds
[2024-07-24T20:17:39.350+0000] {processor.py:157} INFO - Started process (PID=1209) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:17:39.351+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T20:17:39.352+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:17:39.352+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:17:39.382+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T20:17:39.383+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T20:17:39.920+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:17:39.937+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:17:39.937+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T20:17:39.956+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:17:39.955+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T20:17:39.971+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.624 seconds
[2024-07-24T20:18:10.128+0000] {processor.py:157} INFO - Started process (PID=1211) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:18:10.129+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T20:18:10.131+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:18:10.131+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:18:10.160+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T20:18:10.160+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T20:18:10.677+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:18:10.695+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:18:10.694+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T20:18:10.719+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:18:10.719+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T20:18:10.737+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.611 seconds
[2024-07-24T20:18:40.903+0000] {processor.py:157} INFO - Started process (PID=1213) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:18:40.904+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T20:18:40.906+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:18:40.906+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:18:40.934+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T20:18:40.934+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T20:18:41.496+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:18:41.516+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:18:41.516+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T20:18:41.548+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:18:41.548+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T20:18:41.565+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.665 seconds
[2024-07-24T20:19:11.726+0000] {processor.py:157} INFO - Started process (PID=1215) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:19:11.727+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T20:19:11.729+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:19:11.729+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:19:11.760+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T20:19:11.761+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T20:19:12.353+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:19:12.379+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:19:12.378+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T20:19:12.402+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:19:12.402+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T20:19:12.420+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.697 seconds
[2024-07-24T20:19:42.545+0000] {processor.py:157} INFO - Started process (PID=1217) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:19:42.546+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T20:19:42.549+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:19:42.549+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:19:42.581+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T20:19:42.581+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T20:19:43.154+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:19:43.177+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:19:43.176+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T20:19:43.197+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:19:43.197+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T20:19:43.212+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.671 seconds
[2024-07-24T20:20:13.380+0000] {processor.py:157} INFO - Started process (PID=1219) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:20:13.381+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T20:20:13.382+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:20:13.382+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:20:13.412+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T20:20:13.412+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T20:20:13.958+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:20:13.976+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:20:13.975+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T20:20:13.994+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:20:13.993+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T20:20:14.009+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.632 seconds
[2024-07-24T20:20:44.166+0000] {processor.py:157} INFO - Started process (PID=1221) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:20:44.167+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T20:20:44.169+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:20:44.168+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:20:44.197+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T20:20:44.198+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T20:20:44.812+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:20:44.831+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:20:44.830+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T20:20:44.848+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:20:44.848+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T20:20:44.863+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.700 seconds
[2024-07-24T20:21:14.996+0000] {processor.py:157} INFO - Started process (PID=1223) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:21:14.997+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T20:21:14.999+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:21:14.999+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:21:15.029+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T20:21:15.029+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T20:21:15.533+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:21:15.550+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:21:15.550+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T20:21:15.568+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:21:15.568+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T20:21:15.590+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.596 seconds
[2024-07-24T20:21:45.776+0000] {processor.py:157} INFO - Started process (PID=1225) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:21:45.777+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T20:21:45.779+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:21:45.779+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:21:45.810+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T20:21:45.810+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T20:21:46.382+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:21:46.402+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:21:46.402+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T20:21:46.421+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:21:46.421+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T20:21:46.435+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.662 seconds
[2024-07-24T20:22:16.602+0000] {processor.py:157} INFO - Started process (PID=1227) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:22:16.603+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T20:22:16.605+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:22:16.605+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:22:16.637+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T20:22:16.637+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T20:22:17.216+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:22:17.239+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:22:17.238+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T20:22:17.257+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:22:17.257+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T20:22:17.273+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.674 seconds
[2024-07-24T20:22:47.426+0000] {processor.py:157} INFO - Started process (PID=1229) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:22:47.427+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T20:22:47.429+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:22:47.429+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:22:47.460+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T20:22:47.460+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T20:22:48.056+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:22:48.078+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:22:48.077+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T20:22:48.098+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:22:48.097+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T20:22:48.115+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.691 seconds
[2024-07-24T20:23:18.281+0000] {processor.py:157} INFO - Started process (PID=1231) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:23:18.282+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T20:23:18.284+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:23:18.283+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:23:18.313+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T20:23:18.313+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T20:23:18.826+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:23:18.843+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:23:18.843+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T20:23:18.860+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:23:18.860+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T20:23:18.877+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.598 seconds
[2024-07-24T20:23:49.034+0000] {processor.py:157} INFO - Started process (PID=1233) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:23:49.035+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T20:23:49.036+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:23:49.036+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:23:49.065+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T20:23:49.065+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T20:23:49.612+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:23:49.631+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:23:49.630+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T20:23:49.650+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:23:49.650+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T20:23:49.664+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.633 seconds
[2024-07-24T20:24:19.835+0000] {processor.py:157} INFO - Started process (PID=1235) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:24:19.836+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T20:24:19.838+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:24:19.838+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:24:19.868+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T20:24:19.868+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T20:24:20.449+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:24:20.473+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:24:20.472+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T20:24:20.498+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:24:20.498+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T20:24:20.516+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.683 seconds
[2024-07-24T20:24:50.677+0000] {processor.py:157} INFO - Started process (PID=1237) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:24:50.678+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T20:24:50.680+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:24:50.679+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:24:50.710+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T20:24:50.711+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T20:24:51.260+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:24:51.283+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:24:51.282+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T20:24:51.307+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:24:51.306+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T20:24:51.325+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.652 seconds
[2024-07-24T20:25:21.460+0000] {processor.py:157} INFO - Started process (PID=1239) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:25:21.461+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T20:25:21.463+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:25:21.463+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:25:21.494+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T20:25:21.494+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T20:25:22.079+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:25:22.100+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:25:22.099+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T20:25:22.120+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:25:22.120+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T20:25:22.136+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.678 seconds
[2024-07-24T20:25:52.326+0000] {processor.py:157} INFO - Started process (PID=1241) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:25:52.327+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T20:25:52.330+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:25:52.330+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:25:52.363+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T20:25:52.363+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T20:25:52.951+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:25:52.970+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:25:52.970+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T20:25:52.990+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:25:52.990+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T20:25:53.005+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.683 seconds
[2024-07-24T20:26:23.172+0000] {processor.py:157} INFO - Started process (PID=1243) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:26:23.173+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T20:26:23.176+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:26:23.175+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:26:23.209+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T20:26:23.209+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T20:26:23.817+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:26:23.837+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:26:23.836+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T20:26:23.857+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:26:23.856+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T20:26:23.874+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.705 seconds
[2024-07-24T20:26:53.947+0000] {processor.py:157} INFO - Started process (PID=1245) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:26:53.948+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T20:26:53.950+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:26:53.949+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:26:53.980+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T20:26:53.980+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T20:26:54.527+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:26:54.547+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:26:54.547+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T20:26:54.568+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:26:54.568+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T20:26:54.584+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.640 seconds
[2024-07-24T20:27:24.757+0000] {processor.py:157} INFO - Started process (PID=1247) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:27:24.758+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T20:27:24.760+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:27:24.760+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:27:24.791+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T20:27:24.792+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T20:27:25.390+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:27:25.415+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:27:25.414+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T20:27:25.447+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:27:25.446+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T20:27:25.467+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.714 seconds
[2024-07-24T20:27:55.636+0000] {processor.py:157} INFO - Started process (PID=1249) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:27:55.637+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T20:27:55.639+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:27:55.639+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:27:55.672+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T20:27:55.672+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T20:27:56.268+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:27:56.290+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:27:56.290+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T20:27:56.309+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:27:56.309+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T20:27:56.324+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.691 seconds
[2024-07-24T20:28:26.479+0000] {processor.py:157} INFO - Started process (PID=1251) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:28:26.480+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T20:28:26.484+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:28:26.484+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:28:26.531+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T20:28:26.532+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T20:28:27.388+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:28:27.417+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:28:27.416+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T20:28:27.449+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:28:27.449+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T20:28:27.480+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 1.004 seconds
[2024-07-24T20:28:57.670+0000] {processor.py:157} INFO - Started process (PID=1253) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:28:57.671+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T20:28:57.675+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:28:57.674+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:28:57.711+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T20:28:57.712+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T20:28:58.559+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:28:58.594+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:28:58.594+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T20:28:58.628+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:28:58.628+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T20:28:58.655+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.989 seconds
[2024-07-24T20:29:28.833+0000] {processor.py:157} INFO - Started process (PID=1255) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:29:28.834+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T20:29:28.836+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:29:28.836+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:29:28.867+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T20:29:28.868+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T20:29:29.474+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:29:29.502+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:29:29.501+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T20:29:29.532+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:29:29.532+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T20:29:29.550+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.720 seconds
[2024-07-24T20:29:59.731+0000] {processor.py:157} INFO - Started process (PID=1257) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:29:59.732+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T20:29:59.734+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:29:59.733+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:29:59.764+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T20:29:59.765+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T20:30:00.329+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:30:00.348+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:30:00.348+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T20:30:00.368+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:30:00.368+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T20:30:00.383+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.655 seconds
[2024-07-24T20:30:30.547+0000] {processor.py:157} INFO - Started process (PID=1259) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:30:30.548+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T20:30:30.550+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:30:30.550+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:30:30.583+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T20:30:30.583+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T20:30:31.144+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:30:31.166+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:30:31.166+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T20:30:31.189+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:30:31.188+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T20:30:31.204+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.660 seconds
[2024-07-24T20:31:01.262+0000] {processor.py:157} INFO - Started process (PID=1261) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:31:01.263+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T20:31:01.265+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:31:01.265+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:31:01.295+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T20:31:01.296+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T20:31:01.892+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:31:01.917+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:31:01.916+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T20:31:01.939+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:31:01.939+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T20:31:01.955+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.696 seconds
[2024-07-24T20:31:32.123+0000] {processor.py:157} INFO - Started process (PID=1263) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:31:32.124+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T20:31:32.126+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:31:32.126+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:31:32.159+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T20:31:32.160+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T20:31:32.758+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:31:32.779+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:31:32.779+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T20:31:32.797+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:31:32.797+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T20:31:32.812+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.692 seconds
[2024-07-24T20:32:02.993+0000] {processor.py:157} INFO - Started process (PID=1265) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:32:02.994+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T20:32:02.996+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:32:02.996+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:32:03.030+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T20:32:03.030+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T20:32:03.601+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:32:03.625+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:32:03.625+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T20:32:03.644+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:32:03.644+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T20:32:03.658+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.668 seconds
[2024-07-24T20:32:33.756+0000] {processor.py:157} INFO - Started process (PID=1267) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:32:33.757+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T20:32:33.759+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:32:33.759+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:32:33.788+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T20:32:33.789+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T20:32:34.318+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:32:34.337+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:32:34.336+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T20:32:34.355+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:32:34.355+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T20:32:34.368+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.615 seconds
[2024-07-24T20:33:04.531+0000] {processor.py:157} INFO - Started process (PID=1269) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:33:04.532+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T20:33:04.533+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:33:04.533+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:33:04.561+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T20:33:04.561+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T20:33:05.125+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:33:05.151+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:33:05.151+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T20:33:05.174+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:33:05.174+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T20:33:05.189+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.661 seconds
[2024-07-24T20:33:35.348+0000] {processor.py:157} INFO - Started process (PID=1271) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:33:35.349+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T20:33:35.350+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:33:35.350+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:33:35.383+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T20:33:35.383+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T20:33:35.951+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:33:35.969+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:33:35.968+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T20:33:35.986+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:33:35.986+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T20:33:36.002+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.658 seconds
[2024-07-24T20:34:06.184+0000] {processor.py:157} INFO - Started process (PID=1273) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:34:06.185+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T20:34:06.187+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:34:06.186+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:34:06.217+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T20:34:06.217+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T20:34:06.788+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:34:06.807+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:34:06.807+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T20:34:06.826+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:34:06.825+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T20:34:06.850+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.669 seconds
[2024-07-24T20:34:37.022+0000] {processor.py:157} INFO - Started process (PID=1275) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:34:37.023+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T20:34:37.025+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:34:37.025+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:34:37.053+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T20:34:37.053+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T20:34:37.640+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:34:37.660+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:34:37.659+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T20:34:37.684+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:34:37.684+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T20:34:37.700+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.680 seconds
[2024-07-24T20:35:07.776+0000] {processor.py:157} INFO - Started process (PID=1277) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:35:07.776+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T20:35:07.779+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:35:07.778+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:35:07.808+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T20:35:07.808+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T20:35:08.385+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:35:08.406+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:35:08.405+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T20:35:08.426+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:35:08.426+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T20:35:08.442+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.669 seconds
[2024-07-24T20:35:38.608+0000] {processor.py:157} INFO - Started process (PID=1279) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:35:38.609+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T20:35:38.612+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:35:38.611+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:35:38.642+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T20:35:38.642+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T20:35:39.198+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:35:39.216+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:35:39.216+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T20:35:39.234+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:35:39.234+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T20:35:39.248+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.643 seconds
[2024-07-24T20:36:09.401+0000] {processor.py:157} INFO - Started process (PID=1281) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:36:09.402+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T20:36:09.404+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:36:09.403+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:36:09.432+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T20:36:09.433+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T20:36:09.949+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:36:09.967+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:36:09.966+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T20:36:09.989+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:36:09.988+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T20:36:10.005+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.607 seconds
[2024-07-24T20:36:40.192+0000] {processor.py:157} INFO - Started process (PID=1283) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:36:40.192+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T20:36:40.194+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:36:40.194+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:36:40.222+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T20:36:40.223+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T20:36:40.748+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:36:40.768+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:36:40.768+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T20:36:40.789+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:36:40.789+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T20:36:40.807+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.618 seconds
[2024-07-24T20:37:10.967+0000] {processor.py:157} INFO - Started process (PID=1285) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:37:10.968+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T20:37:10.970+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:37:10.970+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:37:11.002+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T20:37:11.002+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T20:37:11.586+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:37:11.606+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:37:11.606+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T20:37:11.625+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:37:11.625+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T20:37:11.639+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.674 seconds
[2024-07-24T20:37:41.811+0000] {processor.py:157} INFO - Started process (PID=1287) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:37:41.812+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T20:37:41.815+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:37:41.815+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:37:41.845+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T20:37:41.846+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T20:37:42.407+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:37:42.425+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:37:42.425+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T20:37:42.444+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:37:42.444+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T20:37:42.470+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.661 seconds
[2024-07-24T20:38:12.631+0000] {processor.py:157} INFO - Started process (PID=1289) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:38:12.632+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T20:38:12.635+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:38:12.634+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:38:12.664+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T20:38:12.665+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T20:38:13.223+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:38:13.240+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:38:13.240+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T20:38:13.258+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:38:13.257+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T20:38:13.271+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.643 seconds
[2024-07-24T20:38:43.387+0000] {processor.py:157} INFO - Started process (PID=1291) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:38:43.387+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T20:38:43.390+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:38:43.389+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:38:43.419+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T20:38:43.420+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T20:38:44.031+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:38:44.053+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:38:44.052+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T20:38:44.073+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:38:44.073+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T20:38:44.089+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.706 seconds
[2024-07-24T20:39:14.262+0000] {processor.py:157} INFO - Started process (PID=1293) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:39:14.262+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T20:39:14.264+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:39:14.264+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:39:14.294+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T20:39:14.294+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T20:39:14.865+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:39:14.886+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:39:14.885+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T20:39:14.904+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:39:14.904+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T20:39:14.920+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.661 seconds
[2024-07-24T20:39:45.088+0000] {processor.py:157} INFO - Started process (PID=1295) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:39:45.089+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T20:39:45.090+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:39:45.090+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:39:45.119+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T20:39:45.119+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T20:39:45.664+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:39:45.685+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:39:45.684+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T20:39:45.703+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:39:45.703+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T20:39:45.717+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.632 seconds
[2024-07-24T20:40:15.829+0000] {processor.py:157} INFO - Started process (PID=1297) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:40:15.830+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T20:40:15.832+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:40:15.832+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:40:15.866+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T20:40:15.866+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T20:40:16.529+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:40:16.550+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:40:16.549+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T20:40:16.572+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:40:16.572+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T20:40:16.590+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.764 seconds
[2024-07-24T20:40:46.773+0000] {processor.py:157} INFO - Started process (PID=1299) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:40:46.774+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T20:40:46.777+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:40:46.776+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:40:46.813+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T20:40:46.814+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T20:40:47.473+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:40:47.499+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:40:47.498+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T20:40:47.521+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:40:47.521+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T20:40:47.539+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.770 seconds
[2024-07-24T20:41:17.710+0000] {processor.py:157} INFO - Started process (PID=1301) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:41:17.711+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T20:41:17.714+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:41:17.714+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:41:17.745+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T20:41:17.746+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T20:41:18.304+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:41:18.322+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:41:18.321+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T20:41:18.339+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:41:18.339+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T20:41:18.354+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.646 seconds
[2024-07-24T20:41:48.508+0000] {processor.py:157} INFO - Started process (PID=1303) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:41:48.508+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T20:41:48.510+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:41:48.510+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:41:48.539+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T20:41:48.540+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T20:41:49.065+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:41:49.083+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:41:49.082+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T20:41:49.101+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:41:49.101+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T20:41:49.115+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.611 seconds
[2024-07-24T20:42:19.291+0000] {processor.py:157} INFO - Started process (PID=1305) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:42:19.292+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T20:42:19.294+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:42:19.293+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:42:19.323+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T20:42:19.324+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T20:42:19.891+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:42:19.912+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:42:19.912+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T20:42:19.934+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:42:19.934+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T20:42:19.951+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.663 seconds
[2024-07-24T20:42:50.109+0000] {processor.py:157} INFO - Started process (PID=1307) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:42:50.110+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T20:42:50.112+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:42:50.112+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:42:50.142+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T20:42:50.142+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T20:42:50.671+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:42:50.690+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:42:50.690+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T20:42:50.710+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:42:50.709+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T20:42:50.734+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.628 seconds
[2024-07-24T20:43:20.905+0000] {processor.py:157} INFO - Started process (PID=1309) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:43:20.905+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T20:43:20.907+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:43:20.907+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:43:20.936+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T20:43:20.936+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T20:43:21.503+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:43:21.524+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:43:21.523+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T20:43:21.544+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:43:21.543+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T20:43:21.559+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.657 seconds
[2024-07-24T20:43:51.716+0000] {processor.py:157} INFO - Started process (PID=1311) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:43:51.716+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T20:43:51.718+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:43:51.718+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:43:51.748+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T20:43:51.749+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T20:43:52.299+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:43:52.320+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:43:52.319+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T20:43:52.344+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:43:52.344+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T20:43:52.360+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.647 seconds
[2024-07-24T20:44:22.530+0000] {processor.py:157} INFO - Started process (PID=1313) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:44:22.531+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T20:44:22.534+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:44:22.534+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:44:22.564+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T20:44:22.565+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T20:44:23.140+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:44:23.161+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:44:23.161+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T20:44:23.181+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:44:23.180+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T20:44:23.201+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.673 seconds
[2024-07-24T20:44:53.367+0000] {processor.py:157} INFO - Started process (PID=1315) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:44:53.368+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T20:44:53.370+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:44:53.370+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:44:53.401+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T20:44:53.401+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T20:44:53.955+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:44:53.974+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:44:53.973+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T20:44:53.995+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:44:53.995+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T20:44:54.011+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.646 seconds
[2024-07-24T20:45:24.170+0000] {processor.py:157} INFO - Started process (PID=1317) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:45:24.171+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T20:45:24.174+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:45:24.173+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:45:24.204+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T20:45:24.205+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T20:45:24.701+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:45:24.721+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:45:24.720+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T20:45:24.740+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:45:24.740+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T20:45:24.753+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.586 seconds
[2024-07-24T20:45:54.920+0000] {processor.py:157} INFO - Started process (PID=1319) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:45:54.921+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T20:45:54.923+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:45:54.923+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:45:54.952+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T20:45:54.952+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T20:45:55.469+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:45:55.491+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:45:55.491+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T20:45:55.511+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:45:55.510+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T20:45:55.528+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.611 seconds
[2024-07-24T20:46:25.708+0000] {processor.py:157} INFO - Started process (PID=1321) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:46:25.709+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T20:46:25.711+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:46:25.710+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:46:25.745+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T20:46:25.745+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T20:46:26.393+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:46:26.416+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:46:26.415+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T20:46:26.436+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:46:26.436+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T20:46:26.456+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.752 seconds
[2024-07-24T20:46:56.631+0000] {processor.py:157} INFO - Started process (PID=1323) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:46:56.632+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T20:46:56.634+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:46:56.634+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:46:56.665+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T20:46:56.665+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T20:46:57.331+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:46:57.349+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:46:57.349+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T20:46:57.372+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:46:57.372+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T20:46:57.390+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.761 seconds
[2024-07-24T20:47:27.576+0000] {processor.py:157} INFO - Started process (PID=1325) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:47:27.577+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T20:47:27.580+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:47:27.580+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:47:27.612+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T20:47:27.612+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T20:47:28.152+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:47:28.169+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:47:28.168+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T20:47:28.187+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:47:28.187+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T20:47:28.204+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.631 seconds
[2024-07-24T20:47:58.364+0000] {processor.py:157} INFO - Started process (PID=1327) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:47:58.364+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T20:47:58.367+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:47:58.366+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:47:58.398+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T20:47:58.399+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T20:47:59.018+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:47:59.046+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:47:59.046+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T20:47:59.073+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:47:59.072+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T20:47:59.091+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.731 seconds
[2024-07-24T20:48:29.246+0000] {processor.py:157} INFO - Started process (PID=1329) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:48:29.247+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T20:48:29.249+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:48:29.248+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:48:29.281+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T20:48:29.281+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T20:48:29.844+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:48:29.866+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:48:29.866+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T20:48:29.885+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:48:29.885+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T20:48:29.903+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.660 seconds
[2024-07-24T20:49:00.074+0000] {processor.py:157} INFO - Started process (PID=1331) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:49:00.075+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T20:49:00.076+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:49:00.076+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:49:00.104+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T20:49:00.105+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T20:49:00.627+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:49:00.647+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:49:00.646+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T20:49:00.666+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:49:00.666+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T20:49:00.680+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.608 seconds
[2024-07-24T20:49:30.841+0000] {processor.py:157} INFO - Started process (PID=1333) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:49:30.842+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T20:49:30.843+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:49:30.843+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:49:30.872+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T20:49:30.873+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T20:49:31.462+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:49:31.484+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:49:31.483+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T20:49:31.505+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:49:31.505+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T20:49:31.522+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.684 seconds
[2024-07-24T20:50:01.672+0000] {processor.py:157} INFO - Started process (PID=1335) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:50:01.673+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T20:50:01.675+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:50:01.674+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:50:01.707+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T20:50:01.707+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T20:50:02.488+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:50:02.507+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:50:02.506+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T20:50:02.529+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:50:02.529+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T20:50:02.548+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.879 seconds
[2024-07-24T20:50:32.726+0000] {processor.py:157} INFO - Started process (PID=1337) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:50:32.727+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T20:50:32.729+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:50:32.729+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:50:32.758+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T20:50:32.759+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T20:50:33.311+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:50:33.336+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:50:33.336+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T20:50:33.356+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:50:33.356+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T20:50:33.371+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.647 seconds
[2024-07-24T20:51:03.536+0000] {processor.py:157} INFO - Started process (PID=1339) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:51:03.537+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T20:51:03.540+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:51:03.539+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:51:03.570+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T20:51:03.571+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T20:51:04.217+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:51:04.237+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:51:04.237+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T20:51:04.258+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:51:04.258+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T20:51:04.273+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.740 seconds
[2024-07-24T20:51:34.436+0000] {processor.py:157} INFO - Started process (PID=1341) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:51:34.437+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T20:51:34.439+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:51:34.439+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:51:34.467+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T20:51:34.468+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T20:51:34.993+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:51:35.011+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:51:35.011+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T20:51:35.028+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:51:35.028+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T20:51:35.047+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.613 seconds
[2024-07-24T20:52:05.239+0000] {processor.py:157} INFO - Started process (PID=1343) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:52:05.240+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T20:52:05.241+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:52:05.241+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:52:05.270+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T20:52:05.270+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T20:52:05.905+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:52:05.925+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:52:05.924+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T20:52:05.944+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:52:05.944+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T20:52:05.960+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.724 seconds
[2024-07-24T20:52:36.125+0000] {processor.py:157} INFO - Started process (PID=1345) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:52:36.125+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T20:52:36.127+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:52:36.127+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:52:36.156+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T20:52:36.157+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T20:52:36.735+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:52:36.755+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:52:36.754+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T20:52:36.774+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:52:36.774+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T20:52:36.789+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.667 seconds
[2024-07-24T20:53:06.867+0000] {processor.py:157} INFO - Started process (PID=1347) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:53:06.868+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T20:53:06.871+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:53:06.870+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:53:06.916+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T20:53:06.917+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T20:53:07.734+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:53:07.753+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:53:07.753+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T20:53:07.772+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:53:07.772+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T20:53:07.786+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.923 seconds
[2024-07-24T20:53:37.964+0000] {processor.py:157} INFO - Started process (PID=1349) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:53:37.965+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T20:53:37.967+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:53:37.967+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:53:37.995+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T20:53:37.996+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T20:53:38.604+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:53:38.624+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:53:38.624+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T20:53:38.643+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:53:38.643+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T20:53:38.658+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.697 seconds
[2024-07-24T20:54:08.822+0000] {processor.py:157} INFO - Started process (PID=1351) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:54:08.823+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T20:54:08.825+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:54:08.825+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:54:08.855+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T20:54:08.855+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T20:54:09.413+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:54:09.432+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:54:09.431+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T20:54:09.452+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:54:09.452+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T20:54:09.467+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.648 seconds
[2024-07-24T20:54:39.631+0000] {processor.py:157} INFO - Started process (PID=1353) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:54:39.632+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T20:54:39.634+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:54:39.634+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:54:39.661+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T20:54:39.662+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T20:54:40.162+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:54:40.181+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:54:40.180+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T20:54:40.199+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:54:40.199+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T20:54:40.214+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.585 seconds
[2024-07-24T20:55:10.386+0000] {processor.py:157} INFO - Started process (PID=1355) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:55:10.387+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T20:55:10.389+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:55:10.389+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:55:10.425+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T20:55:10.425+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T20:55:11.056+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:55:11.077+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:55:11.077+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T20:55:11.103+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:55:11.103+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T20:55:11.123+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.740 seconds
[2024-07-24T20:55:41.301+0000] {processor.py:157} INFO - Started process (PID=1357) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:55:41.302+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T20:55:41.305+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:55:41.305+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:55:41.345+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T20:55:41.346+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T20:55:42.075+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:55:42.099+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:55:42.099+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T20:55:42.123+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:55:42.123+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T20:55:42.150+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.853 seconds
[2024-07-24T20:56:12.314+0000] {processor.py:157} INFO - Started process (PID=1359) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:56:12.315+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T20:56:12.317+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:56:12.317+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:56:12.348+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T20:56:12.348+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T20:56:12.967+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:56:12.986+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:56:12.985+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T20:56:13.009+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:56:13.009+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T20:56:13.033+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.722 seconds
[2024-07-24T20:56:43.225+0000] {processor.py:157} INFO - Started process (PID=1361) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:56:43.226+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T20:56:43.228+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:56:43.228+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:56:43.258+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T20:56:43.259+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T20:56:43.783+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:56:43.801+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:56:43.800+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T20:56:43.819+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:56:43.818+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T20:56:43.834+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.612 seconds
[2024-07-24T20:57:13.998+0000] {processor.py:157} INFO - Started process (PID=1363) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:57:13.999+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T20:57:14.001+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:57:14.001+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:57:14.032+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T20:57:14.032+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T20:57:14.584+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:57:14.602+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:57:14.601+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T20:57:14.620+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:57:14.620+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T20:57:14.637+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.642 seconds
[2024-07-24T20:57:44.722+0000] {processor.py:157} INFO - Started process (PID=1365) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:57:44.723+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T20:57:44.725+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:57:44.725+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:57:44.753+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T20:57:44.754+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T20:57:45.326+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:57:45.344+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:57:45.344+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T20:57:45.362+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:57:45.362+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T20:57:45.378+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.658 seconds
[2024-07-24T20:58:15.541+0000] {processor.py:157} INFO - Started process (PID=1367) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:58:15.542+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T20:58:15.544+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:58:15.544+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:58:15.573+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T20:58:15.574+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T20:58:16.087+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:58:16.107+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:58:16.106+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T20:58:16.126+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:58:16.126+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T20:58:16.140+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.602 seconds
[2024-07-24T20:58:46.292+0000] {processor.py:157} INFO - Started process (PID=1369) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:58:46.293+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T20:58:46.294+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:58:46.294+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:58:46.324+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T20:58:46.325+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T20:58:46.849+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:58:46.865+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:58:46.864+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T20:58:46.882+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:58:46.882+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T20:58:46.900+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.611 seconds
[2024-07-24T20:59:17.060+0000] {processor.py:157} INFO - Started process (PID=1371) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:59:17.061+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T20:59:17.063+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:59:17.062+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:59:17.092+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T20:59:17.092+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T20:59:17.613+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:59:17.634+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:59:17.633+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T20:59:17.652+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:59:17.652+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T20:59:17.667+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.609 seconds
[2024-07-24T20:59:47.827+0000] {processor.py:157} INFO - Started process (PID=1373) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:59:47.828+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T20:59:47.830+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:59:47.830+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:59:47.862+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T20:59:47.862+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T20:59:48.425+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T20:59:48.444+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:59:48.444+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T20:59:48.462+0000] {logging_mixin.py:151} INFO - [2024-07-24T20:59:48.462+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T20:59:48.481+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.657 seconds
[2024-07-24T21:00:18.648+0000] {processor.py:157} INFO - Started process (PID=1375) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:00:18.649+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T21:00:18.651+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:00:18.651+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:00:18.680+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T21:00:18.681+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T21:00:19.202+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:00:19.222+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:00:19.222+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T21:00:19.242+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:00:19.242+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T21:00:19.260+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.615 seconds
[2024-07-24T21:00:49.414+0000] {processor.py:157} INFO - Started process (PID=1377) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:00:49.415+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T21:00:49.417+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:00:49.417+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:00:49.446+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T21:00:49.447+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T21:00:49.968+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:00:49.988+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:00:49.988+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T21:00:50.022+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:00:50.022+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T21:00:50.041+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.629 seconds
[2024-07-24T21:01:20.214+0000] {processor.py:157} INFO - Started process (PID=1379) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:01:20.215+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T21:01:20.217+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:01:20.216+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:01:20.246+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T21:01:20.246+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T21:01:20.808+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:01:20.836+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:01:20.835+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T21:01:20.860+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:01:20.860+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T21:01:20.874+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.663 seconds
[2024-07-24T21:01:51.035+0000] {processor.py:157} INFO - Started process (PID=1381) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:01:51.036+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T21:01:51.038+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:01:51.038+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:01:51.071+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T21:01:51.072+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T21:01:51.622+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:01:51.641+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:01:51.641+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T21:01:51.660+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:01:51.660+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T21:01:51.675+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.643 seconds
[2024-07-24T21:02:21.852+0000] {processor.py:157} INFO - Started process (PID=1383) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:02:21.852+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T21:02:21.854+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:02:21.854+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:02:21.884+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T21:02:21.885+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T21:02:22.460+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:02:22.481+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:02:22.480+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T21:02:22.500+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:02:22.500+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T21:02:22.521+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.672 seconds
[2024-07-24T21:02:52.683+0000] {processor.py:157} INFO - Started process (PID=1385) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:02:52.684+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T21:02:52.686+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:02:52.686+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:02:52.715+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T21:02:52.716+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T21:02:53.248+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:02:53.267+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:02:53.266+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T21:02:53.285+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:02:53.285+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T21:02:53.299+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.619 seconds
[2024-07-24T21:03:23.460+0000] {processor.py:157} INFO - Started process (PID=1387) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:03:23.461+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T21:03:23.464+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:03:23.464+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:03:23.500+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T21:03:23.500+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T21:03:24.078+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:03:24.100+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:03:24.099+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T21:03:24.120+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:03:24.120+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T21:03:24.138+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.681 seconds
[2024-07-24T21:03:54.280+0000] {processor.py:157} INFO - Started process (PID=1389) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:03:54.281+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T21:03:54.282+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:03:54.282+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:03:54.313+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T21:03:54.313+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T21:03:54.822+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:03:54.839+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:03:54.839+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T21:03:54.857+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:03:54.857+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T21:03:54.871+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.593 seconds
[2024-07-24T21:04:25.038+0000] {processor.py:157} INFO - Started process (PID=1391) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:04:25.038+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T21:04:25.040+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:04:25.040+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:04:25.068+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T21:04:25.068+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T21:04:25.590+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:04:25.607+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:04:25.607+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T21:04:25.625+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:04:25.625+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T21:04:25.638+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.603 seconds
[2024-07-24T21:04:55.797+0000] {processor.py:157} INFO - Started process (PID=1393) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:04:55.798+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T21:04:55.799+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:04:55.799+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:04:55.830+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T21:04:55.830+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T21:04:56.391+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:04:56.412+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:04:56.412+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T21:04:56.432+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:04:56.432+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T21:04:56.448+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.654 seconds
[2024-07-24T21:05:26.627+0000] {processor.py:157} INFO - Started process (PID=1395) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:05:26.628+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T21:05:26.630+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:05:26.629+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:05:26.661+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T21:05:26.662+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T21:05:27.307+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:05:27.330+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:05:27.330+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T21:05:27.351+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:05:27.351+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T21:05:27.366+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.742 seconds
[2024-07-24T21:05:57.535+0000] {processor.py:157} INFO - Started process (PID=1397) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:05:57.536+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T21:05:57.538+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:05:57.538+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:05:57.566+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T21:05:57.567+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T21:05:58.118+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:05:58.136+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:05:58.135+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T21:05:58.155+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:05:58.154+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T21:05:58.168+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.636 seconds
[2024-07-24T21:06:28.228+0000] {processor.py:157} INFO - Started process (PID=1399) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:06:28.229+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T21:06:28.232+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:06:28.232+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:06:28.263+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T21:06:28.264+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T21:06:28.880+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:06:28.899+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:06:28.899+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T21:06:28.919+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:06:28.919+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T21:06:28.937+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.712 seconds
[2024-07-24T21:06:59.128+0000] {processor.py:157} INFO - Started process (PID=1401) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:06:59.129+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T21:06:59.130+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:06:59.130+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:06:59.158+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T21:06:59.159+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T21:06:59.716+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:06:59.735+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:06:59.735+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T21:06:59.755+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:06:59.755+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T21:06:59.770+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.644 seconds
[2024-07-24T21:07:29.931+0000] {processor.py:157} INFO - Started process (PID=1403) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:07:29.932+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T21:07:29.933+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:07:29.933+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:07:29.964+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T21:07:29.965+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T21:07:30.533+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:07:30.553+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:07:30.552+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T21:07:30.573+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:07:30.573+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T21:07:30.589+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.661 seconds
[2024-07-24T21:08:00.752+0000] {processor.py:157} INFO - Started process (PID=1405) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:08:00.753+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T21:08:00.754+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:08:00.754+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:08:00.785+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T21:08:00.785+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T21:08:01.365+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:08:01.385+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:08:01.385+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T21:08:01.404+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:08:01.404+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T21:08:01.419+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.669 seconds
[2024-07-24T21:08:31.607+0000] {processor.py:157} INFO - Started process (PID=1407) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:08:31.608+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T21:08:31.610+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:08:31.610+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:08:31.640+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T21:08:31.641+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T21:08:32.233+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:08:32.254+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:08:32.253+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T21:08:32.273+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:08:32.272+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T21:08:32.287+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.683 seconds
[2024-07-24T21:09:02.454+0000] {processor.py:157} INFO - Started process (PID=1409) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:09:02.455+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T21:09:02.457+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:09:02.457+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:09:02.489+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T21:09:02.489+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T21:09:03.035+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:09:03.054+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:09:03.053+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T21:09:03.072+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:09:03.072+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T21:09:03.087+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.637 seconds
[2024-07-24T21:09:33.187+0000] {processor.py:157} INFO - Started process (PID=1411) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:09:33.187+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T21:09:33.189+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:09:33.189+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:09:33.219+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T21:09:33.219+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T21:09:33.765+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:09:33.784+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:09:33.783+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T21:09:33.805+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:09:33.804+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T21:09:33.831+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.647 seconds
[2024-07-24T21:10:03.999+0000] {processor.py:157} INFO - Started process (PID=1413) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:10:03.999+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T21:10:04.002+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:10:04.002+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:10:04.030+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T21:10:04.031+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T21:10:04.557+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:10:04.573+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:10:04.573+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T21:10:04.590+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:10:04.590+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T21:10:04.603+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.607 seconds
[2024-07-24T21:10:34.759+0000] {processor.py:157} INFO - Started process (PID=1415) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:10:34.760+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T21:10:34.761+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:10:34.761+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:10:34.788+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T21:10:34.789+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T21:10:35.312+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:10:35.332+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:10:35.331+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T21:10:35.348+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:10:35.348+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T21:10:35.362+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.605 seconds
[2024-07-24T21:11:05.537+0000] {processor.py:157} INFO - Started process (PID=1417) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:11:05.538+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T21:11:05.540+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:11:05.540+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:11:05.574+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T21:11:05.575+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T21:11:06.114+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:11:06.135+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:11:06.135+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T21:11:06.155+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:11:06.155+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T21:11:06.171+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.638 seconds
[2024-07-24T21:11:36.345+0000] {processor.py:157} INFO - Started process (PID=1419) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:11:36.346+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T21:11:36.347+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:11:36.347+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:11:36.379+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T21:11:36.380+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T21:11:36.990+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:11:37.008+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:11:37.008+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T21:11:37.026+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:11:37.026+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T21:11:37.041+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.699 seconds
[2024-07-24T21:12:07.211+0000] {processor.py:157} INFO - Started process (PID=1421) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:12:07.212+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T21:12:07.214+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:12:07.214+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:12:07.244+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T21:12:07.244+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T21:12:07.808+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:12:07.826+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:12:07.826+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T21:12:07.843+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:12:07.843+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T21:12:07.858+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.650 seconds
[2024-07-24T21:12:38.022+0000] {processor.py:157} INFO - Started process (PID=1423) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:12:38.022+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T21:12:38.025+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:12:38.025+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:12:38.054+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T21:12:38.054+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T21:12:38.586+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:12:38.608+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:12:38.607+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T21:12:38.625+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:12:38.625+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T21:12:38.638+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.619 seconds
[2024-07-24T21:13:08.792+0000] {processor.py:157} INFO - Started process (PID=1425) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:13:08.793+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T21:13:08.794+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:13:08.794+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:13:08.823+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T21:13:08.824+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T21:13:09.342+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:13:09.361+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:13:09.360+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T21:13:09.380+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:13:09.380+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T21:13:09.402+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.613 seconds
[2024-07-24T21:13:39.576+0000] {processor.py:157} INFO - Started process (PID=1427) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:13:39.577+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T21:13:39.578+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:13:39.578+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:13:39.610+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T21:13:39.611+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T21:13:40.180+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:13:40.200+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:13:40.199+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T21:13:40.218+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:13:40.217+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T21:13:40.235+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.662 seconds
[2024-07-24T21:14:10.394+0000] {processor.py:157} INFO - Started process (PID=1429) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:14:10.395+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T21:14:10.397+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:14:10.396+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:14:10.425+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T21:14:10.425+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T21:14:10.932+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:14:10.949+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:14:10.949+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T21:14:10.967+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:14:10.966+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T21:14:10.979+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.588 seconds
[2024-07-24T21:14:41.150+0000] {processor.py:157} INFO - Started process (PID=1431) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:14:41.151+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T21:14:41.152+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:14:41.152+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:14:41.185+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T21:14:41.185+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T21:14:41.770+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:14:41.790+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:14:41.789+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T21:14:41.808+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:14:41.808+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T21:14:41.823+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.676 seconds
[2024-07-24T21:15:11.976+0000] {processor.py:157} INFO - Started process (PID=1433) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:15:11.977+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T21:15:11.979+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:15:11.978+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:15:12.010+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T21:15:12.010+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T21:15:12.561+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:15:12.580+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:15:12.580+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T21:15:12.601+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:15:12.601+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T21:15:12.616+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.643 seconds
[2024-07-24T21:15:42.790+0000] {processor.py:157} INFO - Started process (PID=1435) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:15:42.791+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T21:15:42.793+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:15:42.793+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:15:42.823+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T21:15:42.823+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T21:15:43.389+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:15:43.408+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:15:43.408+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T21:15:43.428+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:15:43.428+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T21:15:43.443+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.655 seconds
[2024-07-24T21:16:13.618+0000] {processor.py:157} INFO - Started process (PID=1437) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:16:13.619+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T21:16:13.621+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:16:13.621+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:16:13.656+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T21:16:13.656+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T21:16:14.160+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:16:14.176+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:16:14.176+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T21:16:14.193+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:16:14.193+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T21:16:14.207+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.591 seconds
[2024-07-24T21:16:44.422+0000] {processor.py:157} INFO - Started process (PID=1439) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:16:44.423+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T21:16:44.424+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:16:44.424+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:16:44.456+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T21:16:44.456+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T21:16:44.993+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:16:45.011+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:16:45.011+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T21:16:45.029+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:16:45.029+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T21:16:45.044+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.625 seconds
[2024-07-24T21:17:15.213+0000] {processor.py:157} INFO - Started process (PID=1441) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:17:15.214+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T21:17:15.215+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:17:15.215+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:17:15.247+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T21:17:15.247+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T21:17:15.802+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:17:15.823+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:17:15.822+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T21:17:15.844+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:17:15.844+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T21:17:15.858+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.649 seconds
[2024-07-24T21:17:46.056+0000] {processor.py:157} INFO - Started process (PID=1443) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:17:46.057+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T21:17:46.060+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:17:46.059+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:17:46.103+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T21:17:46.104+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T21:17:46.746+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:17:46.771+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:17:46.770+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T21:17:46.790+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:17:46.790+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T21:17:46.807+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.754 seconds
[2024-07-24T21:18:16.983+0000] {processor.py:157} INFO - Started process (PID=1445) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:18:16.985+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T21:18:16.988+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:18:16.987+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:18:17.039+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T21:18:17.040+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T21:18:17.667+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:18:17.691+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:18:17.691+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T21:18:17.713+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:18:17.713+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T21:18:17.730+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.751 seconds
[2024-07-24T21:18:47.912+0000] {processor.py:157} INFO - Started process (PID=1447) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:18:47.913+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T21:18:47.916+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:18:47.916+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:18:47.951+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T21:18:47.951+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T21:18:48.715+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:18:48.746+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:18:48.745+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T21:18:48.776+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:18:48.775+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T21:18:48.796+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.887 seconds
[2024-07-24T21:19:18.953+0000] {processor.py:157} INFO - Started process (PID=1449) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:19:18.954+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T21:19:18.957+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:19:18.957+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:19:18.989+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T21:19:18.989+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T21:19:19.560+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:19:19.580+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:19:19.580+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T21:19:19.598+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:19:19.597+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T21:19:19.611+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.661 seconds
[2024-07-24T21:19:49.785+0000] {processor.py:157} INFO - Started process (PID=1451) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:19:49.786+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T21:19:49.789+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:19:49.789+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:19:49.819+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T21:19:49.819+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T21:19:50.387+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:19:50.405+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:19:50.405+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T21:19:50.423+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:19:50.423+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T21:19:50.440+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.657 seconds
[2024-07-24T21:20:20.612+0000] {processor.py:157} INFO - Started process (PID=1453) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:20:20.613+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T21:20:20.614+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:20:20.614+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:20:20.643+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T21:20:20.644+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T21:20:21.224+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:20:21.245+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:20:21.245+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T21:20:21.264+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:20:21.264+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T21:20:21.281+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.672 seconds
[2024-07-24T21:20:51.415+0000] {processor.py:157} INFO - Started process (PID=1455) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:20:51.416+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T21:20:51.418+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:20:51.418+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:20:51.450+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T21:20:51.451+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T21:20:52.060+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:20:52.079+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:20:52.079+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T21:20:52.102+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:20:52.101+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T21:20:52.120+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.708 seconds
[2024-07-24T21:21:22.301+0000] {processor.py:157} INFO - Started process (PID=1457) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:21:22.302+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T21:21:22.305+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:21:22.305+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:21:22.341+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T21:21:22.342+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T21:21:22.934+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:21:22.956+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:21:22.955+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T21:21:22.978+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:21:22.978+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T21:21:22.993+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.696 seconds
[2024-07-24T21:21:53.166+0000] {processor.py:157} INFO - Started process (PID=1459) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:21:53.168+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T21:21:53.170+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:21:53.170+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:21:53.204+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T21:21:53.205+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T21:21:53.775+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:21:53.798+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:21:53.797+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T21:21:53.819+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:21:53.819+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T21:21:53.834+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.671 seconds
[2024-07-24T21:22:23.897+0000] {processor.py:157} INFO - Started process (PID=1461) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:22:23.899+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T21:22:23.902+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:22:23.901+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:22:23.939+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T21:22:23.939+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T21:22:24.628+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:22:24.653+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:22:24.652+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T21:22:24.676+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:22:24.676+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T21:22:24.695+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.800 seconds
[2024-07-24T21:22:54.880+0000] {processor.py:157} INFO - Started process (PID=1463) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:22:54.881+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T21:22:54.883+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:22:54.882+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:22:54.915+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T21:22:54.915+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T21:22:55.460+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:22:55.478+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:22:55.478+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T21:22:55.495+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:22:55.495+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T21:22:55.510+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.632 seconds
[2024-07-24T21:23:25.668+0000] {processor.py:157} INFO - Started process (PID=1465) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:23:25.669+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T21:23:25.670+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:23:25.670+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:23:25.701+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T21:23:25.702+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T21:23:26.253+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:23:26.274+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:23:26.274+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T21:23:26.293+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:23:26.292+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T21:23:26.311+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.645 seconds
[2024-07-24T21:23:56.426+0000] {processor.py:157} INFO - Started process (PID=1467) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:23:56.427+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T21:23:56.428+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:23:56.428+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:23:56.460+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T21:23:56.461+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T21:23:57.040+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:23:57.059+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:23:57.058+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T21:23:57.078+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:23:57.078+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T21:23:57.094+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.671 seconds
[2024-07-24T21:24:27.381+0000] {processor.py:157} INFO - Started process (PID=1469) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:24:27.382+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T21:24:27.383+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:24:27.383+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:24:27.418+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T21:24:27.419+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T21:24:27.990+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:24:28.009+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:24:28.009+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T21:24:28.030+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:24:28.030+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T21:24:28.047+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.669 seconds
[2024-07-24T21:24:58.214+0000] {processor.py:157} INFO - Started process (PID=1471) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:24:58.215+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T21:24:58.217+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:24:58.217+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:24:58.249+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T21:24:58.249+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T21:24:58.788+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:24:58.806+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:24:58.805+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T21:24:58.823+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:24:58.823+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T21:24:58.836+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.625 seconds
[2024-07-24T21:25:28.871+0000] {processor.py:157} INFO - Started process (PID=1473) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:25:28.872+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T21:25:28.875+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:25:28.875+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:25:28.905+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T21:25:28.905+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T21:25:29.408+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:25:29.426+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:25:29.425+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T21:25:29.445+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:25:29.444+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T21:25:29.462+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.593 seconds
[2024-07-24T21:25:59.623+0000] {processor.py:157} INFO - Started process (PID=1475) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:25:59.624+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T21:25:59.626+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:25:59.626+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:25:59.654+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T21:25:59.654+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T21:26:00.217+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:26:00.237+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:26:00.236+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T21:26:00.254+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:26:00.254+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T21:26:00.268+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.647 seconds
[2024-07-24T21:26:30.424+0000] {processor.py:157} INFO - Started process (PID=1477) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:26:30.425+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T21:26:30.427+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:26:30.427+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:26:30.465+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T21:26:30.466+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T21:26:31.199+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:26:31.220+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:26:31.219+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T21:26:31.241+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:26:31.241+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T21:26:31.256+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.837 seconds
[2024-07-24T21:27:01.429+0000] {processor.py:157} INFO - Started process (PID=1479) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:27:01.430+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T21:27:01.432+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:27:01.432+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:27:01.469+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T21:27:01.469+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T21:27:02.035+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:27:02.054+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:27:02.053+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T21:27:02.075+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:27:02.075+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T21:27:02.090+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.663 seconds
[2024-07-24T21:27:32.258+0000] {processor.py:157} INFO - Started process (PID=1481) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:27:32.259+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T21:27:32.261+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:27:32.261+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:27:32.292+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T21:27:32.292+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T21:27:32.872+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:27:32.893+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:27:32.893+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T21:27:32.913+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:27:32.912+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T21:27:32.928+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.673 seconds
[2024-07-24T21:28:03.102+0000] {processor.py:157} INFO - Started process (PID=1483) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:28:03.103+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T21:28:03.107+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:28:03.107+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:28:03.144+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T21:28:03.145+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T21:28:03.743+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:28:03.763+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:28:03.763+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T21:28:03.785+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:28:03.784+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T21:28:03.800+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.702 seconds
[2024-07-24T21:28:33.862+0000] {processor.py:157} INFO - Started process (PID=1485) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:28:33.862+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T21:28:33.865+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:28:33.865+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:28:33.898+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T21:28:33.898+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T21:28:34.436+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:28:34.454+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:28:34.454+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T21:28:34.472+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:28:34.472+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T21:28:34.486+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.628 seconds
[2024-07-24T21:29:04.655+0000] {processor.py:157} INFO - Started process (PID=1487) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:29:04.656+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T21:29:04.657+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:29:04.657+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:29:04.685+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T21:29:04.686+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T21:29:05.214+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:29:05.232+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:29:05.231+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T21:29:05.250+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:29:05.249+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T21:29:05.265+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.613 seconds
[2024-07-24T21:29:35.420+0000] {processor.py:157} INFO - Started process (PID=1489) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:29:35.421+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T21:29:35.423+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:29:35.423+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:29:35.453+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T21:29:35.453+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T21:29:36.013+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:29:36.036+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:29:36.035+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T21:29:36.063+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:29:36.063+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T21:29:36.082+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.664 seconds
[2024-07-24T21:30:06.253+0000] {processor.py:157} INFO - Started process (PID=1491) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:30:06.254+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T21:30:06.255+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:30:06.255+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:30:06.286+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T21:30:06.287+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T21:30:06.860+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:30:06.880+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:30:06.880+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T21:30:06.901+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:30:06.901+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T21:30:06.916+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.666 seconds
[2024-07-24T21:30:37.080+0000] {processor.py:157} INFO - Started process (PID=1493) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:30:37.080+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T21:30:37.083+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:30:37.082+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:30:37.114+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T21:30:37.115+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T21:30:37.692+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:30:37.714+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:30:37.713+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T21:30:37.733+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:30:37.733+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T21:30:37.755+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.679 seconds
[2024-07-24T21:31:07.840+0000] {processor.py:157} INFO - Started process (PID=1495) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:31:07.841+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T21:31:07.843+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:31:07.843+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:31:07.872+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T21:31:07.873+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T21:31:08.419+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:31:08.438+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:31:08.438+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T21:31:08.454+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:31:08.454+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T21:31:08.468+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.631 seconds
[2024-07-24T21:31:38.640+0000] {processor.py:157} INFO - Started process (PID=1497) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:31:38.641+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T21:31:38.643+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:31:38.643+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:31:38.671+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T21:31:38.672+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T21:31:39.258+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:31:39.288+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:31:39.288+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T21:31:39.309+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:31:39.309+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T21:31:39.325+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.688 seconds
[2024-07-24T21:32:09.510+0000] {processor.py:157} INFO - Started process (PID=1499) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:32:09.511+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T21:32:09.513+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:32:09.512+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:32:09.545+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T21:32:09.546+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T21:32:10.076+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:32:10.095+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:32:10.095+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T21:32:10.117+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:32:10.117+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T21:32:10.132+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.625 seconds
[2024-07-24T21:32:40.306+0000] {processor.py:157} INFO - Started process (PID=1501) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:32:40.307+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T21:32:40.308+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:32:40.308+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:32:40.337+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T21:32:40.338+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T21:32:40.842+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:32:40.867+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:32:40.866+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T21:32:40.885+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:32:40.885+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T21:32:40.900+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.597 seconds
[2024-07-24T21:33:11.060+0000] {processor.py:157} INFO - Started process (PID=1503) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:33:11.061+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T21:33:11.063+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:33:11.063+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:33:11.102+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T21:33:11.103+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T21:33:11.681+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:33:11.715+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:33:11.715+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T21:33:11.736+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:33:11.735+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T21:33:11.752+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.695 seconds
[2024-07-24T21:33:41.939+0000] {processor.py:157} INFO - Started process (PID=1505) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:33:41.940+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T21:33:41.942+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:33:41.942+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:33:41.976+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T21:33:41.977+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T21:33:42.564+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:33:42.585+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:33:42.584+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T21:33:42.603+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:33:42.603+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T21:33:42.619+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.683 seconds
[2024-07-24T21:34:12.780+0000] {processor.py:157} INFO - Started process (PID=1507) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:34:12.781+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T21:34:12.783+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:34:12.783+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:34:12.815+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T21:34:12.815+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T21:34:13.357+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:34:13.374+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:34:13.373+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T21:34:13.393+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:34:13.393+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T21:34:13.406+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.629 seconds
[2024-07-24T21:34:43.484+0000] {processor.py:157} INFO - Started process (PID=1509) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:34:43.485+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T21:34:43.487+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:34:43.487+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:34:43.517+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T21:34:43.518+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T21:34:44.062+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:34:44.079+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:34:44.079+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T21:34:44.096+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:34:44.096+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T21:34:44.111+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.629 seconds
[2024-07-24T21:35:14.278+0000] {processor.py:157} INFO - Started process (PID=1511) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:35:14.279+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T21:35:14.281+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:35:14.281+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:35:14.309+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T21:35:14.309+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T21:35:14.837+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:35:14.856+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:35:14.855+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T21:35:14.875+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:35:14.874+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T21:35:14.891+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.616 seconds
[2024-07-24T21:35:45.047+0000] {processor.py:157} INFO - Started process (PID=1513) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:35:45.048+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T21:35:45.049+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:35:45.049+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:35:45.081+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T21:35:45.082+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T21:35:45.676+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:35:45.696+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:35:45.696+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T21:35:45.718+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:35:45.718+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T21:35:45.733+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.689 seconds
[2024-07-24T21:36:15.904+0000] {processor.py:157} INFO - Started process (PID=1515) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:36:15.905+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T21:36:15.907+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:36:15.907+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:36:15.939+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T21:36:15.940+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T21:36:16.505+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:36:16.524+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:36:16.524+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T21:36:16.543+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:36:16.542+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T21:36:16.568+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.667 seconds
[2024-07-24T21:36:46.754+0000] {processor.py:157} INFO - Started process (PID=1517) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:36:46.755+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T21:36:46.757+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:36:46.756+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:36:46.787+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T21:36:46.788+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T21:36:47.389+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:36:47.411+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:36:47.411+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T21:36:47.429+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:36:47.429+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T21:36:47.444+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.692 seconds
[2024-07-24T21:37:17.603+0000] {processor.py:157} INFO - Started process (PID=1519) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:37:17.604+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T21:37:17.606+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:37:17.606+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:37:17.635+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T21:37:17.635+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T21:37:18.222+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:37:18.242+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:37:18.242+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T21:37:18.260+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:37:18.260+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T21:37:18.276+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.676 seconds
[2024-07-24T21:37:48.451+0000] {processor.py:157} INFO - Started process (PID=1521) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:37:48.452+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T21:37:48.455+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:37:48.455+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:37:48.486+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T21:37:48.486+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T21:37:49.369+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:37:49.392+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:37:49.391+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T21:37:49.412+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:37:49.411+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T21:37:49.427+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.980 seconds
[2024-07-24T21:38:19.603+0000] {processor.py:157} INFO - Started process (PID=1523) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:38:19.603+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T21:38:19.606+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:38:19.605+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:38:19.637+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T21:38:19.638+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T21:38:20.160+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:38:20.176+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:38:20.176+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T21:38:20.194+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:38:20.193+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T21:38:20.207+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.608 seconds
[2024-07-24T21:38:50.360+0000] {processor.py:157} INFO - Started process (PID=1525) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:38:50.361+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T21:38:50.363+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:38:50.362+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:38:50.391+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T21:38:50.391+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T21:38:50.943+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:38:50.961+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:38:50.961+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T21:38:50.977+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:38:50.977+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T21:38:51.001+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.643 seconds
[2024-07-24T21:39:21.129+0000] {processor.py:157} INFO - Started process (PID=1527) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:39:21.130+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T21:39:21.131+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:39:21.131+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:39:21.159+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T21:39:21.160+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T21:39:21.721+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:39:21.740+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:39:21.740+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T21:39:21.758+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:39:21.758+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T21:39:21.771+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.644 seconds
[2024-07-24T21:39:51.924+0000] {processor.py:157} INFO - Started process (PID=1529) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:39:51.925+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T21:39:51.927+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:39:51.927+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:39:51.957+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T21:39:51.957+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T21:39:52.513+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:39:52.533+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:39:52.533+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T21:39:52.553+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:39:52.553+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T21:39:52.568+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.646 seconds
[2024-07-24T21:40:22.723+0000] {processor.py:157} INFO - Started process (PID=1531) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:40:22.724+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T21:40:22.725+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:40:22.725+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:40:22.753+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T21:40:22.754+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T21:40:23.300+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:40:23.320+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:40:23.320+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T21:40:23.340+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:40:23.340+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T21:40:23.356+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.635 seconds
[2024-07-24T21:40:53.530+0000] {processor.py:157} INFO - Started process (PID=1533) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:40:53.531+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T21:40:53.532+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:40:53.532+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:40:53.565+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T21:40:53.565+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T21:40:54.071+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:40:54.087+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:40:54.087+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T21:40:54.105+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:40:54.105+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T21:40:54.119+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.592 seconds
[2024-07-24T21:41:24.271+0000] {processor.py:157} INFO - Started process (PID=1535) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:41:24.272+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T21:41:24.274+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:41:24.274+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:41:24.303+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T21:41:24.303+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T21:41:24.797+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:41:24.813+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:41:24.813+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T21:41:24.832+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:41:24.831+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T21:41:24.846+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.578 seconds
[2024-07-24T21:41:55.022+0000] {processor.py:157} INFO - Started process (PID=1537) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:41:55.023+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T21:41:55.024+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:41:55.024+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:41:55.051+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T21:41:55.052+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T21:41:55.553+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:41:55.573+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:41:55.572+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T21:41:55.590+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:41:55.589+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T21:41:55.603+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.583 seconds
[2024-07-24T21:42:25.756+0000] {processor.py:157} INFO - Started process (PID=1539) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:42:25.757+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T21:42:25.759+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:42:25.759+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:42:25.789+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T21:42:25.790+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T21:42:26.340+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:42:26.358+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:42:26.358+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T21:42:26.376+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:42:26.376+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T21:42:26.391+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.638 seconds
[2024-07-24T21:42:56.554+0000] {processor.py:157} INFO - Started process (PID=1541) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:42:56.555+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T21:42:56.556+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:42:56.556+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:42:56.587+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T21:42:56.587+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T21:42:57.101+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:42:57.120+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:42:57.120+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T21:42:57.142+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:42:57.142+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T21:42:57.157+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.606 seconds
[2024-07-24T21:43:27.312+0000] {processor.py:157} INFO - Started process (PID=1543) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:43:27.313+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T21:43:27.315+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:43:27.315+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:43:27.347+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T21:43:27.347+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T21:43:27.852+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:43:27.869+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:43:27.869+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T21:43:27.890+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:43:27.890+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T21:43:27.908+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.598 seconds
[2024-07-24T21:43:58.066+0000] {processor.py:157} INFO - Started process (PID=1545) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:43:58.067+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T21:43:58.069+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:43:58.069+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:43:58.097+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T21:43:58.098+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T21:43:58.619+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:43:58.636+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:43:58.635+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T21:43:58.655+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:43:58.655+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T21:43:58.669+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.606 seconds
[2024-07-24T21:44:28.820+0000] {processor.py:157} INFO - Started process (PID=1547) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:44:28.821+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T21:44:28.824+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:44:28.823+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:44:28.851+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T21:44:28.851+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T21:44:29.382+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:44:29.404+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:44:29.404+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T21:44:29.424+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:44:29.424+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T21:44:29.440+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.622 seconds
[2024-07-24T21:44:59.607+0000] {processor.py:157} INFO - Started process (PID=1549) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:44:59.608+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T21:44:59.610+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:44:59.610+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:44:59.636+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T21:44:59.637+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T21:45:00.151+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:45:00.169+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:45:00.168+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T21:45:00.185+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:45:00.185+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T21:45:00.198+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.594 seconds
[2024-07-24T21:45:30.348+0000] {processor.py:157} INFO - Started process (PID=1551) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:45:30.349+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T21:45:30.351+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:45:30.351+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:45:30.380+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T21:45:30.381+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T21:45:30.920+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:45:30.937+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:45:30.936+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T21:45:30.955+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:45:30.955+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T21:45:30.969+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.623 seconds
[2024-07-24T21:46:01.134+0000] {processor.py:157} INFO - Started process (PID=1553) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:46:01.135+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T21:46:01.137+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:46:01.137+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:46:01.166+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T21:46:01.167+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T21:46:01.712+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:46:01.730+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:46:01.730+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T21:46:01.748+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:46:01.748+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T21:46:01.768+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.636 seconds
[2024-07-24T21:46:31.920+0000] {processor.py:157} INFO - Started process (PID=1555) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:46:31.921+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T21:46:31.923+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:46:31.923+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:46:31.952+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T21:46:31.953+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T21:46:32.526+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:46:32.545+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:46:32.545+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T21:46:32.566+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:46:32.566+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T21:46:32.581+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.664 seconds
[2024-07-24T21:47:02.746+0000] {processor.py:157} INFO - Started process (PID=1557) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:47:02.746+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T21:47:02.748+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:47:02.748+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:47:02.777+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T21:47:02.777+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T21:47:03.336+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:47:03.354+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:47:03.354+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T21:47:03.374+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:47:03.373+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T21:47:03.388+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.645 seconds
[2024-07-24T21:47:33.549+0000] {processor.py:157} INFO - Started process (PID=1559) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:47:33.550+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T21:47:33.552+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:47:33.552+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:47:33.585+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T21:47:33.585+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T21:47:34.201+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:47:34.222+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:47:34.221+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T21:47:34.243+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:47:34.242+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T21:47:34.257+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.710 seconds
[2024-07-24T21:48:04.345+0000] {processor.py:157} INFO - Started process (PID=1561) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:48:04.346+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T21:48:04.348+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:48:04.348+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:48:04.379+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T21:48:04.380+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T21:48:04.914+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:48:04.931+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:48:04.931+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T21:48:04.950+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:48:04.950+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T21:48:04.964+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.621 seconds
[2024-07-24T21:48:35.133+0000] {processor.py:157} INFO - Started process (PID=1563) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:48:35.134+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T21:48:35.137+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:48:35.137+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:48:35.169+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T21:48:35.169+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T21:48:35.712+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:48:35.732+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:48:35.731+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T21:48:35.749+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:48:35.749+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T21:48:35.763+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.633 seconds
[2024-07-24T21:49:05.930+0000] {processor.py:157} INFO - Started process (PID=1565) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:49:05.931+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T21:49:05.933+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:49:05.933+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:49:05.967+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T21:49:05.967+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T21:49:06.538+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:49:06.559+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:49:06.558+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T21:49:06.577+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:49:06.577+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T21:49:06.595+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.668 seconds
[2024-07-24T21:49:36.764+0000] {processor.py:157} INFO - Started process (PID=1567) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:49:36.765+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T21:49:36.766+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:49:36.766+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:49:36.798+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T21:49:36.798+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T21:49:37.347+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:49:37.366+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:49:37.365+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T21:49:37.384+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:49:37.384+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T21:49:37.399+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.638 seconds
[2024-07-24T21:50:07.553+0000] {processor.py:157} INFO - Started process (PID=1569) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:50:07.554+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T21:50:07.555+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:50:07.555+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:50:07.585+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T21:50:07.586+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T21:50:08.084+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:50:08.109+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:50:08.109+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T21:50:08.127+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:50:08.126+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T21:50:08.139+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.589 seconds
[2024-07-24T21:50:38.295+0000] {processor.py:157} INFO - Started process (PID=1571) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:50:38.295+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T21:50:38.297+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:50:38.297+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:50:38.326+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T21:50:38.326+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T21:50:38.815+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:50:38.832+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:50:38.832+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T21:50:38.849+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:50:38.849+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T21:50:38.862+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.570 seconds
[2024-07-24T21:51:09.007+0000] {processor.py:157} INFO - Started process (PID=1573) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:51:09.008+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T21:51:09.009+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:51:09.009+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:51:09.036+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T21:51:09.036+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T21:51:09.580+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:51:09.599+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:51:09.599+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T21:51:09.617+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:51:09.617+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T21:51:09.630+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.626 seconds
[2024-07-24T21:51:39.784+0000] {processor.py:157} INFO - Started process (PID=1575) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:51:39.785+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T21:51:39.787+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:51:39.786+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:51:39.814+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T21:51:39.815+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T21:51:40.305+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:51:40.321+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:51:40.321+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T21:51:40.336+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:51:40.336+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T21:51:40.348+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.567 seconds
[2024-07-24T21:52:10.497+0000] {processor.py:157} INFO - Started process (PID=1577) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:52:10.498+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T21:52:10.499+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:52:10.499+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:52:10.526+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T21:52:10.527+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T21:52:11.036+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:52:11.056+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:52:11.055+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T21:52:11.076+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:52:11.075+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T21:52:11.090+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.596 seconds
[2024-07-24T21:52:41.251+0000] {processor.py:157} INFO - Started process (PID=1579) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:52:41.252+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T21:52:41.254+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:52:41.253+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:52:41.283+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T21:52:41.283+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T21:52:41.861+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:52:41.878+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:52:41.877+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T21:52:41.896+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:52:41.895+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T21:52:41.911+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.662 seconds
[2024-07-24T21:53:12.064+0000] {processor.py:157} INFO - Started process (PID=1581) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:53:12.065+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T21:53:12.067+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:53:12.067+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:53:12.102+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T21:53:12.102+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T21:53:13.566+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:53:13.607+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:53:13.606+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T21:53:13.637+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:53:13.637+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T21:53:13.660+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 1.599 seconds
[2024-07-24T21:53:43.872+0000] {processor.py:157} INFO - Started process (PID=1583) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:53:43.874+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T21:53:43.876+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:53:43.875+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:53:43.911+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T21:53:43.911+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T21:53:44.457+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:53:44.476+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:53:44.476+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T21:53:44.495+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:53:44.495+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T21:53:44.509+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.639 seconds
[2024-07-24T21:54:14.668+0000] {processor.py:157} INFO - Started process (PID=1585) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:54:14.669+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T21:54:14.671+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:54:14.671+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:54:14.700+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T21:54:14.700+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T21:54:15.261+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:54:15.283+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:54:15.282+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T21:54:15.302+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:54:15.302+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T21:54:15.316+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.651 seconds
[2024-07-24T21:54:45.490+0000] {processor.py:157} INFO - Started process (PID=1587) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:54:45.491+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T21:54:45.494+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:54:45.493+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:54:45.527+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T21:54:45.528+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T21:54:46.077+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:54:46.096+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:54:46.095+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T21:54:46.113+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:54:46.113+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T21:54:46.128+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.641 seconds
[2024-07-24T21:55:16.295+0000] {processor.py:157} INFO - Started process (PID=1589) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:55:16.296+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T21:55:16.299+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:55:16.298+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:55:16.339+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T21:55:16.340+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T21:55:16.937+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:55:16.956+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:55:16.955+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T21:55:16.975+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:55:16.975+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T21:55:16.990+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.698 seconds
[2024-07-24T21:55:47.095+0000] {processor.py:157} INFO - Started process (PID=1591) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:55:47.096+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T21:55:47.098+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:55:47.098+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:55:47.133+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T21:55:47.134+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T21:55:47.748+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:55:47.768+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:55:47.767+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T21:55:47.789+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:55:47.789+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T21:55:47.807+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.715 seconds
[2024-07-24T21:56:18.049+0000] {processor.py:157} INFO - Started process (PID=1593) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:56:18.051+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T21:56:18.053+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:56:18.052+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:56:18.083+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T21:56:18.084+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T21:56:18.701+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:56:18.728+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:56:18.727+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T21:56:18.745+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:56:18.745+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T21:56:18.758+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.768 seconds
[2024-07-24T21:56:48.931+0000] {processor.py:157} INFO - Started process (PID=1595) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:56:48.932+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T21:56:48.934+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:56:48.934+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:56:48.963+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T21:56:48.963+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T21:56:49.509+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:56:49.533+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:56:49.532+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T21:56:49.557+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:56:49.557+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T21:56:49.575+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.646 seconds
[2024-07-24T21:57:19.736+0000] {processor.py:157} INFO - Started process (PID=1597) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:57:19.737+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T21:57:19.740+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:57:19.740+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:57:19.772+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T21:57:19.773+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T21:57:20.355+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:57:20.374+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:57:20.374+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T21:57:20.395+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:57:20.395+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T21:57:20.412+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.680 seconds
[2024-07-24T21:57:50.598+0000] {processor.py:157} INFO - Started process (PID=1599) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:57:50.599+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T21:57:50.602+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:57:50.601+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:57:50.642+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T21:57:50.642+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T21:57:51.210+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:57:51.232+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:57:51.231+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T21:57:51.251+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:57:51.251+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T21:57:51.268+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.674 seconds
[2024-07-24T21:58:21.434+0000] {processor.py:157} INFO - Started process (PID=1601) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:58:21.435+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T21:58:21.437+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:58:21.437+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:58:21.466+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T21:58:21.467+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T21:58:22.032+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:58:22.052+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:58:22.051+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T21:58:22.072+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:58:22.072+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T21:58:22.089+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.658 seconds
[2024-07-24T21:58:52.155+0000] {processor.py:157} INFO - Started process (PID=1603) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:58:52.156+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T21:58:52.157+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:58:52.157+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:58:52.189+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T21:58:52.189+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T21:58:52.753+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:58:52.777+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:58:52.776+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T21:58:52.800+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:58:52.800+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T21:58:52.818+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.666 seconds
[2024-07-24T21:59:22.990+0000] {processor.py:157} INFO - Started process (PID=1605) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:59:22.991+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T21:59:22.993+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:59:22.993+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:59:23.024+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T21:59:23.024+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T21:59:23.575+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:59:23.605+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:59:23.604+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T21:59:23.625+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:59:23.625+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T21:59:23.640+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.653 seconds
[2024-07-24T21:59:53.810+0000] {processor.py:157} INFO - Started process (PID=1607) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:59:53.811+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T21:59:53.812+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:59:53.812+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:59:53.844+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T21:59:53.844+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T21:59:54.394+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T21:59:54.413+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:59:54.412+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T21:59:54.435+0000] {logging_mixin.py:151} INFO - [2024-07-24T21:59:54.435+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T21:59:54.451+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.644 seconds
[2024-07-24T22:00:24.626+0000] {processor.py:157} INFO - Started process (PID=1609) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:00:24.627+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T22:00:24.630+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:00:24.629+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:00:24.661+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T22:00:24.661+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T22:00:25.199+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:00:25.218+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:00:25.218+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T22:00:25.236+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:00:25.235+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T22:00:25.249+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.626 seconds
[2024-07-24T22:00:55.406+0000] {processor.py:157} INFO - Started process (PID=1611) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:00:55.407+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T22:00:55.409+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:00:55.409+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:00:55.438+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T22:00:55.438+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T22:00:55.947+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:00:55.965+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:00:55.965+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T22:00:55.985+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:00:55.984+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T22:00:56.003+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.600 seconds
[2024-07-24T22:01:26.198+0000] {processor.py:157} INFO - Started process (PID=1613) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:01:26.199+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T22:01:26.202+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:01:26.201+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:01:26.233+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T22:01:26.233+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T22:01:26.822+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:01:26.841+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:01:26.841+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T22:01:26.862+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:01:26.861+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T22:01:26.878+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.684 seconds
[2024-07-24T22:01:57.041+0000] {processor.py:157} INFO - Started process (PID=1615) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:01:57.042+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T22:01:57.044+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:01:57.044+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:01:57.075+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T22:01:57.076+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T22:01:57.635+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:01:57.657+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:01:57.656+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T22:01:57.677+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:01:57.677+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T22:01:57.697+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.659 seconds
[2024-07-24T22:02:27.870+0000] {processor.py:157} INFO - Started process (PID=1617) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:02:27.871+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T22:02:27.873+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:02:27.873+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:02:27.906+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T22:02:27.906+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T22:02:28.623+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:02:28.640+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:02:28.640+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T22:02:28.656+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:02:28.656+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T22:02:28.670+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.803 seconds
[2024-07-24T22:02:58.847+0000] {processor.py:157} INFO - Started process (PID=1619) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:02:58.847+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T22:02:58.849+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:02:58.849+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:02:58.878+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T22:02:58.878+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T22:02:59.418+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:02:59.444+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:02:59.443+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T22:02:59.472+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:02:59.471+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T22:02:59.498+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.654 seconds
[2024-07-24T22:03:29.662+0000] {processor.py:157} INFO - Started process (PID=1621) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:03:29.662+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T22:03:29.664+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:03:29.664+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:03:29.693+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T22:03:29.693+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T22:03:30.235+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:03:30.255+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:03:30.254+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T22:03:30.274+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:03:30.274+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T22:03:30.300+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.643 seconds
[2024-07-24T22:04:00.461+0000] {processor.py:157} INFO - Started process (PID=1623) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:04:00.462+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T22:04:00.464+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:04:00.464+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:04:00.494+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T22:04:00.495+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T22:04:01.083+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:04:01.106+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:04:01.106+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T22:04:01.128+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:04:01.128+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T22:04:01.146+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.688 seconds
[2024-07-24T22:04:31.354+0000] {processor.py:157} INFO - Started process (PID=1625) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:04:31.355+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T22:04:31.357+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:04:31.357+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:04:31.388+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T22:04:31.389+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T22:04:31.972+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:04:31.997+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:04:31.996+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T22:04:32.016+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:04:32.016+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T22:04:32.030+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.680 seconds
[2024-07-24T22:05:02.195+0000] {processor.py:157} INFO - Started process (PID=1627) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:05:02.196+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T22:05:02.199+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:05:02.199+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:05:02.230+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T22:05:02.231+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T22:05:02.816+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:05:02.838+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:05:02.837+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T22:05:02.856+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:05:02.856+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T22:05:02.873+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.681 seconds
[2024-07-24T22:05:33.032+0000] {processor.py:157} INFO - Started process (PID=1629) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:05:33.033+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T22:05:33.034+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:05:33.034+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:05:33.065+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T22:05:33.065+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T22:05:33.661+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:05:33.683+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:05:33.683+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T22:05:33.705+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:05:33.705+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T22:05:33.720+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.692 seconds
[2024-07-24T22:06:03.907+0000] {processor.py:157} INFO - Started process (PID=1631) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:06:03.908+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T22:06:03.910+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:06:03.910+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:06:03.940+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T22:06:03.940+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T22:06:04.487+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:06:04.505+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:06:04.505+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T22:06:04.523+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:06:04.523+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T22:06:04.538+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.634 seconds
[2024-07-24T22:06:34.703+0000] {processor.py:157} INFO - Started process (PID=1633) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:06:34.704+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T22:06:34.706+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:06:34.705+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:06:34.736+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T22:06:34.737+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T22:06:35.297+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:06:35.317+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:06:35.316+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T22:06:35.338+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:06:35.338+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T22:06:35.353+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.653 seconds
[2024-07-24T22:07:05.481+0000] {processor.py:157} INFO - Started process (PID=1635) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:07:05.482+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T22:07:05.484+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:07:05.483+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:07:05.512+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T22:07:05.513+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T22:07:06.126+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:07:06.146+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:07:06.145+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T22:07:06.167+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:07:06.167+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T22:07:06.183+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.705 seconds
[2024-07-24T22:07:36.348+0000] {processor.py:157} INFO - Started process (PID=1637) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:07:36.349+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T22:07:36.351+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:07:36.351+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:07:36.381+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T22:07:36.382+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T22:07:36.945+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:07:36.967+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:07:36.966+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T22:07:36.988+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:07:36.987+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T22:07:37.007+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.661 seconds
[2024-07-24T22:08:07.166+0000] {processor.py:157} INFO - Started process (PID=1639) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:08:07.167+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T22:08:07.170+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:08:07.170+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:08:07.202+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T22:08:07.202+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T22:08:07.776+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:08:07.793+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:08:07.793+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T22:08:07.812+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:08:07.812+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T22:08:07.826+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.663 seconds
[2024-07-24T22:08:37.903+0000] {processor.py:157} INFO - Started process (PID=1641) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:08:37.904+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T22:08:37.906+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:08:37.906+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:08:37.939+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T22:08:37.940+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T22:08:38.471+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:08:38.490+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:08:38.489+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T22:08:38.507+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:08:38.507+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T22:08:38.525+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.625 seconds
[2024-07-24T22:09:08.699+0000] {processor.py:157} INFO - Started process (PID=1643) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:09:08.701+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T22:09:08.702+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:09:08.702+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:09:08.733+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T22:09:08.734+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T22:09:09.296+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:09:09.313+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:09:09.313+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T22:09:09.333+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:09:09.332+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T22:09:09.350+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.653 seconds
[2024-07-24T22:09:39.509+0000] {processor.py:157} INFO - Started process (PID=1645) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:09:39.510+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T22:09:39.512+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:09:39.512+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:09:39.549+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T22:09:39.549+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T22:09:40.138+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:09:40.155+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:09:40.154+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T22:09:40.173+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:09:40.173+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T22:09:40.191+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.685 seconds
[2024-07-24T22:10:10.368+0000] {processor.py:157} INFO - Started process (PID=1647) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:10:10.369+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T22:10:10.371+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:10:10.370+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:10:10.400+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T22:10:10.401+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T22:10:10.929+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:10:10.948+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:10:10.948+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T22:10:10.967+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:10:10.967+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T22:10:10.981+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.616 seconds
[2024-07-24T22:10:41.153+0000] {processor.py:157} INFO - Started process (PID=1649) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:10:41.154+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T22:10:41.157+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:10:41.157+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:10:41.192+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T22:10:41.192+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T22:10:41.953+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:10:41.978+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:10:41.978+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T22:10:42.008+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:10:42.007+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T22:10:42.037+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.887 seconds
[2024-07-24T22:11:12.212+0000] {processor.py:157} INFO - Started process (PID=1651) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:11:12.213+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T22:11:12.215+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:11:12.215+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:11:12.247+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T22:11:12.248+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T22:11:12.816+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:11:12.835+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:11:12.834+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T22:11:12.856+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:11:12.856+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T22:11:12.872+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.663 seconds
[2024-07-24T22:11:43.051+0000] {processor.py:157} INFO - Started process (PID=1653) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:11:43.052+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T22:11:43.054+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:11:43.054+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:11:43.083+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T22:11:43.084+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T22:11:43.614+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:11:43.631+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:11:43.631+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T22:11:43.648+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:11:43.648+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T22:11:43.661+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.613 seconds
[2024-07-24T22:12:13.828+0000] {processor.py:157} INFO - Started process (PID=1655) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:12:13.829+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T22:12:13.830+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:12:13.830+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:12:13.860+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T22:12:13.860+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T22:12:14.393+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:12:14.410+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:12:14.410+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T22:12:14.430+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:12:14.430+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T22:12:14.445+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.620 seconds
[2024-07-24T22:12:44.624+0000] {processor.py:157} INFO - Started process (PID=1657) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:12:44.625+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T22:12:44.627+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:12:44.627+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:12:44.662+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T22:12:44.662+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T22:12:45.268+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:12:45.287+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:12:45.286+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T22:12:45.306+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:12:45.305+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T22:12:45.319+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.698 seconds
[2024-07-24T22:13:15.487+0000] {processor.py:157} INFO - Started process (PID=1659) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:13:15.488+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T22:13:15.489+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:13:15.489+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:13:15.520+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T22:13:15.520+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T22:13:16.151+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:13:16.172+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:13:16.171+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T22:13:16.192+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:13:16.192+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T22:13:16.208+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.724 seconds
[2024-07-24T22:13:46.264+0000] {processor.py:157} INFO - Started process (PID=1661) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:13:46.265+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T22:13:46.267+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:13:46.267+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:13:46.301+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T22:13:46.301+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T22:13:46.920+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:13:46.971+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:13:46.970+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T22:13:47.007+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:13:47.006+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T22:13:47.028+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.768 seconds
[2024-07-24T22:14:17.217+0000] {processor.py:157} INFO - Started process (PID=1663) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:14:17.218+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T22:14:17.221+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:14:17.220+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:14:17.252+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T22:14:17.252+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T22:14:17.819+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:14:17.839+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:14:17.838+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T22:14:17.858+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:14:17.858+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T22:14:17.874+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.660 seconds
[2024-07-24T22:14:48.075+0000] {processor.py:157} INFO - Started process (PID=1665) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:14:48.077+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T22:14:48.080+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:14:48.080+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:14:48.124+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T22:14:48.124+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T22:14:48.742+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:14:48.766+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:14:48.766+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T22:14:48.790+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:14:48.790+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T22:14:48.809+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.737 seconds
[2024-07-24T22:15:18.974+0000] {processor.py:157} INFO - Started process (PID=1667) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:15:18.975+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T22:15:18.977+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:15:18.977+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:15:19.011+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T22:15:19.011+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T22:15:19.645+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:15:19.666+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:15:19.666+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T22:15:19.689+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:15:19.689+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T22:15:19.712+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.743 seconds
[2024-07-24T22:15:49.903+0000] {processor.py:157} INFO - Started process (PID=1669) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:15:49.904+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T22:15:49.906+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:15:49.905+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:15:49.935+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T22:15:49.935+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T22:15:50.489+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:15:50.508+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:15:50.507+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T22:15:50.526+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:15:50.525+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T22:15:50.539+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.639 seconds
[2024-07-24T22:16:20.697+0000] {processor.py:157} INFO - Started process (PID=1671) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:16:20.698+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T22:16:20.700+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:16:20.699+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:16:20.729+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T22:16:20.729+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T22:16:21.284+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:16:21.309+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:16:21.308+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T22:16:21.330+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:16:21.330+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T22:16:21.347+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.653 seconds
[2024-07-24T22:16:51.408+0000] {processor.py:157} INFO - Started process (PID=1673) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:16:51.409+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T22:16:51.411+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:16:51.410+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:16:51.442+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T22:16:51.443+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T22:16:52.028+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:16:52.046+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:16:52.046+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T22:16:52.065+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:16:52.065+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T22:16:52.082+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.678 seconds
[2024-07-24T22:17:22.260+0000] {processor.py:157} INFO - Started process (PID=1675) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:17:22.261+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T22:17:22.266+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:17:22.266+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:17:22.311+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T22:17:22.311+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T22:17:22.929+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:17:22.947+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:17:22.947+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T22:17:22.966+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:17:22.966+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T22:17:22.981+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.725 seconds
[2024-07-24T22:17:53.143+0000] {processor.py:157} INFO - Started process (PID=1677) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:17:53.144+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T22:17:53.147+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:17:53.147+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:17:53.179+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T22:17:53.179+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T22:17:53.705+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:17:53.721+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:17:53.721+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T22:17:53.738+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:17:53.738+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T22:17:53.752+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.612 seconds
[2024-07-24T22:18:23.928+0000] {processor.py:157} INFO - Started process (PID=1679) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:18:23.928+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T22:18:23.930+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:18:23.930+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:18:23.960+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T22:18:23.960+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T22:18:24.471+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:18:24.488+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:18:24.487+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T22:18:24.507+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:18:24.507+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T22:18:24.523+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.599 seconds
[2024-07-24T22:18:54.692+0000] {processor.py:157} INFO - Started process (PID=1681) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:18:54.693+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T22:18:54.695+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:18:54.695+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:18:54.728+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T22:18:54.729+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T22:18:55.328+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:18:55.348+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:18:55.348+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T22:18:55.366+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:18:55.366+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T22:18:55.382+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.693 seconds
[2024-07-24T22:19:25.564+0000] {processor.py:157} INFO - Started process (PID=1683) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:19:25.565+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T22:19:25.567+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:19:25.567+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:19:25.599+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T22:19:25.599+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T22:19:26.159+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:19:26.177+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:19:26.177+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T22:19:26.195+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:19:26.194+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T22:19:26.209+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.648 seconds
[2024-07-24T22:19:56.378+0000] {processor.py:157} INFO - Started process (PID=1685) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:19:56.378+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T22:19:56.380+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:19:56.380+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:19:56.412+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T22:19:56.412+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T22:19:57.007+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:19:57.026+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:19:57.025+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T22:19:57.046+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:19:57.046+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T22:19:57.067+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.692 seconds
[2024-07-24T22:20:27.317+0000] {processor.py:157} INFO - Started process (PID=1687) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:20:27.318+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T22:20:27.320+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:20:27.319+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:20:27.350+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T22:20:27.351+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T22:20:27.911+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:20:27.932+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:20:27.931+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T22:20:27.951+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:20:27.951+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T22:20:27.967+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.653 seconds
[2024-07-24T22:20:58.145+0000] {processor.py:157} INFO - Started process (PID=1689) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:20:58.146+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T22:20:58.148+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:20:58.147+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:20:58.182+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T22:20:58.182+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T22:20:58.784+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:20:58.806+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:20:58.805+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T22:20:58.828+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:20:58.828+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T22:20:58.846+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.704 seconds
[2024-07-24T22:21:29.033+0000] {processor.py:157} INFO - Started process (PID=1691) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:21:29.034+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T22:21:29.036+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:21:29.036+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:21:29.067+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T22:21:29.068+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T22:21:29.597+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:21:29.615+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:21:29.614+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T22:21:29.633+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:21:29.632+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T22:21:29.648+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.618 seconds
[2024-07-24T22:21:59.787+0000] {processor.py:157} INFO - Started process (PID=1693) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:21:59.788+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T22:21:59.790+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:21:59.790+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:21:59.826+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T22:21:59.826+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T22:22:00.406+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:22:00.426+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:22:00.426+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T22:22:00.446+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:22:00.445+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T22:22:00.471+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.686 seconds
[2024-07-24T22:22:30.651+0000] {processor.py:157} INFO - Started process (PID=1695) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:22:30.652+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T22:22:30.654+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:22:30.654+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:22:30.686+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T22:22:30.687+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T22:22:31.253+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:22:31.280+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:22:31.279+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T22:22:31.299+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:22:31.299+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T22:22:31.315+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.667 seconds
[2024-07-24T22:23:01.486+0000] {processor.py:157} INFO - Started process (PID=1697) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:23:01.487+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T22:23:01.488+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:23:01.488+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:23:01.521+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T22:23:01.521+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T22:23:02.092+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:23:02.111+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:23:02.110+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T22:23:02.129+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:23:02.129+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T22:23:02.145+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.663 seconds
[2024-07-24T22:23:32.237+0000] {processor.py:157} INFO - Started process (PID=1699) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:23:32.237+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T22:23:32.239+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:23:32.239+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:23:32.269+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T22:23:32.269+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T22:23:32.839+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:23:32.857+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:23:32.857+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T22:23:32.875+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:23:32.875+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T22:23:32.896+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.662 seconds
[2024-07-24T22:24:03.074+0000] {processor.py:157} INFO - Started process (PID=1701) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:24:03.075+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T22:24:03.077+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:24:03.077+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:24:03.106+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T22:24:03.106+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T22:24:03.693+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:24:03.711+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:24:03.710+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T22:24:03.729+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:24:03.729+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T22:24:03.742+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.671 seconds
[2024-07-24T22:24:33.903+0000] {processor.py:157} INFO - Started process (PID=1703) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:24:33.904+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T22:24:33.906+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:24:33.906+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:24:33.936+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T22:24:33.936+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T22:24:34.483+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:24:34.503+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:24:34.503+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T22:24:34.521+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:24:34.521+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T22:24:34.537+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.637 seconds
[2024-07-24T22:25:04.710+0000] {processor.py:157} INFO - Started process (PID=1705) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:25:04.711+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T22:25:04.713+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:25:04.713+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:25:04.742+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T22:25:04.743+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T22:25:05.276+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:25:05.294+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:25:05.294+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T22:25:05.312+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:25:05.312+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T22:25:05.327+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.619 seconds
[2024-07-24T22:25:35.497+0000] {processor.py:157} INFO - Started process (PID=1707) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:25:35.497+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T22:25:35.500+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:25:35.499+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:25:35.529+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T22:25:35.529+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T22:25:36.052+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:25:36.071+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:25:36.071+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T22:25:36.093+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:25:36.093+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T22:25:36.110+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.616 seconds
[2024-07-24T22:26:06.271+0000] {processor.py:157} INFO - Started process (PID=1709) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:26:06.272+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T22:26:06.274+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:26:06.273+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:26:06.307+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T22:26:06.307+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T22:26:06.922+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:26:06.943+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:26:06.943+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T22:26:06.968+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:26:06.968+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T22:26:06.987+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.719 seconds
[2024-07-24T22:26:37.148+0000] {processor.py:157} INFO - Started process (PID=1711) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:26:37.149+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T22:26:37.151+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:26:37.151+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:26:37.186+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T22:26:37.187+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T22:26:38.040+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:26:38.073+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:26:38.073+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T22:26:38.104+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:26:38.103+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T22:26:38.125+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.979 seconds
[2024-07-24T22:27:08.346+0000] {processor.py:157} INFO - Started process (PID=1713) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:27:08.346+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T22:27:08.349+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:27:08.349+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:27:08.384+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T22:27:08.384+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T22:27:09.048+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:27:09.070+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:27:09.070+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T22:27:09.090+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:27:09.090+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T22:27:09.108+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.765 seconds
[2024-07-24T22:27:39.285+0000] {processor.py:157} INFO - Started process (PID=1715) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:27:39.286+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T22:27:39.288+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:27:39.288+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:27:39.318+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T22:27:39.319+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T22:27:39.864+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:27:39.885+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:27:39.884+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T22:27:39.903+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:27:39.903+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T22:27:39.919+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.637 seconds
[2024-07-24T22:28:10.123+0000] {processor.py:157} INFO - Started process (PID=1717) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:28:10.124+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T22:28:10.126+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:28:10.126+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:28:10.157+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T22:28:10.157+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T22:28:10.702+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:28:10.720+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:28:10.720+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T22:28:10.738+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:28:10.738+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T22:28:10.752+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.632 seconds
[2024-07-24T22:28:40.804+0000] {processor.py:157} INFO - Started process (PID=1719) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:28:40.805+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T22:28:40.807+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:28:40.807+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:28:40.836+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T22:28:40.836+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T22:28:41.388+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:28:41.408+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:28:41.408+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T22:28:41.427+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:28:41.427+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T22:28:41.443+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.641 seconds
[2024-07-24T22:29:11.625+0000] {processor.py:157} INFO - Started process (PID=1721) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:29:11.626+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T22:29:11.628+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:29:11.628+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:29:11.666+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T22:29:11.666+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T22:29:12.261+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:29:12.280+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:29:12.280+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T22:29:12.299+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:29:12.299+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T22:29:12.314+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.693 seconds
[2024-07-24T22:29:42.478+0000] {processor.py:157} INFO - Started process (PID=1723) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:29:42.479+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T22:29:42.481+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:29:42.481+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:29:42.512+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T22:29:42.512+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T22:29:43.096+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:29:43.116+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:29:43.115+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T22:29:43.136+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:29:43.136+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T22:29:43.154+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.679 seconds
[2024-07-24T22:30:13.341+0000] {processor.py:157} INFO - Started process (PID=1725) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:30:13.342+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T22:30:13.344+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:30:13.344+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:30:13.373+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T22:30:13.374+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T22:30:13.955+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:30:13.975+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:30:13.974+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T22:30:13.994+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:30:13.994+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T22:30:14.010+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.672 seconds
[2024-07-24T22:30:44.179+0000] {processor.py:157} INFO - Started process (PID=1727) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:30:44.180+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T22:30:44.182+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:30:44.182+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:30:44.212+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T22:30:44.212+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T22:30:44.775+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:30:44.794+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:30:44.793+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T22:30:44.811+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:30:44.811+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T22:30:44.831+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.655 seconds
[2024-07-24T22:31:14.906+0000] {processor.py:157} INFO - Started process (PID=1729) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:31:14.907+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T22:31:14.909+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:31:14.909+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:31:14.937+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T22:31:14.938+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T22:31:15.497+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:31:15.516+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:31:15.515+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T22:31:15.535+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:31:15.535+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T22:31:15.549+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.646 seconds
[2024-07-24T22:31:45.734+0000] {processor.py:157} INFO - Started process (PID=1731) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:31:45.735+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T22:31:45.736+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:31:45.736+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:31:45.765+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T22:31:45.766+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T22:31:46.327+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:31:46.348+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:31:46.348+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T22:31:46.369+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:31:46.369+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T22:31:46.384+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.653 seconds
[2024-07-24T22:32:16.548+0000] {processor.py:157} INFO - Started process (PID=1733) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:32:16.549+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T22:32:16.551+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:32:16.551+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:32:16.584+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T22:32:16.584+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T22:32:17.173+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:32:17.199+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:32:17.198+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T22:32:17.225+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:32:17.225+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T22:32:17.241+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.696 seconds
[2024-07-24T22:32:47.408+0000] {processor.py:157} INFO - Started process (PID=1735) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:32:47.409+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T22:32:47.411+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:32:47.410+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:32:47.441+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T22:32:47.442+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T22:32:48.004+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:32:48.021+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:32:48.021+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T22:32:48.040+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:32:48.040+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T22:32:48.055+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.650 seconds
[2024-07-24T22:33:18.228+0000] {processor.py:157} INFO - Started process (PID=1737) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:33:18.230+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T22:33:18.231+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:33:18.231+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:33:18.266+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T22:33:18.267+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T22:33:19.012+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:33:19.037+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:33:19.037+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T22:33:19.058+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:33:19.058+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T22:33:19.074+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.849 seconds
[2024-07-24T22:33:49.258+0000] {processor.py:157} INFO - Started process (PID=1739) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:33:49.258+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T22:33:49.261+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:33:49.260+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:33:49.289+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T22:33:49.290+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T22:33:49.838+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:33:49.857+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:33:49.857+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T22:33:49.874+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:33:49.874+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T22:33:49.889+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.634 seconds
[2024-07-24T22:34:20.040+0000] {processor.py:157} INFO - Started process (PID=1741) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:34:20.041+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T22:34:20.044+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:34:20.043+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:34:20.074+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T22:34:20.074+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T22:34:20.634+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:34:20.652+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:34:20.652+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T22:34:20.669+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:34:20.668+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T22:34:20.686+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.650 seconds
[2024-07-24T22:34:50.859+0000] {processor.py:157} INFO - Started process (PID=1743) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:34:50.860+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T22:34:50.862+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:34:50.862+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:34:50.891+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T22:34:50.891+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T22:34:51.482+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:34:51.501+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:34:51.501+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T22:34:51.521+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:34:51.521+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T22:34:51.542+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.685 seconds
[2024-07-24T22:35:21.706+0000] {processor.py:157} INFO - Started process (PID=1745) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:35:21.707+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T22:35:21.708+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:35:21.708+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:35:21.738+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T22:35:21.739+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T22:35:22.270+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:35:22.292+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:35:22.291+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T22:35:22.312+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:35:22.312+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T22:35:22.326+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.624 seconds
[2024-07-24T22:35:52.462+0000] {processor.py:157} INFO - Started process (PID=1747) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:35:52.462+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T22:35:52.464+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:35:52.464+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:35:52.495+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T22:35:52.495+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T22:35:53.068+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:35:53.087+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:35:53.086+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T22:35:53.107+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:35:53.106+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T22:35:53.122+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.663 seconds
[2024-07-24T22:36:23.286+0000] {processor.py:157} INFO - Started process (PID=1749) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:36:23.286+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T22:36:23.288+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:36:23.288+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:36:23.318+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T22:36:23.318+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T22:36:23.850+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:36:23.868+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:36:23.868+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T22:36:23.886+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:36:23.886+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T22:36:23.900+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.617 seconds
[2024-07-24T22:36:54.057+0000] {processor.py:157} INFO - Started process (PID=1751) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:36:54.058+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T22:36:54.060+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:36:54.060+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:36:54.090+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T22:36:54.091+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T22:36:54.618+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:36:54.638+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:36:54.637+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T22:36:54.655+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:36:54.655+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T22:36:54.668+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.614 seconds
[2024-07-24T22:37:24.832+0000] {processor.py:157} INFO - Started process (PID=1753) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:37:24.833+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T22:37:24.835+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:37:24.835+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:37:24.864+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T22:37:24.864+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T22:37:25.412+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:37:25.430+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:37:25.429+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T22:37:25.451+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:37:25.451+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T22:37:25.467+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.638 seconds
[2024-07-24T22:37:55.633+0000] {processor.py:157} INFO - Started process (PID=1755) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:37:55.634+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T22:37:55.636+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:37:55.636+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:37:55.666+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T22:37:55.666+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T22:37:56.211+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:37:56.230+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:37:56.229+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T22:37:56.248+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:37:56.248+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T22:37:56.263+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.633 seconds
[2024-07-24T22:38:26.439+0000] {processor.py:157} INFO - Started process (PID=1757) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:38:26.440+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T22:38:26.442+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:38:26.441+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:38:26.471+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T22:38:26.471+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T22:38:27.010+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:38:27.029+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:38:27.028+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T22:38:27.049+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:38:27.049+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T22:38:27.064+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.628 seconds
[2024-07-24T22:38:57.227+0000] {processor.py:157} INFO - Started process (PID=1759) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:38:57.228+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T22:38:57.230+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:38:57.230+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:38:57.260+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T22:38:57.260+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T22:38:57.837+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:38:57.861+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:38:57.860+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T22:38:57.882+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:38:57.882+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T22:38:57.900+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.675 seconds
[2024-07-24T22:39:28.056+0000] {processor.py:157} INFO - Started process (PID=1761) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:39:28.057+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T22:39:28.059+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:39:28.059+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:39:28.091+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T22:39:28.092+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T22:39:28.654+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:39:28.673+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:39:28.673+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T22:39:28.691+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:39:28.691+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T22:39:28.705+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.651 seconds
[2024-07-24T22:39:58.894+0000] {processor.py:157} INFO - Started process (PID=1763) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:39:58.895+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T22:39:58.898+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:39:58.898+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:39:58.927+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T22:39:58.928+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T22:39:59.466+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:39:59.485+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:39:59.484+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T22:39:59.504+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:39:59.504+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T22:39:59.517+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.627 seconds
[2024-07-24T22:40:29.684+0000] {processor.py:157} INFO - Started process (PID=1765) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:40:29.685+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T22:40:29.688+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:40:29.687+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:40:29.718+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T22:40:29.719+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T22:40:30.269+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:40:30.287+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:40:30.286+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T22:40:30.306+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:40:30.306+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T22:40:30.321+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.640 seconds
[2024-07-24T22:41:00.530+0000] {processor.py:157} INFO - Started process (PID=1767) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:41:00.532+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T22:41:00.536+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:41:00.536+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:41:00.578+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T22:41:00.579+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T22:41:01.444+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:41:01.489+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:41:01.488+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T22:41:01.521+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:41:01.521+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T22:41:01.543+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 1.017 seconds
[2024-07-24T22:41:31.699+0000] {processor.py:157} INFO - Started process (PID=1769) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:41:31.700+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T22:41:31.702+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:41:31.701+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:41:31.737+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T22:41:31.738+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T22:41:32.402+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:41:32.424+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:41:32.423+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T22:41:32.450+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:41:32.450+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T22:41:32.471+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.774 seconds
[2024-07-24T22:42:02.666+0000] {processor.py:157} INFO - Started process (PID=1771) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:42:02.667+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T22:42:02.669+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:42:02.668+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:42:02.700+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T22:42:02.700+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T22:42:03.294+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:42:03.316+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:42:03.316+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T22:42:03.338+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:42:03.338+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T22:42:03.357+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.695 seconds
[2024-07-24T22:42:33.554+0000] {processor.py:157} INFO - Started process (PID=1773) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:42:33.555+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T22:42:33.561+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:42:33.560+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:42:33.605+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T22:42:33.606+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T22:42:34.224+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:42:34.245+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:42:34.244+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T22:42:34.268+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:42:34.268+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T22:42:34.286+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.736 seconds
[2024-07-24T22:43:04.502+0000] {processor.py:157} INFO - Started process (PID=1775) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:43:04.503+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T22:43:04.506+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:43:04.506+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:43:04.554+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T22:43:04.554+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T22:43:05.311+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:43:05.341+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:43:05.340+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T22:43:05.371+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:43:05.371+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T22:43:05.406+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.910 seconds
[2024-07-24T22:43:35.600+0000] {processor.py:157} INFO - Started process (PID=1777) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:43:35.602+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T22:43:35.604+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:43:35.604+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:43:35.638+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T22:43:35.639+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T22:43:36.203+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:43:36.223+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:43:36.223+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T22:43:36.244+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:43:36.243+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T22:43:36.260+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.663 seconds
[2024-07-24T22:44:06.437+0000] {processor.py:157} INFO - Started process (PID=1779) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:44:06.438+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T22:44:06.440+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:44:06.440+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:44:06.474+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T22:44:06.474+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T22:44:07.044+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:44:07.073+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:44:07.073+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T22:44:07.099+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:44:07.099+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T22:44:07.116+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.683 seconds
[2024-07-24T22:44:37.282+0000] {processor.py:157} INFO - Started process (PID=1781) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:44:37.283+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T22:44:37.284+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:44:37.284+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:44:37.315+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T22:44:37.315+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T22:44:37.872+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:44:37.892+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:44:37.891+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T22:44:37.914+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:44:37.914+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T22:44:37.935+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.656 seconds
[2024-07-24T22:45:08.013+0000] {processor.py:157} INFO - Started process (PID=1783) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:45:08.014+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T22:45:08.016+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:45:08.016+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:45:08.049+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T22:45:08.049+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T22:45:08.557+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:45:08.579+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:45:08.579+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T22:45:08.597+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:45:08.597+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T22:45:08.613+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.605 seconds
[2024-07-24T22:45:38.776+0000] {processor.py:157} INFO - Started process (PID=1785) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:45:38.777+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T22:45:38.779+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:45:38.778+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:45:38.809+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T22:45:38.810+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T22:45:39.366+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:45:39.384+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:45:39.383+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T22:45:39.403+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:45:39.403+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T22:45:39.417+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.644 seconds
[2024-07-24T22:46:09.564+0000] {processor.py:157} INFO - Started process (PID=1787) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:46:09.565+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T22:46:09.567+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:46:09.567+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:46:09.595+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T22:46:09.596+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T22:46:10.106+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:46:10.122+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:46:10.122+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T22:46:10.141+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:46:10.141+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T22:46:10.154+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.592 seconds
[2024-07-24T22:46:40.318+0000] {processor.py:157} INFO - Started process (PID=1789) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:46:40.319+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T22:46:40.321+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:46:40.321+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:46:40.354+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T22:46:40.355+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T22:46:40.840+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:46:40.857+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:46:40.857+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T22:46:40.875+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:46:40.875+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T22:46:40.893+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.578 seconds
[2024-07-24T22:47:11.071+0000] {processor.py:157} INFO - Started process (PID=1791) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:47:11.072+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T22:47:11.074+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:47:11.074+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:47:11.105+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T22:47:11.105+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T22:47:11.636+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:47:11.654+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:47:11.653+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T22:47:11.674+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:47:11.673+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T22:47:11.688+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.620 seconds
[2024-07-24T22:47:41.862+0000] {processor.py:157} INFO - Started process (PID=1793) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:47:41.863+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T22:47:41.865+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:47:41.865+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:47:41.895+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T22:47:41.896+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T22:47:42.488+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:47:42.548+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:47:42.547+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T22:47:42.567+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:47:42.566+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T22:47:42.580+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.721 seconds
[2024-07-24T22:48:12.746+0000] {processor.py:157} INFO - Started process (PID=1795) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:48:12.747+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T22:48:12.749+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:48:12.749+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:48:12.782+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T22:48:12.782+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T22:48:13.297+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:48:13.314+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:48:13.313+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T22:48:13.331+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:48:13.330+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T22:48:13.344+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.601 seconds
[2024-07-24T22:48:43.479+0000] {processor.py:157} INFO - Started process (PID=1797) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:48:43.480+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T22:48:43.481+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:48:43.481+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:48:43.514+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T22:48:43.514+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T22:48:44.040+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:48:44.057+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:48:44.056+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T22:48:44.073+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:48:44.073+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T22:48:44.098+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.622 seconds
[2024-07-24T22:49:14.265+0000] {processor.py:157} INFO - Started process (PID=1799) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:49:14.266+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T22:49:14.267+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:49:14.267+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:49:14.295+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T22:49:14.296+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T22:49:14.858+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:49:14.876+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:49:14.876+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T22:49:14.894+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:49:14.894+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T22:49:14.909+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.647 seconds
[2024-07-24T22:49:45.068+0000] {processor.py:157} INFO - Started process (PID=1801) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:49:45.069+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T22:49:45.072+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:49:45.072+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:49:45.102+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T22:49:45.103+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T22:49:45.634+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:49:45.652+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:49:45.652+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T22:49:45.672+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:49:45.672+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T22:49:45.690+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.625 seconds
[2024-07-24T22:50:15.858+0000] {processor.py:157} INFO - Started process (PID=1803) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:50:15.859+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T22:50:15.862+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:50:15.861+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:50:15.890+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T22:50:15.891+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T22:50:16.443+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:50:16.461+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:50:16.460+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T22:50:16.482+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:50:16.482+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T22:50:16.498+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.643 seconds
[2024-07-24T22:50:46.657+0000] {processor.py:157} INFO - Started process (PID=1805) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:50:46.658+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T22:50:46.660+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:50:46.660+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:50:46.692+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T22:50:46.692+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T22:50:47.242+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:50:47.263+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:50:47.263+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T22:50:47.281+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:50:47.281+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T22:50:47.298+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.644 seconds
[2024-07-24T22:51:17.469+0000] {processor.py:157} INFO - Started process (PID=1807) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:51:17.470+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T22:51:17.472+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:51:17.472+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:51:17.502+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T22:51:17.502+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T22:51:18.046+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:51:18.064+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:51:18.064+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T22:51:18.082+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:51:18.082+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T22:51:18.101+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.635 seconds
[2024-07-24T22:51:48.284+0000] {processor.py:157} INFO - Started process (PID=1809) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:51:48.285+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T22:51:48.286+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:51:48.286+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:51:48.317+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T22:51:48.317+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T22:51:48.816+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:51:48.834+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:51:48.834+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T22:51:48.853+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:51:48.853+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T22:51:48.868+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.587 seconds
[2024-07-24T22:52:19.057+0000] {processor.py:157} INFO - Started process (PID=1811) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:52:19.059+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T22:52:19.071+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:52:19.071+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:52:19.137+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T22:52:19.138+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T22:52:20.075+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:52:20.098+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:52:20.097+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T22:52:20.127+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:52:20.127+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T22:52:20.149+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 1.097 seconds
[2024-07-24T22:52:50.219+0000] {processor.py:157} INFO - Started process (PID=1813) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:52:50.219+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T22:52:50.222+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:52:50.221+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:52:50.251+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T22:52:50.252+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T22:52:50.804+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:52:50.824+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:52:50.823+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T22:52:50.843+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:52:50.843+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T22:52:50.897+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.682 seconds
[2024-07-24T22:53:21.084+0000] {processor.py:157} INFO - Started process (PID=1815) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:53:21.085+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T22:53:21.088+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:53:21.088+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:53:21.118+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T22:53:21.118+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T22:53:21.739+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:53:21.758+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:53:21.758+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T22:53:21.777+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:53:21.777+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T22:53:21.792+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.715 seconds
[2024-07-24T22:53:51.953+0000] {processor.py:157} INFO - Started process (PID=1817) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:53:51.955+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T22:53:51.957+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:53:51.957+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:53:51.988+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T22:53:51.989+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T22:53:52.540+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:53:52.559+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:53:52.559+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T22:53:52.579+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:53:52.579+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T22:53:52.594+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.648 seconds
[2024-07-24T22:54:22.734+0000] {processor.py:157} INFO - Started process (PID=1819) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:54:22.735+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T22:54:22.737+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:54:22.737+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:54:22.767+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T22:54:22.768+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T22:54:23.308+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:54:23.327+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:54:23.327+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T22:54:23.345+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:54:23.345+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T22:54:23.363+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.631 seconds
[2024-07-24T22:54:53.531+0000] {processor.py:157} INFO - Started process (PID=1821) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:54:53.532+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T22:54:53.534+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:54:53.534+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:54:53.567+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T22:54:53.567+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T22:54:54.128+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:54:54.146+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:54:54.145+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T22:54:54.164+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:54:54.164+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T22:54:54.178+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.649 seconds
[2024-07-24T22:55:24.335+0000] {processor.py:157} INFO - Started process (PID=1823) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:55:24.336+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T22:55:24.337+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:55:24.337+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:55:24.366+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T22:55:24.367+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T22:55:24.910+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:55:24.927+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:55:24.927+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T22:55:24.945+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:55:24.945+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T22:55:24.964+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.632 seconds
[2024-07-24T22:55:55.131+0000] {processor.py:157} INFO - Started process (PID=1825) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:55:55.132+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T22:55:55.134+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:55:55.134+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:55:55.163+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T22:55:55.163+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T22:55:55.685+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:55:55.721+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:55:55.720+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T22:55:55.739+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:55:55.739+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T22:55:55.752+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.624 seconds
[2024-07-24T22:56:25.909+0000] {processor.py:157} INFO - Started process (PID=1827) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:56:25.910+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T22:56:25.912+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:56:25.912+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:56:25.943+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T22:56:25.943+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T22:56:26.537+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:56:26.556+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:56:26.556+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T22:56:26.575+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:56:26.574+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T22:56:26.590+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.683 seconds
[2024-07-24T22:56:56.786+0000] {processor.py:157} INFO - Started process (PID=1829) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:56:56.787+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T22:56:56.789+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:56:56.789+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:56:56.819+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T22:56:56.820+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T22:56:57.419+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:56:57.437+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:56:57.436+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T22:56:57.456+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:56:57.456+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T22:56:57.472+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.689 seconds
[2024-07-24T22:57:27.647+0000] {processor.py:157} INFO - Started process (PID=1831) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:57:27.648+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T22:57:27.650+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:57:27.650+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:57:27.681+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T22:57:27.681+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T22:57:28.227+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:57:28.246+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:57:28.245+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T22:57:28.265+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:57:28.265+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T22:57:28.284+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.639 seconds
[2024-07-24T22:57:58.441+0000] {processor.py:157} INFO - Started process (PID=1833) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:57:58.442+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T22:57:58.444+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:57:58.443+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:57:58.491+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T22:57:58.492+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T22:57:59.051+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:57:59.070+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:57:59.069+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T22:57:59.089+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:57:59.089+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T22:57:59.103+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.665 seconds
[2024-07-24T22:58:29.210+0000] {processor.py:157} INFO - Started process (PID=1835) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:58:29.211+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T22:58:29.213+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:58:29.213+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:58:29.243+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T22:58:29.244+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T22:58:29.789+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:58:29.808+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:58:29.807+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T22:58:29.827+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:58:29.826+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T22:58:29.850+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.643 seconds
[2024-07-24T22:59:00.016+0000] {processor.py:157} INFO - Started process (PID=1837) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:59:00.017+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T22:59:00.019+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:59:00.019+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:59:00.049+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T22:59:00.050+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T22:59:00.588+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:59:00.604+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:59:00.604+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T22:59:00.621+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:59:00.621+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T22:59:00.636+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.623 seconds
[2024-07-24T22:59:30.793+0000] {processor.py:157} INFO - Started process (PID=1839) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:59:30.794+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T22:59:30.796+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:59:30.796+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:59:30.825+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T22:59:30.825+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T22:59:31.367+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T22:59:31.389+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:59:31.388+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T22:59:31.409+0000] {logging_mixin.py:151} INFO - [2024-07-24T22:59:31.409+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T22:59:31.425+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.635 seconds
[2024-07-24T23:00:01.596+0000] {processor.py:157} INFO - Started process (PID=1841) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:00:01.597+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T23:00:01.600+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:00:01.599+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:00:01.631+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T23:00:01.632+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T23:00:02.199+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:00:02.219+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:00:02.218+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T23:00:02.238+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:00:02.237+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T23:00:02.254+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.660 seconds
[2024-07-24T23:00:32.414+0000] {processor.py:157} INFO - Started process (PID=1843) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:00:32.415+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T23:00:32.417+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:00:32.416+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:00:32.447+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T23:00:32.447+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T23:00:33.007+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:00:33.025+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:00:33.024+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T23:00:33.043+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:00:33.042+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T23:00:33.055+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.645 seconds
[2024-07-24T23:01:03.210+0000] {processor.py:157} INFO - Started process (PID=1845) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:01:03.211+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T23:01:03.213+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:01:03.213+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:01:03.243+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T23:01:03.243+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T23:01:03.765+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:01:03.784+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:01:03.783+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T23:01:03.803+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:01:03.803+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T23:01:03.819+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.612 seconds
[2024-07-24T23:01:33.986+0000] {processor.py:157} INFO - Started process (PID=1847) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:01:33.987+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T23:01:33.989+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:01:33.989+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:01:34.018+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T23:01:34.019+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T23:01:34.553+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:01:34.573+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:01:34.572+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T23:01:34.593+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:01:34.593+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T23:01:34.610+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.626 seconds
[2024-07-24T23:02:04.764+0000] {processor.py:157} INFO - Started process (PID=1849) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:02:04.765+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T23:02:04.767+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:02:04.767+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:02:04.799+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T23:02:04.800+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T23:02:05.345+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:02:05.369+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:02:05.368+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T23:02:05.385+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:02:05.385+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T23:02:05.399+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.638 seconds
[2024-07-24T23:02:35.560+0000] {processor.py:157} INFO - Started process (PID=1851) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:02:35.561+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T23:02:35.563+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:02:35.563+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:02:35.591+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T23:02:35.591+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T23:02:36.124+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:02:36.143+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:02:36.143+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T23:02:36.163+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:02:36.162+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T23:02:36.179+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.622 seconds
[2024-07-24T23:03:06.334+0000] {processor.py:157} INFO - Started process (PID=1853) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:03:06.334+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T23:03:06.336+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:03:06.336+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:03:06.366+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T23:03:06.367+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T23:03:06.912+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:03:06.929+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:03:06.928+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T23:03:06.946+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:03:06.946+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T23:03:06.960+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.630 seconds
[2024-07-24T23:03:37.129+0000] {processor.py:157} INFO - Started process (PID=1855) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:03:37.130+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T23:03:37.132+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:03:37.132+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:03:37.164+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T23:03:37.165+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T23:03:37.714+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:03:37.733+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:03:37.733+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T23:03:37.755+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:03:37.755+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T23:03:37.771+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.645 seconds
[2024-07-24T23:04:07.929+0000] {processor.py:157} INFO - Started process (PID=1857) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:04:07.930+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T23:04:07.932+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:04:07.932+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:04:07.962+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T23:04:07.962+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T23:04:08.471+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:04:08.493+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:04:08.492+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T23:04:08.511+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:04:08.511+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T23:04:08.525+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.598 seconds
[2024-07-24T23:04:38.688+0000] {processor.py:157} INFO - Started process (PID=1859) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:04:38.689+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T23:04:38.690+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:04:38.690+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:04:38.719+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T23:04:38.720+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T23:04:39.266+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:04:39.285+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:04:39.284+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T23:04:39.303+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:04:39.303+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T23:04:39.316+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.631 seconds
[2024-07-24T23:05:09.473+0000] {processor.py:157} INFO - Started process (PID=1861) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:05:09.474+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T23:05:09.476+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:05:09.476+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:05:09.504+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T23:05:09.504+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T23:05:10.023+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:05:10.043+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:05:10.042+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T23:05:10.064+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:05:10.064+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T23:05:10.081+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.611 seconds
[2024-07-24T23:05:40.262+0000] {processor.py:157} INFO - Started process (PID=1863) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:05:40.263+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T23:05:40.266+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:05:40.266+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:05:40.302+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T23:05:40.302+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T23:05:40.891+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:05:40.909+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:05:40.909+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T23:05:40.927+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:05:40.927+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T23:05:40.942+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.687 seconds
[2024-07-24T23:06:11.104+0000] {processor.py:157} INFO - Started process (PID=1865) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:06:11.104+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T23:06:11.108+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:06:11.107+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:06:11.137+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T23:06:11.137+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T23:06:11.697+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:06:11.716+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:06:11.716+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T23:06:11.735+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:06:11.735+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T23:06:11.751+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.650 seconds
[2024-07-24T23:06:41.912+0000] {processor.py:157} INFO - Started process (PID=1867) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:06:41.913+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T23:06:41.914+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:06:41.914+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:06:41.947+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T23:06:41.948+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T23:06:42.579+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:06:42.600+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:06:42.599+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T23:06:42.621+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:06:42.620+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T23:06:42.636+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.728 seconds
[2024-07-24T23:07:12.850+0000] {processor.py:157} INFO - Started process (PID=1869) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:07:12.851+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T23:07:12.853+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:07:12.853+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:07:12.885+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T23:07:12.885+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T23:07:13.496+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:07:13.516+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:07:13.515+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T23:07:13.534+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:07:13.534+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T23:07:13.550+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.704 seconds
[2024-07-24T23:07:43.731+0000] {processor.py:157} INFO - Started process (PID=1871) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:07:43.732+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T23:07:43.735+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:07:43.734+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:07:43.771+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T23:07:43.771+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T23:07:44.360+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:07:44.385+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:07:44.385+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T23:07:44.409+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:07:44.408+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T23:07:44.424+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.696 seconds
[2024-07-24T23:08:14.594+0000] {processor.py:157} INFO - Started process (PID=1873) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:08:14.594+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T23:08:14.597+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:08:14.596+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:08:14.628+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T23:08:14.628+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T23:08:15.365+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:08:15.390+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:08:15.389+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T23:08:15.416+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:08:15.416+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T23:08:15.438+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.847 seconds
[2024-07-24T23:08:45.527+0000] {processor.py:157} INFO - Started process (PID=1875) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:08:45.528+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T23:08:45.531+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:08:45.530+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:08:45.565+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T23:08:45.565+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T23:08:46.282+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:08:46.301+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:08:46.300+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T23:08:46.323+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:08:46.323+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T23:08:46.341+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.817 seconds
[2024-07-24T23:09:16.529+0000] {processor.py:157} INFO - Started process (PID=1877) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:09:16.530+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T23:09:16.533+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:09:16.533+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:09:16.563+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T23:09:16.564+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T23:09:17.118+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:09:17.140+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:09:17.140+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T23:09:17.163+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:09:17.162+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T23:09:17.183+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.657 seconds
[2024-07-24T23:09:47.367+0000] {processor.py:157} INFO - Started process (PID=1879) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:09:47.368+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T23:09:47.371+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:09:47.370+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:09:47.419+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T23:09:47.420+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T23:09:48.100+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:09:48.125+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:09:48.124+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T23:09:48.147+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:09:48.147+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T23:09:48.167+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.803 seconds
[2024-07-24T23:10:18.356+0000] {processor.py:157} INFO - Started process (PID=1881) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:10:18.358+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T23:10:18.360+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:10:18.360+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:10:18.402+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T23:10:18.402+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T23:10:19.104+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:10:19.130+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:10:19.129+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T23:10:19.166+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:10:19.166+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T23:10:19.198+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.846 seconds
[2024-07-24T23:10:49.379+0000] {processor.py:157} INFO - Started process (PID=1883) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:10:49.380+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T23:10:49.382+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:10:49.381+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:10:49.414+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T23:10:49.414+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T23:10:50.013+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:10:50.039+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:10:50.039+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T23:10:50.064+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:10:50.064+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T23:10:50.081+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.705 seconds
[2024-07-24T23:11:20.265+0000] {processor.py:157} INFO - Started process (PID=1885) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:11:20.266+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T23:11:20.268+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:11:20.268+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:11:20.299+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T23:11:20.300+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T23:11:20.844+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:11:20.862+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:11:20.861+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T23:11:20.881+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:11:20.880+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T23:11:20.900+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.638 seconds
[2024-07-24T23:11:51.091+0000] {processor.py:157} INFO - Started process (PID=1887) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:11:51.092+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T23:11:51.094+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:11:51.094+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:11:51.128+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T23:11:51.128+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T23:11:51.770+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:11:51.790+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:11:51.790+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T23:11:51.812+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:11:51.812+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T23:11:51.830+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.743 seconds
[2024-07-24T23:12:21.989+0000] {processor.py:157} INFO - Started process (PID=1889) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:12:21.989+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T23:12:21.991+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:12:21.991+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:12:22.023+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T23:12:22.023+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T23:12:22.613+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:12:22.634+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:12:22.633+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T23:12:22.656+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:12:22.656+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T23:12:22.676+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.690 seconds
[2024-07-24T23:12:52.852+0000] {processor.py:157} INFO - Started process (PID=1891) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:12:52.853+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T23:12:52.855+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:12:52.855+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:12:52.885+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T23:12:52.885+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T23:12:53.480+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:12:53.506+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:12:53.505+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T23:12:53.529+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:12:53.529+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T23:12:53.550+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.701 seconds
[2024-07-24T23:13:23.715+0000] {processor.py:157} INFO - Started process (PID=1893) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:13:23.716+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T23:13:23.719+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:13:23.718+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:13:23.750+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T23:13:23.750+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T23:13:24.306+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:13:24.326+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:13:24.326+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T23:13:24.346+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:13:24.346+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T23:13:24.364+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.651 seconds
[2024-07-24T23:13:54.413+0000] {processor.py:157} INFO - Started process (PID=1895) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:13:54.414+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T23:13:54.416+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:13:54.416+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:13:54.449+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T23:13:54.449+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T23:13:55.038+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:13:55.057+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:13:55.057+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T23:13:55.077+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:13:55.077+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T23:13:55.094+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.684 seconds
[2024-07-24T23:14:25.271+0000] {processor.py:157} INFO - Started process (PID=1897) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:14:25.272+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T23:14:25.274+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:14:25.274+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:14:25.307+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T23:14:25.307+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T23:14:25.892+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:14:25.911+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:14:25.910+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T23:14:25.929+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:14:25.929+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T23:14:25.949+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.681 seconds
[2024-07-24T23:14:56.114+0000] {processor.py:157} INFO - Started process (PID=1899) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:14:56.115+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T23:14:56.117+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:14:56.117+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:14:56.150+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T23:14:56.150+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T23:14:56.928+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:14:56.954+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:14:56.953+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T23:14:56.973+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:14:56.973+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T23:14:56.992+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.881 seconds
[2024-07-24T23:15:27.157+0000] {processor.py:157} INFO - Started process (PID=1901) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:15:27.158+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T23:15:27.160+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:15:27.160+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:15:27.194+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T23:15:27.194+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T23:15:27.850+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:15:27.873+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:15:27.872+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T23:15:27.901+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:15:27.901+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T23:15:27.923+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.770 seconds
[2024-07-24T23:15:58.105+0000] {processor.py:157} INFO - Started process (PID=1903) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:15:58.106+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T23:15:58.108+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:15:58.107+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:15:58.139+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T23:15:58.139+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T23:15:58.756+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:15:58.774+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:15:58.773+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T23:15:58.794+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:15:58.794+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T23:15:58.810+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.709 seconds
[2024-07-24T23:16:28.996+0000] {processor.py:157} INFO - Started process (PID=1905) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:16:28.997+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T23:16:28.999+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:16:28.999+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:16:29.033+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T23:16:29.033+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T23:16:29.602+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:16:29.624+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:16:29.624+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T23:16:29.646+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:16:29.646+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T23:16:29.664+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.672 seconds
[2024-07-24T23:16:59.825+0000] {processor.py:157} INFO - Started process (PID=1907) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:16:59.828+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T23:16:59.830+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:16:59.829+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:16:59.861+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T23:16:59.862+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T23:17:00.438+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:17:00.458+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:17:00.458+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T23:17:00.478+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:17:00.478+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T23:17:00.494+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.671 seconds
[2024-07-24T23:17:30.674+0000] {processor.py:157} INFO - Started process (PID=1909) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:17:30.675+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T23:17:30.677+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:17:30.677+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:17:30.712+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T23:17:30.712+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T23:17:31.271+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:17:31.295+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:17:31.294+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T23:17:31.315+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:17:31.314+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T23:17:31.333+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.661 seconds
[2024-07-24T23:18:01.496+0000] {processor.py:157} INFO - Started process (PID=1911) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:18:01.497+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T23:18:01.499+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:18:01.499+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:18:01.532+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T23:18:01.532+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T23:18:02.114+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:18:02.136+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:18:02.135+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T23:18:02.158+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:18:02.158+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T23:18:02.174+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.681 seconds
[2024-07-24T23:18:32.248+0000] {processor.py:157} INFO - Started process (PID=1913) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:18:32.249+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T23:18:32.252+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:18:32.252+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:18:32.284+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T23:18:32.284+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T23:18:32.847+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:18:32.865+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:18:32.864+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T23:18:32.887+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:18:32.886+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T23:18:32.910+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.664 seconds
[2024-07-24T23:19:03.098+0000] {processor.py:157} INFO - Started process (PID=1915) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:19:03.099+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T23:19:03.108+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:19:03.107+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:19:03.146+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T23:19:03.147+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T23:19:03.755+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:19:03.775+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:19:03.775+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T23:19:03.796+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:19:03.796+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T23:19:03.813+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.718 seconds
[2024-07-24T23:19:34.003+0000] {processor.py:157} INFO - Started process (PID=1917) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:19:34.005+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T23:19:34.008+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:19:34.008+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:19:34.043+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T23:19:34.043+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T23:19:34.603+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:19:34.624+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:19:34.623+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T23:19:34.641+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:19:34.641+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T23:19:34.657+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.659 seconds
[2024-07-24T23:20:04.776+0000] {processor.py:157} INFO - Started process (PID=1919) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:20:04.777+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T23:20:04.778+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:20:04.778+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:20:04.810+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T23:20:04.811+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T23:20:05.348+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:20:05.367+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:20:05.366+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T23:20:05.386+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:20:05.386+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T23:20:05.403+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.631 seconds
[2024-07-24T23:20:35.578+0000] {processor.py:157} INFO - Started process (PID=1921) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:20:35.579+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T23:20:35.581+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:20:35.581+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:20:35.619+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T23:20:35.620+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T23:20:36.243+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:20:36.267+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:20:36.267+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T23:20:36.288+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:20:36.287+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T23:20:36.304+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.729 seconds
[2024-07-24T23:21:06.478+0000] {processor.py:157} INFO - Started process (PID=1923) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:21:06.480+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T23:21:06.483+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:21:06.482+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:21:06.517+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T23:21:06.517+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T23:21:07.131+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:21:07.154+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:21:07.153+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T23:21:07.174+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:21:07.174+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T23:21:07.190+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.715 seconds
[2024-07-24T23:21:37.251+0000] {processor.py:157} INFO - Started process (PID=1925) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:21:37.252+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T23:21:37.255+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:21:37.255+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:21:37.289+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T23:21:37.289+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T23:21:37.896+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:21:37.915+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:21:37.915+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T23:21:37.937+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:21:37.936+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T23:21:37.951+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.704 seconds
[2024-07-24T23:22:08.168+0000] {processor.py:157} INFO - Started process (PID=1927) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:22:08.169+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T23:22:08.171+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:22:08.171+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:22:08.205+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T23:22:08.206+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T23:22:08.863+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:22:08.886+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:22:08.885+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T23:22:08.908+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:22:08.908+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T23:22:08.924+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.761 seconds
[2024-07-24T23:22:39.095+0000] {processor.py:157} INFO - Started process (PID=1929) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:22:39.096+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T23:22:39.098+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:22:39.098+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:22:39.127+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T23:22:39.128+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T23:22:39.728+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:22:39.747+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:22:39.746+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T23:22:39.766+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:22:39.766+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T23:22:39.781+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.690 seconds
[2024-07-24T23:23:09.957+0000] {processor.py:157} INFO - Started process (PID=1931) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:23:09.958+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T23:23:09.960+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:23:09.959+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:23:09.995+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T23:23:09.996+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T23:23:10.607+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:23:10.629+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:23:10.628+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T23:23:10.648+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:23:10.648+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T23:23:10.664+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.711 seconds
[2024-07-24T23:23:40.864+0000] {processor.py:157} INFO - Started process (PID=1933) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:23:40.865+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T23:23:40.866+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:23:40.866+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:23:40.902+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T23:23:40.902+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T23:23:41.572+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:23:41.598+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:23:41.597+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T23:23:41.631+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:23:41.631+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T23:23:41.655+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.795 seconds
[2024-07-24T23:24:11.840+0000] {processor.py:157} INFO - Started process (PID=1935) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:24:11.842+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T23:24:11.845+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:24:11.845+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:24:11.878+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T23:24:11.878+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T23:24:12.529+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:24:12.554+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:24:12.554+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T23:24:12.583+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:24:12.582+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T23:24:12.605+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.769 seconds
[2024-07-24T23:24:42.782+0000] {processor.py:157} INFO - Started process (PID=1937) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:24:42.783+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T23:24:42.786+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:24:42.785+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:24:42.818+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T23:24:42.819+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T23:24:43.386+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:24:43.408+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:24:43.408+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T23:24:43.427+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:24:43.427+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T23:24:43.443+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.665 seconds
[2024-07-24T23:25:13.544+0000] {processor.py:157} INFO - Started process (PID=1939) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:25:13.545+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T23:25:13.547+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:25:13.547+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:25:13.576+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T23:25:13.576+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T23:25:14.182+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:25:14.217+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:25:14.216+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T23:25:14.255+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:25:14.255+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T23:25:14.279+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.738 seconds
[2024-07-24T23:25:44.467+0000] {processor.py:157} INFO - Started process (PID=1941) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:25:44.468+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T23:25:44.470+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:25:44.470+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:25:44.508+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T23:25:44.509+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T23:25:45.132+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:25:45.157+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:25:45.156+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T23:25:45.180+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:25:45.180+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T23:25:45.197+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.734 seconds
[2024-07-24T23:26:15.376+0000] {processor.py:157} INFO - Started process (PID=1943) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:26:15.377+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T23:26:15.379+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:26:15.379+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:26:15.412+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T23:26:15.412+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T23:26:15.950+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:26:15.969+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:26:15.968+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T23:26:15.988+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:26:15.988+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T23:26:16.009+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.635 seconds
[2024-07-24T23:26:46.172+0000] {processor.py:157} INFO - Started process (PID=1945) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:26:46.173+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T23:26:46.175+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:26:46.175+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:26:46.208+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T23:26:46.209+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T23:26:46.788+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:26:46.808+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:26:46.807+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T23:26:46.829+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:26:46.829+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T23:26:46.844+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.675 seconds
[2024-07-24T23:27:17.029+0000] {processor.py:157} INFO - Started process (PID=1947) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:27:17.030+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T23:27:17.032+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:27:17.032+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:27:17.069+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T23:27:17.069+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T23:27:17.682+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:27:17.705+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:27:17.704+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T23:27:17.726+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:27:17.726+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T23:27:17.744+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.718 seconds
[2024-07-24T23:27:47.923+0000] {processor.py:157} INFO - Started process (PID=1949) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:27:47.925+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T23:27:47.928+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:27:47.927+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:27:47.960+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T23:27:47.960+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T23:27:48.541+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:27:48.564+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:27:48.564+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T23:27:48.588+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:27:48.588+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T23:27:48.606+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.687 seconds
[2024-07-24T23:28:18.646+0000] {processor.py:157} INFO - Started process (PID=1951) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:28:18.647+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T23:28:18.649+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:28:18.649+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:28:18.683+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T23:28:18.684+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T23:28:19.312+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:28:19.330+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:28:19.330+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T23:28:19.349+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:28:19.349+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T23:28:19.364+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.723 seconds
[2024-07-24T23:28:49.550+0000] {processor.py:157} INFO - Started process (PID=1953) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:28:49.551+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T23:28:49.552+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:28:49.552+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:28:49.585+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T23:28:49.585+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T23:28:50.145+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:28:50.165+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:28:50.164+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T23:28:50.182+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:28:50.182+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T23:28:50.196+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.658 seconds
[2024-07-24T23:29:20.365+0000] {processor.py:157} INFO - Started process (PID=1955) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:29:20.366+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T23:29:20.368+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:29:20.368+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:29:20.398+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T23:29:20.399+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T23:29:20.986+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:29:21.012+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:29:21.011+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T23:29:21.042+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:29:21.041+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T23:29:21.063+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.702 seconds
[2024-07-24T23:29:51.222+0000] {processor.py:157} INFO - Started process (PID=1957) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:29:51.223+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T23:29:51.225+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:29:51.225+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:29:51.265+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T23:29:51.266+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T23:29:51.889+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:29:51.909+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:29:51.909+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T23:29:51.936+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:29:51.936+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T23:29:51.955+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.736 seconds
[2024-07-24T23:30:22.188+0000] {processor.py:157} INFO - Started process (PID=1959) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:30:22.189+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T23:30:22.209+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:30:22.208+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:30:22.334+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T23:30:22.335+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T23:30:22.999+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:30:23.026+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:30:23.025+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T23:30:23.055+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:30:23.055+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T23:30:23.073+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.889 seconds
[2024-07-24T23:30:53.262+0000] {processor.py:157} INFO - Started process (PID=1961) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:30:53.263+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T23:30:53.265+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:30:53.265+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:30:53.298+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T23:30:53.298+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T23:30:53.858+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:30:53.880+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:30:53.880+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T23:30:53.900+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:30:53.899+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T23:30:53.915+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.655 seconds
[2024-07-24T23:31:24.100+0000] {processor.py:157} INFO - Started process (PID=1963) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:31:24.104+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T23:31:24.108+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:31:24.108+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:31:24.158+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T23:31:24.159+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T23:31:25.194+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:31:25.225+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:31:25.224+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T23:31:25.267+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:31:25.267+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T23:31:25.293+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 1.197 seconds
[2024-07-24T23:31:55.457+0000] {processor.py:157} INFO - Started process (PID=1965) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:31:55.459+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T23:31:55.462+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:31:55.462+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:31:55.499+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T23:31:55.499+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T23:31:56.314+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:31:56.369+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:31:56.368+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T23:31:56.446+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:31:56.446+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T23:31:56.508+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 1.055 seconds
[2024-07-24T23:32:26.614+0000] {processor.py:157} INFO - Started process (PID=1967) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:32:26.615+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T23:32:26.617+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:32:26.617+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:32:26.651+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T23:32:26.652+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T23:32:27.269+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:32:27.289+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:32:27.289+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T23:32:27.311+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:32:27.311+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T23:32:27.328+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.718 seconds
[2024-07-24T23:32:57.510+0000] {processor.py:157} INFO - Started process (PID=1969) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:32:57.511+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T23:32:57.514+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:32:57.514+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:32:57.549+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T23:32:57.550+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T23:32:58.142+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:32:58.165+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:32:58.165+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T23:32:58.188+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:32:58.188+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T23:32:58.206+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.699 seconds
[2024-07-24T23:33:28.379+0000] {processor.py:157} INFO - Started process (PID=1971) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:33:28.380+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T23:33:28.383+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:33:28.382+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:33:28.418+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T23:33:28.419+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T23:33:28.995+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:33:29.014+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:33:29.014+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T23:33:29.033+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:33:29.033+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T23:33:29.049+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.674 seconds
[2024-07-24T23:33:59.123+0000] {processor.py:157} INFO - Started process (PID=1973) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:33:59.124+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T23:33:59.127+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:33:59.127+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:33:59.158+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T23:33:59.159+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T23:33:59.713+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:33:59.736+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:33:59.735+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T23:33:59.755+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:33:59.755+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T23:33:59.775+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.654 seconds
[2024-07-24T23:34:29.947+0000] {processor.py:157} INFO - Started process (PID=1975) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:34:29.947+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T23:34:29.949+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:34:29.949+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:34:29.979+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T23:34:29.979+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T23:34:30.522+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:34:30.540+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:34:30.540+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T23:34:30.557+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:34:30.557+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T23:34:30.571+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.628 seconds
[2024-07-24T23:35:00.726+0000] {processor.py:157} INFO - Started process (PID=1977) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:35:00.727+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T23:35:00.729+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:35:00.729+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:35:00.759+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T23:35:00.760+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T23:35:01.352+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:35:01.373+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:35:01.372+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T23:35:01.400+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:35:01.399+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T23:35:01.418+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.695 seconds
[2024-07-24T23:35:31.604+0000] {processor.py:157} INFO - Started process (PID=1979) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:35:31.605+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T23:35:31.608+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:35:31.607+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:35:31.649+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T23:35:31.649+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T23:35:32.266+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:35:32.290+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:35:32.290+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T23:35:32.310+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:35:32.310+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T23:35:32.326+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.726 seconds
[2024-07-24T23:36:02.498+0000] {processor.py:157} INFO - Started process (PID=1981) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:36:02.499+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T23:36:02.501+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:36:02.501+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:36:02.533+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T23:36:02.534+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T23:36:03.132+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:36:03.154+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:36:03.153+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T23:36:03.181+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:36:03.180+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T23:36:03.203+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.708 seconds
[2024-07-24T23:36:33.433+0000] {processor.py:157} INFO - Started process (PID=1983) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:36:33.434+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T23:36:33.436+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:36:33.436+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:36:33.474+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T23:36:33.474+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T23:36:34.089+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:36:34.109+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:36:34.108+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T23:36:34.127+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:36:34.127+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T23:36:34.142+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.761 seconds
[2024-07-24T23:37:04.349+0000] {processor.py:157} INFO - Started process (PID=1985) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:37:04.350+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T23:37:04.352+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:37:04.352+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:37:04.385+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T23:37:04.385+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T23:37:05.055+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:37:05.077+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:37:05.076+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T23:37:05.097+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:37:05.096+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T23:37:05.113+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.770 seconds
[2024-07-24T23:37:35.300+0000] {processor.py:157} INFO - Started process (PID=1987) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:37:35.301+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T23:37:35.304+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:37:35.303+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:37:35.339+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T23:37:35.339+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T23:37:35.991+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:37:36.020+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:37:36.020+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T23:37:36.049+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:37:36.049+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T23:37:36.071+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.774 seconds
[2024-07-24T23:38:06.264+0000] {processor.py:157} INFO - Started process (PID=1989) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:38:06.264+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T23:38:06.266+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:38:06.266+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:38:06.303+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T23:38:06.304+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T23:38:06.912+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:38:06.934+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:38:06.933+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T23:38:06.957+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:38:06.956+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T23:38:06.972+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.717 seconds
[2024-07-24T23:38:37.150+0000] {processor.py:157} INFO - Started process (PID=1991) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:38:37.152+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T23:38:37.154+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:38:37.153+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:38:37.194+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T23:38:37.195+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T23:38:37.846+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:38:37.868+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:38:37.867+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T23:38:37.893+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:38:37.893+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T23:38:37.911+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.765 seconds
[2024-07-24T23:39:07.967+0000] {processor.py:157} INFO - Started process (PID=1993) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:39:07.968+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T23:39:07.970+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:39:07.969+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:39:08.006+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T23:39:08.006+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T23:39:08.697+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:39:08.724+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:39:08.723+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T23:39:08.751+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:39:08.750+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T23:39:08.777+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.814 seconds
[2024-07-24T23:39:38.960+0000] {processor.py:157} INFO - Started process (PID=1995) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:39:38.961+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T23:39:38.963+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:39:38.963+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:39:38.992+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T23:39:38.992+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T23:39:39.549+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:39:39.570+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:39:39.569+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T23:39:39.590+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:39:39.590+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T23:39:39.605+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.648 seconds
[2024-07-24T23:40:09.768+0000] {processor.py:157} INFO - Started process (PID=1997) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:40:09.769+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T23:40:09.771+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:40:09.771+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:40:09.801+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T23:40:09.802+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T23:40:10.348+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:40:10.368+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:40:10.368+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T23:40:10.390+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:40:10.390+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T23:40:10.409+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.644 seconds
[2024-07-24T23:40:40.500+0000] {processor.py:157} INFO - Started process (PID=1999) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:40:40.501+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T23:40:40.504+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:40:40.503+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:40:40.537+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T23:40:40.537+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T23:40:41.160+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:40:41.188+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:40:41.187+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T23:40:41.216+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:40:41.215+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T23:40:41.237+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.741 seconds
[2024-07-24T23:41:11.442+0000] {processor.py:157} INFO - Started process (PID=2001) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:41:11.444+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T23:41:11.446+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:41:11.446+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:41:11.480+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T23:41:11.481+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T23:41:12.148+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:41:12.176+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:41:12.175+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T23:41:12.213+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:41:12.213+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T23:41:12.242+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.803 seconds
[2024-07-24T23:41:42.438+0000] {processor.py:157} INFO - Started process (PID=2003) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:41:42.439+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T23:41:42.442+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:41:42.441+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:41:42.475+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T23:41:42.476+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T23:41:43.108+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:41:43.128+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:41:43.127+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T23:41:43.149+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:41:43.149+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T23:41:43.170+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.734 seconds
[2024-07-24T23:42:13.362+0000] {processor.py:157} INFO - Started process (PID=2005) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:42:13.363+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T23:42:13.365+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:42:13.365+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:42:13.395+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T23:42:13.396+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T23:42:13.946+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:42:13.965+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:42:13.965+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T23:42:13.983+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:42:13.983+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T23:42:13.999+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.640 seconds
[2024-07-24T23:42:44.174+0000] {processor.py:157} INFO - Started process (PID=2007) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:42:44.175+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T23:42:44.177+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:42:44.177+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:42:44.210+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T23:42:44.211+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T23:42:44.802+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:42:44.824+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:42:44.823+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T23:42:44.848+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:42:44.848+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T23:42:44.866+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.695 seconds
[2024-07-24T23:43:15.066+0000] {processor.py:157} INFO - Started process (PID=2009) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:43:15.069+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T23:43:15.076+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:43:15.074+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:43:15.117+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T23:43:15.118+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T23:43:16.138+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:43:16.173+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:43:16.172+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T23:43:16.200+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:43:16.200+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T23:43:16.221+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 1.167 seconds
[2024-07-24T23:43:46.387+0000] {processor.py:157} INFO - Started process (PID=2011) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:43:46.388+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T23:43:46.391+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:43:46.390+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:43:46.423+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T23:43:46.424+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T23:43:47.025+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:43:47.047+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:43:47.047+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T23:43:47.080+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:43:47.079+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T23:43:47.101+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.717 seconds
[2024-07-24T23:44:17.294+0000] {processor.py:157} INFO - Started process (PID=2013) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:44:17.295+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T23:44:17.297+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:44:17.297+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:44:17.338+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T23:44:17.338+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T23:44:17.979+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:44:18.006+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:44:18.006+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T23:44:18.041+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:44:18.041+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T23:44:18.064+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.775 seconds
[2024-07-24T23:44:48.265+0000] {processor.py:157} INFO - Started process (PID=2015) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:44:48.267+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T23:44:48.279+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:44:48.278+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:44:48.325+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T23:44:48.326+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T23:44:48.980+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:44:49.001+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:44:49.000+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T23:44:49.019+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:44:49.019+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T23:44:49.032+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.771 seconds
[2024-07-24T23:45:19.207+0000] {processor.py:157} INFO - Started process (PID=2017) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:45:19.208+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T23:45:19.210+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:45:19.209+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:45:19.243+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T23:45:19.243+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T23:45:19.860+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:45:19.889+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:45:19.888+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T23:45:19.908+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:45:19.908+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T23:45:19.924+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.721 seconds
[2024-07-24T23:45:49.989+0000] {processor.py:157} INFO - Started process (PID=2019) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:45:49.990+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T23:45:49.992+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:45:49.992+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:45:50.026+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T23:45:50.027+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T23:45:50.617+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:45:50.638+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:45:50.638+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T23:45:50.657+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:45:50.657+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T23:45:50.672+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.686 seconds
[2024-07-24T23:46:20.849+0000] {processor.py:157} INFO - Started process (PID=2021) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:46:20.850+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T23:46:20.852+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:46:20.852+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:46:20.883+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T23:46:20.883+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T23:46:21.483+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:46:21.504+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:46:21.503+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T23:46:21.523+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:46:21.523+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T23:46:21.541+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.694 seconds
[2024-07-24T23:46:51.722+0000] {processor.py:157} INFO - Started process (PID=2023) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:46:51.723+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T23:46:51.725+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:46:51.725+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:46:51.756+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T23:46:51.757+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T23:46:52.402+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:46:52.423+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:46:52.422+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T23:46:52.444+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:46:52.444+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T23:46:52.461+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.742 seconds
[2024-07-24T23:47:22.603+0000] {processor.py:157} INFO - Started process (PID=2025) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:47:22.605+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T23:47:22.608+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:47:22.608+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:47:22.677+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T23:47:22.677+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T23:47:23.540+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:47:23.578+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:47:23.577+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T23:47:23.610+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:47:23.610+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T23:47:23.635+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 1.039 seconds
[2024-07-24T23:47:53.798+0000] {processor.py:157} INFO - Started process (PID=2027) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:47:53.799+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T23:47:53.802+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:47:53.802+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:47:53.834+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T23:47:53.835+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T23:47:54.361+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:47:54.384+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:47:54.383+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T23:47:54.403+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:47:54.403+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T23:47:54.420+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.624 seconds
[2024-07-24T23:48:24.609+0000] {processor.py:157} INFO - Started process (PID=2029) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:48:24.610+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T23:48:24.611+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:48:24.611+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:48:24.651+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T23:48:24.652+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T23:48:25.431+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:48:25.453+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:48:25.452+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T23:48:25.473+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:48:25.472+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T23:48:25.492+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.886 seconds
[2024-07-24T23:48:55.686+0000] {processor.py:157} INFO - Started process (PID=2031) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:48:55.687+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T23:48:55.690+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:48:55.690+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:48:55.735+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T23:48:55.736+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T23:48:56.668+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:48:56.691+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:48:56.690+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T23:48:56.716+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:48:56.716+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T23:48:56.738+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 1.056 seconds
[2024-07-24T23:49:26.848+0000] {processor.py:157} INFO - Started process (PID=2033) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:49:26.849+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T23:49:26.852+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:49:26.851+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:49:26.887+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T23:49:26.888+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T23:49:27.573+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:49:27.594+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:49:27.594+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T23:49:27.615+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:49:27.615+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T23:49:27.632+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.788 seconds
[2024-07-24T23:49:57.817+0000] {processor.py:157} INFO - Started process (PID=2035) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:49:57.818+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T23:49:57.821+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:49:57.821+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:49:57.857+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T23:49:57.858+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T23:49:58.459+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:49:58.479+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:49:58.479+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T23:49:58.498+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:49:58.498+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T23:49:58.518+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.704 seconds
[2024-07-24T23:50:28.702+0000] {processor.py:157} INFO - Started process (PID=2037) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:50:28.703+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T23:50:28.705+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:50:28.705+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:50:28.740+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T23:50:28.741+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T23:50:29.333+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:50:29.360+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:50:29.360+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T23:50:29.389+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:50:29.389+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T23:50:29.411+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.712 seconds
[2024-07-24T23:50:59.578+0000] {processor.py:157} INFO - Started process (PID=2039) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:50:59.579+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T23:50:59.581+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:50:59.580+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:50:59.615+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T23:50:59.615+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T23:51:00.237+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:51:00.258+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:51:00.257+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T23:51:00.277+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:51:00.276+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T23:51:00.292+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.717 seconds
[2024-07-24T23:51:30.486+0000] {processor.py:157} INFO - Started process (PID=2041) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:51:30.487+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T23:51:30.489+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:51:30.488+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:51:30.525+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T23:51:30.525+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T23:51:31.103+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:51:31.122+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:51:31.122+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T23:51:31.141+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:51:31.140+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T23:51:31.158+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.675 seconds
[2024-07-24T23:52:01.324+0000] {processor.py:157} INFO - Started process (PID=2043) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:52:01.325+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T23:52:01.328+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:52:01.327+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:52:01.363+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T23:52:01.363+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T23:52:01.936+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:52:01.953+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:52:01.952+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T23:52:01.978+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:52:01.978+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T23:52:01.998+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.676 seconds
[2024-07-24T23:52:32.164+0000] {processor.py:157} INFO - Started process (PID=2045) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:52:32.165+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T23:52:32.168+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:52:32.168+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:52:32.216+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T23:52:32.218+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T23:52:32.809+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:52:32.829+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:52:32.829+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T23:52:32.851+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:52:32.850+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T23:52:32.868+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.707 seconds
[2024-07-24T23:53:03.043+0000] {processor.py:157} INFO - Started process (PID=2047) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:53:03.044+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T23:53:03.046+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:53:03.046+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:53:03.077+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T23:53:03.078+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T23:53:03.635+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:53:03.652+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:53:03.652+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T23:53:03.670+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:53:03.670+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T23:53:03.685+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.646 seconds
[2024-07-24T23:53:33.847+0000] {processor.py:157} INFO - Started process (PID=2049) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:53:33.848+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T23:53:33.850+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:53:33.849+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:53:33.879+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T23:53:33.880+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T23:53:34.422+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:53:34.440+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:53:34.440+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T23:53:34.459+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:53:34.458+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T23:53:34.473+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.628 seconds
[2024-07-24T23:54:04.586+0000] {processor.py:157} INFO - Started process (PID=2051) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:54:04.587+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T23:54:04.589+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:54:04.589+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:54:04.619+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T23:54:04.620+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T23:54:05.148+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:54:05.166+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:54:05.166+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T23:54:05.185+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:54:05.185+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T23:54:05.202+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.618 seconds
[2024-07-24T23:54:35.376+0000] {processor.py:157} INFO - Started process (PID=2053) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:54:35.377+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T23:54:35.380+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:54:35.379+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:54:35.416+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T23:54:35.416+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T23:54:36.007+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:54:36.028+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:54:36.027+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T23:54:36.049+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:54:36.048+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T23:54:36.066+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.693 seconds
[2024-07-24T23:55:06.233+0000] {processor.py:157} INFO - Started process (PID=2055) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:55:06.234+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T23:55:06.236+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:55:06.236+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:55:06.264+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T23:55:06.265+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T23:55:06.816+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:55:06.837+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:55:06.836+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T23:55:06.855+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:55:06.855+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T23:55:06.872+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.642 seconds
[2024-07-24T23:55:37.050+0000] {processor.py:157} INFO - Started process (PID=2057) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:55:37.051+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T23:55:37.053+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:55:37.053+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:55:37.084+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T23:55:37.084+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T23:55:37.645+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:55:37.664+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:55:37.664+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T23:55:37.686+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:55:37.686+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T23:55:37.706+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.659 seconds
[2024-07-24T23:56:07.885+0000] {processor.py:157} INFO - Started process (PID=2059) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:56:07.886+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T23:56:07.889+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:56:07.889+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:56:07.923+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T23:56:07.924+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T23:56:08.552+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:56:08.573+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:56:08.573+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T23:56:08.593+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:56:08.593+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T23:56:08.609+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.726 seconds
[2024-07-24T23:56:38.778+0000] {processor.py:157} INFO - Started process (PID=2061) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:56:38.778+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T23:56:38.781+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:56:38.781+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:56:38.812+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T23:56:38.813+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T23:56:39.345+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:56:39.365+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:56:39.364+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T23:56:39.384+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:56:39.383+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T23:56:39.402+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.627 seconds
[2024-07-24T23:57:09.497+0000] {processor.py:157} INFO - Started process (PID=2063) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:57:09.498+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T23:57:09.500+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:57:09.499+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:57:09.530+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T23:57:09.530+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T23:57:10.050+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:57:10.068+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:57:10.068+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T23:57:10.094+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:57:10.094+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T23:57:10.112+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.619 seconds
[2024-07-24T23:57:40.277+0000] {processor.py:157} INFO - Started process (PID=2065) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:57:40.278+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T23:57:40.280+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:57:40.280+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:57:40.314+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T23:57:40.314+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T23:57:40.860+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:57:40.881+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:57:40.880+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T23:57:40.901+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:57:40.901+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T23:57:40.918+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.644 seconds
[2024-07-24T23:58:11.080+0000] {processor.py:157} INFO - Started process (PID=2067) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:58:11.081+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T23:58:11.085+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:58:11.084+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:58:11.115+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T23:58:11.116+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T23:58:11.680+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:58:11.700+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:58:11.699+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T23:58:11.720+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:58:11.720+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T23:58:11.738+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.661 seconds
[2024-07-24T23:58:41.913+0000] {processor.py:157} INFO - Started process (PID=2069) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:58:41.914+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T23:58:41.917+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:58:41.916+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:58:41.946+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T23:58:41.947+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T23:58:42.510+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:58:42.529+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:58:42.529+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T23:58:42.550+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:58:42.550+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T23:58:42.565+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.654 seconds
[2024-07-24T23:59:12.726+0000] {processor.py:157} INFO - Started process (PID=2071) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:59:12.727+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T23:59:12.729+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:59:12.729+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:59:12.759+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T23:59:12.759+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T23:59:13.271+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:59:13.289+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:59:13.288+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T23:59:13.306+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:59:13.305+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T23:59:13.319+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.595 seconds
[2024-07-24T23:59:43.487+0000] {processor.py:157} INFO - Started process (PID=2073) to work on /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:59:43.488+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-07-24T23:59:43.490+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:59:43.490+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:59:43.517+0000] {logging_mixin.py:151} INFO - PARSER IS ALIVE
[2024-07-24T23:59:43.518+0000] {logging_mixin.py:151} INFO - COMPLETED PARSING CONSTANTS
[2024-07-24T23:59:44.027+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_reddit_pipeline']) retrieved from /opt/airflow/dags/reddit_dag.py
[2024-07-24T23:59:44.045+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:59:44.044+0000] {dag.py:2941} INFO - Sync 1 DAGs
[2024-07-24T23:59:44.067+0000] {logging_mixin.py:151} INFO - [2024-07-24T23:59:44.067+0000] {dag.py:3722} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-07-24T00:00:00+00:00, run_after=2024-07-25T00:00:00+00:00
[2024-07-24T23:59:44.083+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.600 seconds
